
% Default to the notebook output style

    


% Inherit from the specified cell style.




    \documentclass[journal,twocolumn]{IEEEtran}
\usepackage[noadjust]{cite}

    
    

    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    
\usepackage{mathptmx}
\usepackage{algorithm}\usepackage{algorithmic}


    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{paper}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    
\renewcommand{\figurename}{Figure}
\captionsetup{labelformat=simple}\title{Bibliometric Analysis of ICLS and CSCL research}
\author{\IEEEauthorblockN{LÃ©onore Valentine Guillain, Nour Ghalia Abassi}\par
    \IEEEauthorblockA{EPFL}
}
\maketitle

    
    

    

\begin{abstract}
    In this report we explore ways to automatically analyze ICLS and CSCL
papers to gain an understanding of the community formed by conferences
participants and of the research being conducted within the community.
After describing how to extract data from the original dataset we we
derive insights into the community using social network analysis and
{[}Nour: NLP etc{]}. We discover patterns of collaboration between
participants, their migration and the global spread of the community.
{[}Something about the insights in keywords/NLP/Co-Citation{]}
\end{abstract}
    \hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

The starting point of the project are the proceedings of the
International Conference of the Learning Sciences (ICLS), respectively
the International Conference on Computer-Supported Collaborative
Learning (CSCL). The two conferences are organized by the International
Society of the Learning Sciences (ISLS) and held in biennial alternation
with each other. At the time of writing the dataset contains 4 years of
the proceedings, staring from 2015 to 2018. While ICLS covers the entire
field of the learning sciences, CSCL focuses on learning through
collaboration with the help of communication technologies \cite{ICLS}.
To understand the community formed by the participants of the
conferences as well as to understand the research being conducted within
the community, we explore methods to bring insight into the following
aspects:

\begin{itemize}
\tightlist
\item
  the institutions and countries dominant in the conferences
\item
  the differences between ICLS and CSCL with respect to citations,
  contributors and publications
\item
  first insight into changes of the years with respect to citations,
  contributors and publications
\item
  collaboration network patterns with respect to countries, institutions
  and authors
\item
  the migration of participants across institutions
\item
  \ldots{} {[}Nour{]}
\end{itemize}

To this means we first present an extensive pipeline to process the
textual data and extract features relevant to bibliographic analysis
from the textual data. After this we perform a first bibliographic
analysis based on the extracted data.

    \hypertarget{data-processing}{%
\section{Data Processing}\label{data-processing}}

    The provided raw dataset consists of papers in \emph{pdf} format, as
well as associated metadata in \emph{xml} format. To extract information
from the dataset we first convert the data to a format that is more
adapt to the task of data analysis and more compatible with a variety of
external libraries. We store all the resulting data in \emph{csv}
format, as csv is a human readable format and can easily be processed by
all the data analysis tools we employ.

    \hypertarget{processing-the-metadata}{%
\subsection{Processing the Metadata}\label{processing-the-metadata}}

We extract the metadata from the xml files using the \emph{lxmltree}
python library. We associate the contents of each xml tag to a column in
csv file. As the xml files do not exactly follow the xml standard, we
handle errors by iteratively parsing the xml tree. For some files, the
section containing the keywords is malformed which can not be handled by
the parser. This is remedied by using regular expressions to match the
section contains the keywords. To each file we associate the name of the
source file and the order in which an author name occurs in the xml
file. Each author is associated one line in the XML file. From the
associated citation string we add to the first part of the citation
containing the author names, year and title. which we use to identify
references in other papers to that paper. Then we add a shortened
version of author names (i.e.~West, R.) by considering the author order.
Note that the short name extracted from the citation and the
corresponding long name (Robert West) in the metadata do at times not
match up. Thus the author order listed in the citation string does not
always correspond to the author order listed in the metadata. After
extraction, we clean the data by unifying names, as slight spelling
differences exist within the same author name (i.e.~Robert West, Robert
A. West). When unifying the names we need to make sure to not overmerge
by labeling to different authors as having the same name. First, we use
a strict condition to catch small differences in names, such as an extra
middle name. This is done by splitting up names into substrings based on
whitespace and commas. For two names in the dataset we check whether the
intersection of their sets of substrings contains at least 2 names of
length at least 3. If the condition is met we consider these two names
to be identical, and add the mapping from one name to the other to a
dictionary. Once all tuples of names (s.t. the first name appears before
the second one in order) are checked and added to the dictionary we
unify the names. Using this method we reduce the number of authors in
the dataset from 1951 down to 1879. Further, to merge names with
misspellings or people that use nicknames (Christian vs Chris) and avoid
over-merging we consider only names of authors that have similar
collaboration patterns. To do this we build the graph of co-authors and
check names of authors that are in the same neighborhood of a node on
the graph (see methodology for further details). Then we use the
\emph{difflib} library to check for similarity of two strings and
consider a name to belong to the same person if they reach the threshold
of 0.8. Again names considered to belong to the same person are added to
the mapping dictionary which is then applied to all names. This method
again reduces the number of authors in the dataset from 1879 to 1951.

    \hypertarget{handling-of-pdf-data}{%
\subsection{Handling of pdf data}\label{handling-of-pdf-data}}

To extract information from the papers themselves we need to first
extract the text from the pdfs. Various parsers performing this task
exist. However, parsers such as pdfminer, PyPDF2 that claim to be able
to extract the text as well as metadata from papers have not been kept
up to date, which leads to issues with dependencies. We found that the
most reliable parser is \emph{poppler} \cite{poppler} which can be
installed and then used via the command line to extract the text
contained in the pdfs to \emph{txt} format. All methods describing how
to extract data from the papers work on the extracted \emph{txt} files.
The characters contained in the \emph{txt} files are not all encoded
uniformly, which means that characters that look the same to humans may
be encoded in different way . Hence, for many applications we need to
unify the formating. This is done using the \emph{unicodedata} module
and setting converting all characters to `Normal Form Composed' form .

A convenient property of papers is that they are structured fairly
regularly. The ISLS Author Guidelines \cite{guidelines} give us guidance
on how the papers in our dataset should be structured. We use this to
our advantage when extracting information from the paper text. While the
vast majority of papers in the dataset follow these guidelines,
exceptions do exist. We thus make sure to handle deviations from the
structure.

\hypertarget{extracting-the-papers-cited-by-a-paper}{%
\subsection{Extracting the papers cited by a
paper}\label{extracting-the-papers-cited-by-a-paper}}

We know that each valid paper contains a reference section at the end
listing all the references cited in the paper. While tools are being
developed to extract the reference section directly from pdfs, notably
Scholarcy \cite{scholarcy}, they did not produce satisfactory results.
That is they had issues splitting the references correctly and
categorizing parts of the reference accurately. This may occur because
they are trained to extract any reference format, and thus can not make
many assumptions on the structure. But in our dataset papers following
the guidelines should list their references in APA format. Hence using
regular expressions and background on how APA references gives us better
results than out of the box methods.

\textbf{Regular expressions in python:} Python has two modules that
implement regular expressions \emph{re} and \emph{regex}. We used regex
as --unlike the re module-- this module allows to handle strings with
non-ascii characters by having unicode expressions. As many authors have
names with non-ascii characters this is an important capability.

\textbf{To get the reference section for of a paper} we use a regular
expressions to find the reference header and determine the sections end
by checking for acknowledgement or appendix sections in the text file
previously extracted using poppler. Then, to extract individual
references we split the section into a list of substrings at each new
line character and glue substrings back together if they satisfy a set
of conditions. We found this method to be robust as it makes sure that
all properly formatted references are separated.

\textbf{To unify substrings} we exploit the fact that that APA
references all have the same initial structure: \emph{Lastname F.M.
(when published) Title which may contain colons and other non-alphabetic
chars. Where published. Who published} \cite{apaformat}. We thus check
that the beginning of each reference contains ``Lastname F.M. (when
published)''. It is relatively easy to check for this using regular
expressions as author names and year have a given structure. If a string
does not contain such a section, then it must be part of an other
reference, which is lies before in the list of reference substrings.
Note that this beginning is also what is added to the metadata as
identifying string. We found that in practical application it is
important to first merge lines if they were clearly cut off, as papers
with many authors may have the beginning on multiple lines. We can find
split up lines by checking for lines starting with non-alphabetic
characters or starting in a sentence. We merge these with the line
above. We can identify lines by their ending as being split up, such as
if they end in a comma, colon or only contain author names. We merge
these lines with the lines below them. Only then can we check whether a
line contains a valid citation start. If it does not we merge it with
the line above it. After this merging process we validate the returned
reference split by looking at the overall length and the initials, as
well as checking that a random sample returns satisfactory splitting
accuracy. We find that the above method of extracting the references
produces good results for proper APA citations and on APA citations
indeed outperforms Scolarcy when it comes to splitting up the lines.
\emph{To then get the subcomponents of a reference} is rather trivial,
as we can again use the structure of APA citations. The string before
the first parenthesis contains the authors names, inside the parentheses
we find the year or some other reference to publication time, after
which the title follows until the first dot. Then a reference to the
publishing venue follows.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_10_0.png}\end{center}
        \caption{Different formats used to reference institution}
        \label{figA}
    \end{figure}
    

    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_11_0.png}\end{center}
        \caption{Different formats used to reference institution}
        \label{figB}
    \end{figure}
    
    \hypertarget{extracting-institution-and-location}{%
\subsection{Extracting Institution and
Location}\label{extracting-institution-and-location}}

Now we consider the section before the abstract, which contains the
authors names, their email and the institution they are affiliated with.
Affiliation with institution can give us a lot of insight where people
are working and gives use a way estimate where they are from. To go from
this section to an exact affiliation with an institution a multifaceted
approach was used.

\hypertarget{approach-1}{%
\subsubsection{Approach 1}\label{approach-1}}

The most robust way to get a persons institution affiliation is to use
their email. Emails can easily be extracted from the header section
using a regular expression and given a domain-university mapping
\cite{mapping} give us an onto mapping from domain to institution. The
additional advantage of this method is that using the additional
metadata associated to each university in the mapping, we also access
information about the country of the university.

\hypertarget{approach-2}{%
\subsubsection{Approach 2}\label{approach-2}}

The header section itself contains information on the institution, but
as different was to list authors are accepted, notably two ways as can
be seen in Figure \ref{figA} and Figure \ref{figB}. Hence, we first
detect the structure of the header section before extracting the
association. We tag the substring present in this section using a
combination of \emph{named entity detection}, using the implementation
provided by \emph{spacy}, simple string comparison matching of the names
of authors and institutions we extracted from the metadata respectively
from Approach 1 and tagging sections containing ``emails'' using regex ,
to detect where in the header the email is. Then, taking the tagged
section, we detect which subsections correspond to which
author-affiliation listing type, and extract the author email -
institution mapping accordingly.

Approach 2 allows to match many more institutions to authors than
Approach 1 as not every institution is present in the mapping --no
companies will be in the mapping-- and many participants use gmail and
similar providers as their official correspondence email. However,
Approach 2 may contain differences in spelling in the institution name,
which need to be merged. Thus we combine both approaches, and use the
corresponding institutions in Approach 1 to unify the institutions
obtained in Approach 2. Furthermore, we later merge names by considering
the location of the institutions.

But first as with Approach 2 institution affiliation does not allow us
to automatically associate a country to the institution, we need to get
this through other means. A first step is to use the top level domains
present in the emails to associate a country of origin to the
participants and their associated institutions. Then, to plot the
community we augment the dataset with geolocation information. As a
further benefit, having this information we can then easily associate a
country to the institution for the emails that do not have a top level
domain indicating location and merge the names of institutions where
necessary based on location.

As there are a variety of way in which the name of an institution could
be written --consider EPFL, EPF Lausanne-- crawling the english, french
and german version of Wikipedia (not using the API, or the API wrapper,
as this does not give access to the desired information) was a quick and
accurate way to get the location. The advantage of sending requests over
using the API is due to the website redirecting to the relevant article
on the institution even when misspellings are present. Requests were
sent using \emph{urllib3} and \emph{beautifulsoup} was used to parse
them get the coordinates present in the article. Regretfully not all
institutions, even well known ones such as Tokyo University, have their
coordinates listed on their Wikipedia. To get the remaining locations we
use the geopy, which is a client for geocoding web services. These
results however are more error prone and will always return a result. It
is thus recommended to verify the correctness of the results. To get the
country we can then again use geopy. The resulting data containing paper
identifier, the authors email, its associated domain, order in which the
author was cited, the institution, country, Longitude and Latitude are
saved to a csv file. We use the paper identifier and author order to
associate this data with the metadata to link information about authors
with the papers they authored.

    \hypertarget{keywords-extraction}{%
\subsection{Keywords extraction}\label{keywords-extraction}}

    \hypertarget{text-preprocessing}{%
\subsubsection{Text preprocessing}\label{text-preprocessing}}

    From the pdf parser some of the documents were corrupted with some weird
caracters such as , white space strings

\begin{itemize}
\item
  We use \textbf{\emph{beautiful soup}} and \textbf{\emph{string}}
  libraries to only keep the text that we needed without any special
  character.
\item
  we apply \textbf{\emph{regular expression}} to remove numbers because
  we thought that numbers were not that important.
\item
  We put all the words to lower cases
\item
  We remove the words that were connecting parts of a sentence rather
  than showing subjects, objects or intent. For us these words were not
  that important and were only going to increase the bag of words length
\item
  In some methods we use \textbf{\emph{Lemmatization}} and
  \textbf{\emph{stemming}} and in other methods we don't because we
  wanted to do a mapping with \textbf{\emph{GloVe}} vocabulary which we
  will explain later on.
\end{itemize}

\textbf{\emph{Lemmatization}} is a word variations to the root of the
word (e.g.~working, works, worked changed to work).
\textbf{\emph{Stemming}} is the process of reducing inflected and
derived words to their word stem, base or root form---generally a
written word form As the use of the stop words to reduce the BOW (bag of
words size we also used lemmatization and stemming to be able to reduce
the number of words without taking off any information)

    \textbf{\emph{Lemmatization}} and different form of
\textbf{\emph{Stemming}} were only used with \textbf{\emph{TF-IDF}}
while doing the Keywords extraction, otherwise we prefer keeping the
words as they are because we are doing a mapping with a pretrained
stanford word embedding and we do not want to loose interpretability.

    For the Keywords extraction we use three methods :

The first one whe is a trivial one is to search in the pdf itself for
keywords and extract them, this method is the most accurate but we found
out that for 492 papers we had no keywords included. So we deciced to
use two methods for keywords extraction. The first one is
\textbf{\emph{TF-IDF}} and the second one is \textbf{\emph{Rake}}.

    \begin{itemize}
\tightlist
\item
  \textbf{\emph{TF-IDF}}: Tf--idf is one of the most popular
  term-weighting schemes today, in information retrieval, tf--idf or
  TFIDF, short for term frequency--inverse document frequency, is a
  numerical statistic that is intended to reflect how important a word
  is to a document in a collection or corpus. It is often used as a
  weighting factor in searches of information retrieval, text mining,
  and user modeling. The tf--idf value increases proportionally to the
  number of times a word appears in the document and is offset by the
  number of documents in the corpus that contain the word, which helps
  to adjust for the fact that some words appear more frequently in
  general.
\end{itemize}

    With TF-IDF we sorted for each document the words with the highest score
and then selected the the 7 first words for each document.

    \begin{itemize}
\tightlist
\item
  \textbf{\emph{Rake}}: Rake stands for Rapid Automatic Keyword
  Extraction. It is an existing python implementation that uses the NLTK
  toolkit for the score calculations.
\end{itemize}

\textbf{\emph{How it works}} : * It splits the text into sentences and
generates some words as candidates.\\
* Various punctuation signs will be treated as sentence boundaries. *
All words listed in the stopwords file will be treated as phrase
boundaries. This helps generate candidates that consist of one or more
non-stopwords. However ,it won't work in cases where the stopword is
part of the phrase. For example, `new' is listed in RAKE's stopword
list. This means that neither `New York' nor `New Zealand' can be ever a
keyword. * For each keyword candidate generated, the algorithm computes
the score of the pretended keyword, which is the sum of the scores for
each of its words (if it's a composed word). The words are scored
according to their frequency and the typical length of a candidate
phrase in which they appear. * The last step is ranking the scores ansd
selecting the ones with the highest score

    \textbf{\emph{Keywords in text}}: Select from the text the keywords
already given by the authors of the paper.

    Once we have all the keywords from the three methods, we try to select
the most accurate keywords. So we create a method that selects the
intersection of TF-IDF keywords and the Rake keywords with the keywords
of the authors (if there are any keywords in the text), if the
intersection is empty and we have the keywords in the text we only take
the keywords of the text. If we don't have any keywords we take the
intersection of the TF-IDF and the Rake. If again, the intersection is
empty then we only take the TF-IDF keywords because they were closer to
the text keywords than the Rake Keywords.

    Once we have our final keywords, we create a method that goes into the
metadata and assign to every document a publication year.


    Since we don't have the same number of documents for each year we needed
to scale over the years and use the percentage of appearence of a word.
So after groupping the words by year and scaling them we found graph
below


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_26_0.png}\end{center}
        \caption{}
        \label{}
    \end{figure}
    
    We wanted then to know what are the trends of words for each specific
year, so we select distinct words for each year, it means that we select
for each year the words that were present for that year and never for
the others. We get the graph below. As we can see from the graph, the
most used word in the papers, more than 10\% of 2015 papers uses the
word `peer', more than 6\% of the papers uses the words `building',
`embodied' in 2016. In 2017 the most frequent keyword on is `reponsive'
and in 2018 `tools' is present with more than 6\%.

    \hypertarget{methodology-find-better-section-head..}{%
\section{Methodology {[}find better section
head..{]}}\label{methodology-find-better-section-head..}}

    \hypertarget{document-clustering}{%
\subsection{Document Clustering}\label{document-clustering}}

To cluster the documents we decided to use two different approaches. The
first one is Natural Langage Processing (NLP) and the second one is a
network analysis. The most accurate one is the network clustering using
the references graph.

\hypertarget{approach-1-natural-langage-processing-nlp}{%
\subsubsection{Approach 1 : Natural Langage Processing
(NLP)}\label{approach-1-natural-langage-processing-nlp}}

\hypertarget{baseline-tf-idf-combined-with-k-means}{%
\paragraph{Baseline : TF-IDF combined with
K-means}\label{baseline-tf-idf-combined-with-k-means}}

With the natural langage processing, we start with a naive method which
consists of clustering the documents using only TF-IDF. Each document
has a vector representation with values for each word present in the
document depending on the frequency of the word in this document
regarding its frequency in all the other documents. So if a word is a
keyword to the document and is not a common word than its value will be
more important than the other words in the vector of the document. Once
we have our vector representation, we select the max features parameter
as 2500, it means that we will only consider the top max\_features
ordered by term frequency across the corpus. On this word representation
we will run K-means.

\textbf{\emph{K-means}} clustering is a method used for clustering
analysis, especially in data mining and statistics. It aims to partition
a set of observations into a number of clusters (k), resulting in the
partitioning of the data into Voronoi cells. It can be considered a
method of finding out which group a certain object really belongs to.

This method does not give good results as we can see it in the graph
below all the vectors are together there is no clear separation between
the documents. So we decide to use other methods using word embeddings
and properties of TF-IDF.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_30_0.png}\end{center}
        \caption{}
        \label{}
    \end{figure}
    
    \hypertarget{second-method-average-of-glove-vectors-combined-with-k-means}{%
\paragraph{Second method : Average of GloVe vectors combined with
K-means}\label{second-method-average-of-glove-vectors-combined-with-k-means}}

The second method was to use pre-trained words embeddings. We decided to
use \textbf{\emph{GloVe}} library pre-trained on Wikipedia dataset.

\textbf{\emph{GloVe}}, coined from Global Vectors, is a model for
distributed word representation. The model is an unsupervised learning
algorithm for obtaining vector representations for words. Training is
performed on aggregated global word-word co-occurrence statistics from a
corpus (Wikipedia), and the resulting representations showcase
interesting linear substructures of the word vector space. It is
developed as an open-source project at Stanford.{[}1{]}

We construct a dictionnary from the words embeddings mapping each word
of a document to its vector representation, than we average all the
vectors of each document. Since each words has its representation in
space, the average of all the words for each document will lead to a
vector representing a document.

Once we get a document vector representation, we apply to it K-means.
The results are better, but we have only two clusters. If we increase
the number of clusters than there is no clear separation anymore
separation.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_32_0.png}\end{center}
        \caption{}
        \label{}
    \end{figure}
    
    \hypertarget{third-method-weighted-average-of-tf-idf-glove-combined-with-k-means}{%
\paragraph{Third method : Weighted average of TF-IDF + GloVe combined
with
K-means}\label{third-method-weighted-average-of-tf-idf-glove-combined-with-k-means}}

With this method, we did not use the parameter max features of the
TF-IDF vector, we took all the vocabulary generated by all the documents
and we created a method that sorts the document frequency vector, take
the 500 first words and do a weighted average of the words embedding
vectors with the TF-IDF score.

Once we get the weighted average vector, and since we are using a words
embedding of 50 dimentions. We decide to use feature reduction technique
to reduce the number of dimensions and represent our vectors in an other
vector space where all the features are orthogonal to each others.

To apply the feature reduction to our vectors we used
\textbf{\emph{Principal Component Analysis (PCA)}} algorithm.

\textbf{PCA} is a statistical procedure that uses an orthogonal
transformation to convert a set of observations of possibly correlated
variables (entities each of which takes on various numerical values)
into a set of values of linearly uncorrelated variables called principal
components. If there are \textbf{\emph{n}} observations with ****p****
variables, then the number of distinct principal components is
\textbf{\emph{min(n-1,p)}}. This transformation is defined in such a way
that the first principal component has the largest possible variance
(that is, accounts for as much of the variability in the data as
possible), and each succeeding component in turn has the highest
variance possible under the constraint that it is orthogonal to the
preceding components. The resulting vectors (each being a linear
combination of the variables and containing n observations) are an
uncorrelated orthogonal basis set. PCA is sensitive to the relative
scaling of the original variables.

Once we get our new vector representation, we apply K-means and we get
the best separation, with a clear separation of 4 clusters as we can see
it in the figure below.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_34_0.png}\end{center}
        \caption{}
        \label{}
    \end{figure}
    
    \hypertarget{approach-2-references-network-analysis}{%
\subsubsection{Approach 2 : References Network
Analysis}\label{approach-2-references-network-analysis}}

For the references graph we choose to construct 4 different graphs to be
able to extract the more information we can. Our first purpose is to be
able to cluster the documents that are about the same subject.

    \hypertarget{first-reference-graph-intersection-between-referenced-documents}{%
\paragraph{First reference graph : Intersection between referenced
documents}\label{first-reference-graph-intersection-between-referenced-documents}}

Our first graph is constructed using the references of each document. It
is constructed as follow, for each two documents, if there is an
intersection between the references of both documents then there is a
link. The weights of the edges are the length of the intersection set.
The intuition behind is that the more the weight is high, the more the
documents are likely to be about the same subjects since they are citing
the same documents. From this graph, we create a subgraph of it, we
filter the edges keeping only those who have a weight strictly higher
than 3. It means that, each node of the subgraph, is related to at least
one other node with 4 same documents referenced.



    The resulted graph, is an undirected graph with 805 nodes and 6699
edges. On this graph we extract two main information : * Strongly
connected components to be able to detect clusters and related
documents. * In\_degree in order to check if there are documents that
are related to a lot of documents and those documents would be more
general than others, and won't necessarly belong to one specific
cluster.

    \hypertarget{second-graph-relations-between-references}{%
\paragraph{Second graph: Relations between
references}\label{second-graph-relations-between-references}}

The second graph is more about the referenced documents and not about
the documents themselves. The aim of this graph is to see if there are
documents that are more likely to cited together. If it is the case,
then these documents are likely to be about the same subject. It is a
weighted graph constructed as follow: for each document, we create a
link between the references of the document. We create a dictionnary of
links, if the link is already in the dictionnary then we increase the
weight of the link. So for instance, if two references are present
together in the 5 distinct documents, then the weight of the edge would
be of 5. We obtain a graph of 10845 nodes and 148594 edges.



    We can directly select from the interactive graph the nodes with degree
higher than 100, these nodes represent the more cited documents, but we
will construct a subgraph to specificly tackle these important
referenced documents. On this graph, to be able to detect similar
documents we also run Strongly connected components algorithm. On top of
it, we inspected the documents with higher weights, we will explain more
about what we extracted from this graph in the result section.

    \hypertarget{third-graph-title-to-find}{%
\paragraph{Third graph: ??? Title to
find}\label{third-graph-title-to-find}}

With this graph we aim to find what are the most cited documents, what
are the documents that are the most influencial and how citations evolve
over time.

This is a directed graph constructed in this way. For each document, if
the document cites another document than there is a link between the
document and the cited one. We get a graph of 11650 nodes and 13243
egdes. We also construct a subgraph of this graph, which is a
co-citations in conferences, but because a lack of data, we find only
three documents that cites other documents.

On this graph, we run \textbf{\emph{PageRank}} algorithm to be able to
extract the most influential documents.

\textbf{PageRank (PR)}* is an algorithm used by Google Search to rank
web pages in their search engine results. In our case, It measure
PageRank is a way of measuring the importance of referenced papers.
Compared to the in-degree analysis we get almost the same results since
we have no out degree for almost every node.



    \hypertarget{co-authorship}{%
\subsection{Co-authorship}\label{co-authorship}}

To get insight into collaborations within the community we build graphs
based on co-authorship of papers. To build the graph structure and
visualize it the python package \emph{networkx} was used. First, we
built a co-authorship graph based on individual authors. Each node
represents an author, and two nodes are connected if the authors
collaborated on a paper. Other works \cite{cheong2009social},
\cite{hesford2006management} have used a directed graph that only adds a
link from the first author of the paper to all other authors. However,
as we are more interested in collaboration patterns and less in finding
the most influential first author, we build an undirected graph with
edges between all co-authors. On the downside, yields a very dense graph
that can not directly be interpreted. Hence, we prune the initial graph
by removing nodes that do not satisfy additional conditions. Intuitively
it is clear that a singular collaboration on a paper does not
necessarily indicate that two people even know each other. A first
condition was to only link two nodes together if the authors
corresponding to each node have appear as co-authors in more than one
conference. The graph produced by this approach is much more sparse and
contains much fewer authors. Additionally all connected components
contain fewer than 50 authors, making it possible to consider each
author individually. Such a component is shown in figure \ref{fig_plot}.
A second alternative approach was to consider the strength of bounds. In
the graph authors are only linked if they have written at least 2 papers
together. This method yields a much larger component --containing 441
authors-- as this second condition is weaker. To get an understanding of
author to author relations in this component we need to further prune
away parts of the graph that are not insightful. When we look at
individual author within the community, we are interested well
collaborative individuals. Hence we remove nodes that have collaborated
with fewer than 3 connections to other authors. The resulting graph can
be seen in figure \ref{author_collab_large}. Details creating such a
plot can be found in the section dedicated to this below.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_48_0.png}\end{center}
        \caption{One of the connected components of the co-authorship graph}
        \label{fig_plot}
    \end{figure}
    

    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_49_0.png}\end{center}
        \caption{Largest Component}
        \label{author_collab_large}
    \end{figure}
    
    Focusing in on the authors is not the only way to interpret the
co-authorship graph. One should also consider the overall structure of
the graph. As graph is not connected, and contains one large main
component with 1141 authors and 208 very small components with less than
25 authors in them we focus on analyzing the structure of the main
component of the graph. First we aim to detect sub-communities within
the graph. Modularity is one measure used to find such communities. It
measures the connection strength within a community relative to the
strength of outgoing connections. Networks with high modularity have
dense connections between the nodes within modules but sparse
connections between nodes in different modules. A non-parametric
algorithm for community detection is the Louvain Method
\cite{blondel2008fast}. We used the implementation of Thomas Aynaud
\cite{louvain}. This gives a good partitioning of the graph into 26
communities of median size 42.84. This is quite a large number of
communities, so we consider an alternative way to yield bigger
partitions. An alternate way to find subcommunities is to look at
normalized cuts, which partition the nodes into groups with many
within-group connections and relatively few between-group connections.
The advantage of this method is that we can choose how many cuts to
make, and we are thus able to define the size of subcommunities. Finding
normalized cuts in a graph can be done using \emph{sklearns} spectral
clustering algorithm by passing it the adjacency matrix of the graph.
With this method we are able to neatly separate the main component graph
in three large communities. We note that the clusters found using method
one are contained in the clusters found by method 2. See Figure
\ref{Louvain} and Figure \ref{normalized}.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_51_0.png}\end{center}
        \caption{Subcommunities by Louvain}
        \label{Louvain}
    \end{figure}
    

    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_52_0.png}\end{center}
        \caption{Subcommunities via normalized cuts}
        \label{normalized}
    \end{figure}
    
    To further analyze these subcommunities we look at the distribution of
nationalities and institutions over the years. Additionally, to
understand differences in the research being conducted, we analyze the
frequency of keywords used for the papers that were writing within that.

To get a better overview of how co-authorship happens across countries
and institutions, we built graphs where every node represents a country
respectively an institution. Two countries or institutions are linked if
two authors from these two institutions collaborated on a paper
together. The country graph is simple enough that it does not have to be
pruned. The institution graph on the other hand is again not
comprehensible without further processing. Thus, similar to the first
approach on pruning presented, we only consider institutions who
collaborated in at least two conferences. This the main component of
this graph is shown in figure \ref{unigraph}.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_54_0.png}\end{center}
        \caption{Different formats used to reference institution}
        \label{unigraph}
    \end{figure}
    
    \textbf{Plotting of co-authorship graphs}: Graphs were visualised with
the built in plot functions and graph layouts of networkx. The force
directed layout was used as it clusters together nodes with strong
connections. Then the size of each node was changed based on the
position of the node in the graph. Central nodes with a lot of
connections should be larger, while nodes with fewer connections should
be smaller. To achieve this we used the measures of degree centrality
for graphs with few nodes and edges and betweenness centrality for
larger graphs . Degree centrality measures the fraction of degrees
(number of connections) a node has with respect to the total number of
connections in the graph. Betweenness centrality of a node is the sum of
the fraction of all-pairs shortest paths that pass through it. Degree
centrality is more adept for smaller graphs as their structure is still
easy to interpret. As by construction betweenness takes into
consideration the degrees of it neighboring nodes, it better structures
the nodes large graphs into central and less central nodes.

    \hypertarget{creating-interactive-visualizations}{%
\subsection{Creating Interactive
Visualizations}\label{creating-interactive-visualizations}}

To better understand and interact with the data we create interactive
graphs on top of the static graphs. Many libraries exist to allow for
interaction. While working on the project, we worked with \emph{bookeh},
\emph{holoviews}, \emph{mpld3} and \emph{plotly}. For some time
implementing a custom visualization using \emph{D3} was also
contemplated. However, functions provided by plotly is most general and
allows to easily build interactive figures, while not being as heavy as
booked and holoviews. Additionally, it is the most well maintained. D3
requires a running server to be viewed and has a hard time dealing with
large graphs. To plot figures we create them in networkx to grab the
positioning of the nodes. Then we pass this positioning to plotly to
plot.

    \hypertarget{results}{%
\section{Results}\label{results}}


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_58_0.png}\end{center}
        \caption{Distribution of papers published over the years}
        \label{fig1}
    \end{figure}
    
    \hypertarget{community-composition-structure}{%
\subsection{Community Composition \&
Structure}\label{community-composition-structure}}

We find that participants from 372 institutions participated over the
last 4 years, with a median of 128 institutions per year. Numbers of
institutions participating are increasing each year, with a noticeable
peak in 2018, which participants from 261 institutions. This is a
considerable up from the last conference ICLS in 2016 which had 135
participating institutions. Most participants are from the US, with a
average of 50\% of all participants being from the US. We can also
observe a trend of increase in US participants with only 35\% of US
participants in 2015 and 51\% in 2017, followed by 64\% in 2018.
Overall, participants from 41 nations have participated over the 4
years, which an average of 28 nations represented each conference. After
the US, participants of Germany, Canada and Singapore make up the
biggest proportion of participants \(\ref{figrep}\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Collaboration
\item
  Movement of participants
\item
  Citation
\end{enumerate}


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_60_0.png}\end{center}
        \caption{Different formats used to reference institution}
        \label{figrep}
    \end{figure}
    
    \hypertarget{collaboration}{%
\subsubsection{Collaboration}\label{collaboration}}


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_62_0.png}\end{center}
        \caption{Collaborative papers between countries}
        \label{figcollab}
    \end{figure}
    
    \hypertarget{movement-of-particpants}{%
\subsubsection{Movement of particpants}\label{movement-of-particpants}}

Using the data we have of participants over the years, and knowing their
affiliation, we can now find members that change institutions over the
years. Over the years of recording we find 14 participants that have
changed moved countries and 47 that have moved institutions. We can see
that most movement occured within the US respectively Europe,
{[}fig\_move{]}.

    \hypertarget{research-topics-in-the-community}{%
\subsection{Research topics in the
Community}\label{research-topics-in-the-community}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Keywords
\item
  Outside influence: Conferences being sited
\end{enumerate}

    \hypertarget{conclusions}{%
\section{Conclusions}\label{conclusions}}

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod
tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim
veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea
commodo consequat. Duis aute irure dolor in reprehenderit in voluptate
velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint
occaecat cupidatat non proident, sunt in culpa qui officia deserunt
mollit anim id est laborum.
\newpage

    % Add a bibliography block to the postdoc
    
    
\bibliographystyle{unsrt}
\bibliography{references}

    
    \end{document}
