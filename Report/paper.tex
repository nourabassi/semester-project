
% Default to the notebook output style

    


% Inherit from the specified cell style.




    \documentclass[article,twocolumn]{IEEEtran}
\usepackage[noadjust]{cite}

    
    

    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    
\usepackage{mathptmx}
\usepackage{algorithm}\usepackage{algorithmic}


    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{paper}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    
\renewcommand{\figurename}{Figure}
\captionsetup{labelformat=simple}\title{   }
\author{
}
\maketitle

    
    

    

\begin{abstract}
    In this report we investigate ways to automatically analyze ICLS and
CSCL papers to gain an understanding of the community formed by
conference participants and of the research being conducted within the
community. After describing how to extract data from the base dataset we
derive insights into the community using network analysis, natural
language processing and text mining. We discover patterns of
collaboration between participants, their migration and the global
spread of the community. We extract keywords and find clusters of
referenced papers and documents within the conferences.
\end{abstract}
    \hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

The starting point of the project are the proceedings of the
International Conference of the Learning Sciences (ICLS), respectively
the International Conference on Computer-Supported Collaborative
Learning (CSCL). The two conferences are organized by the International
Society of the Learning Sciences (ISLS) and held in biennial alternation
with each other. At the time of writing the dataset contains 4 years of
the proceedings, from 2015 to 2018. While ICLS covers the entire field
of the learning sciences, CSCL focuses on learning through collaboration
with the help of communication technologies \cite{ICLS}. To understand
the community formed by the participants of the conferences as well as
to understand the research being conducted within the community, we
explore methods to bring insight into the following aspects:

\begin{itemize}
\tightlist
\item
  The institutions and countries dominant in the conferences
\item
  The differences between ICLS and CSCL with respect to citations,
  research conducted and contributors
\item
  Collaboration network patterns with respect to countries, institutions
  and authors
\item
  First insight into changes over the years with respect to contributors
  and collaboration patterns
\item
  The migration of participants across institutions
\item
  Popularity of keywords over the years
\item
  Trends in regards to cited papers and the evolution of their
  prevalence from 1977 until 2018
\item
  Clusters of the cited documents
\item
  Clusters of the papers written within the conferences
\end{itemize}

To this means we first present an extensive pipeline to process the
textual data and extract features relevant to bibliographic analysis
from the textual data. After this we perform a first bibliographic
analysis based on the extracted data.

    \hypertarget{data-processing}{%
\section{Data Processing}\label{data-processing}}

    The provided raw dataset consists of papers in \emph{pdf} format, as
well as associated metadata in \emph{xml} format. To extract information
from the dataset we first converted the data to a format that is better
adapted to the task of data analysis and more compatible with a variety
of external libraries. We stored all the resulting data in \emph{csv}
format, as csv is a human readable format and can easily be processed by
all the data analysis tools we employ.

    \hypertarget{processing-the-metadata}{%
\subsection{Processing the Metadata}\label{processing-the-metadata}}

We extracted the metadata from the xml files using the \emph{lxmltree}
python library. We associated the contents of each xml tag to a column
in a csv file. As the xml files do not exactly follow the xml standard,
we handled errors by iteratively parsing the xml tree. For some files,
the section containing the keywords is malformed and can not be
extracted by the parser. This was remedied using regular expressions to
match the section containing the keywords. To each file we associated
the name of the source file and the order in which an author name occurs
in the xml file. Each author is associated to one line in the xml file.
From the associated citation string we add the first part of the
citation containing the author names, year, and title. This is later
used to identify the paper when it is referenced in an other paper. Then
we add a shortened version of author names (i.e.~West, R.) by
considering the author order. Note that it happens that the short name
extracted from the citation and the corresponding long name (Robert
West) in the metadata do not match up. Thus the author order listed in
the citation string does not always correspond to the author order
listed in the metadata. After extraction, we cleaned the data by
unifying names, as slight spelling differences exist within the same
author name (i.e.~Robert West, Robert A. West). When unifying the names
we needed to make sure not to overmerge by labeling two different
authors as having the same name. First, we used a strict condition to
catch small differences in names, such as an extra middle name. This is
done by splitting up names into substrings based on whitespaces and
commas. For two names in the dataset we checked whether the intersection
of their sets of substrings contained at least 2 names of length at
least 3. If the condition was met we considered these two names to be
identical, and added the mapping from one name to the other to a
dictionary. Once all tuples of names (s.t. the first name appears before
the second one in order) were checked and added to the dictionary we
unified the names. Using this method we reduced the number of authors in
the dataset from 1951 down to 1879. Further, to merge names with
misspellings or people who use nicknames (Christian vs Chris) and to
avoid overmerging we considered only names of authors that have similar
collaboration patterns. To do this we built a graph based on
co-authorship relations and checked names of authors that were in the
same neighborhood of a node on the graph (see methodology for further
details). Then, we used the \emph{difflib} library to check the
similarity of two strings and considered a name to belong to the same
person if the similarity measure reached the threshold of 0.8. Again,
names considered to belong to the same person were added to the mapping
dictionary which was then applied to all names. This method again
reduced the number of authors in the dataset from 1879 to 1851.

    \hypertarget{handling-of-pdf-data}{%
\subsection{Handling of pdf data}\label{handling-of-pdf-data}}

To extract information from the papers themselves we needed first to
extract the text from the pdfs. Various parsers performing this task
exist. However, parsers such as pdfminer or PyPDF2 that claim to be able
to extract the text as well as metadata from papers have not been kept
up to date, which leads to issues with dependencies. We found that the
most reliable parser is \emph{poppler} \cite{poppler} which can be
installed and then used via the command line to extract the text
contained in the pdfs to \emph{txt} format. All methods describing how
to extract data from the papers worked on the extracted txt files. The
characters contained in the txt files were not all encoded uniformly,
which means that characters that look the same to humans may be encoded
in different ways. Hence, for many applications we converted all
characters to `Normal Form Composed' form.

A convenient property of papers is that they are structured fairly
regularly. The ISLS Author Guidelines \cite{guidelines} gave us guidance
on how the papers in our dataset should be structured. We used this to
our advantage when extracting information from the paper text. While the
vast majority of papers in the dataset follow these guidelines,
exceptions do exist. We thus made sure to handle deviations from the
structure.

\hypertarget{extracting-the-papers-cited-by-a-paper}{%
\subsection{Extracting the papers cited by a
paper}\label{extracting-the-papers-cited-by-a-paper}}

Each valid paper contains a reference section at the end listing all the
references cited in the paper. While tools are being developed to
extract the reference section directly from pdfs, notably Scholarcy
\cite{scholarcy}, they did not produce satisfactory results. That is,
they did not split the references correctly and categorized parts of the
reference inaccurately. This may occur because they are trained to
extract any reference format, and thus can not make many assumptions on
the structure. But in our dataset papers following the guidelines should
list their references in APA format. Hence using regular expressions and
background on APA referencing gave us better results than out of the box
methods.

\textbf{Regular expressions in python:} Python has two modules that
implement regular expressions, \emph{re} and \emph{regex}. We used regex
as --unlike the re module-- this module allows to handle strings with
non-ascii characters by allowing the use of unicode regular expressions.
As many authors have names containing non-ascii characters this is an
important capability.

\textbf{To get the reference section for of a paper} we used a regular
expressions to find the reference header and determine the end of the
section by checking for acknowledgements or appendix sections in the
text file previously extracted using poppler. Then, to extract
individual references we split the section into a list of substrings at
each new line character and concatenated substrings back together if
they satisfied a set of conditions. We found this method to be robust as
it makes sure that all properly formatted references are separated.

\textbf{To unify substrings} we exploited the fact that APA references
all have the same underlying structure: \emph{Lastname F.M. (when
published) Title which may contain colons and other non-alphabetic
chars. Where published. Who published} \cite{apaformat}. We thus checked
that the beginning of each reference contains ``Lastname F.M. (when
published)''. It is relatively easy to check for this using regular
expressions as author names and year have a given structure. If a string
does not contain such a section, then it must be part of an other
reference, which lies before in the list of reference substrings. Note
that this beginning is also what is added to the metadata as identifying
string. We found that in practical application it is important to first
merge lines if they were clearly cut off, as papers with many authors
may have the beginning on multiple lines. We could find split up lines
by checking for lines starting with non-alphabetic characters or
starting in a sentence. We merged these with the line above. We could
identify lines by their ending as being split up, such as if they end in
a comma, colon or only contain author names. We merged these lines with
the lines below them. Only then could we check whether a line contained
a valid citation start. If it did not, we merged it with the line above
it. After this merging process we validated the returned reference split
by looking at the overall length and the initials, as well as by
checking that a random sample returned satisfactory splitting accuracy.
We found that the above method of extracting the references produced
good results for proper APA citations and on APA citations indeed
outperformed Scolarcy when it came to splitting up the lines.

\textbf{To then get the subcomponents of a reference} was rather
trivial, as we could again use the structure of APA citations. The
string before the first parenthesis contains the authors names, inside
the parentheses we find the year or some other reference to publication
time, after which the title follows until the first dot. Then a
reference to the publishing venue follows.

    \hypertarget{extracting-institution-and-location}{%
\subsection{Extracting Institution and
Location}\label{extracting-institution-and-location}}

Now we consider the section before the abstract, which contains the
authors names, their email addresses and the institutions they are
affiliated with. Affiliation with institution can give us a lot of
insight where people are working and gives use a way estimate where they
are from. To extract the exact affiliation of an authors with an
institution two approaches were used.

\hypertarget{approach-1}{%
\subsubsection{Approach 1}\label{approach-1}}

The most robust way to get a persons institution affiliation is to use
their email address. Email addresses can easily be extracted from the
header section using a regular expression. Then, the part of the email
containing the university domain can be extracted using an other regular
expression. After this we used a domain-university mapping
\cite{mapping} to map domains to institutions. The additional advantage
of this method is that using the additional metadata associated to each
university in the mapping, we also access information about the country
of the university.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_11_0.png}\end{center}
        \caption{Different formats used to reference institution}
        \label{figA}
    \end{figure}
    

    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_12_0.png}\end{center}
        \caption{Different formats used to reference institution}
        \label{figB}
    \end{figure}
    
    \hypertarget{approach-2}{%
\subsubsection{Approach 2}\label{approach-2}}

The header section itself contains information on the institution, but
different ways to list authors are accepted, notably two ways as can be
seen in Figure \ref{figA} and Figure \ref{figB}. Hence, we first
detected the structure of the header section before extracting the
association. We tagged the substring present in this section using a
combination of \emph{named entity detection}, which automatically
classifies strings as belonging to some pre-defined category, using the
implementation provided by \emph{spacy}\cite{spacy2}, and simple string
comparison matching of the names of authors and institutions we
extracted from the metadata respectively from Approach 1 and tagging
sections containing ``emails'' using regex , to detect where in the
header the email is. Then, taking the tagged section, we detected which
subsections corresponded to which author-affiliation listing type, and
extracted the author email - institution mapping accordingly.

Approach 2 allows to match many more institutions to authors than
Approach 1 as not every institution is present in the mapping and many
participants use gmail and similar providers as their official email.
However, Approach 2 may contain differences in spelling in the
institution name, which needed to be unified. Thus we combined both
approaches, and used the corresponding institutions in Approach 1 to
unify the institutions obtained in Approach 2. Furthermore, we later
merged names by considering the location of the institutions.

But first, as with Approach 2 institution affiliation did not allow us
to automatically associate a country to the institution, we needed to
get this information through other means. A first step was to use the
top level domains present in the emails to associate a country of origin
to the participants and their associated institutions. Then, to plot the
community we augmented the dataset with geolocation information. As a
further benefit, having this information we could then easily associate
a country to the institution for the emails that ido not have a top
level domain indicating location and further merge the names of
institutions based on location.

As there are a variety of way in which the name of an institution could
be written --consider EPFL, EPF Lausanne-- crawling the english, french
and german version of Wikipedia (not using the API, or the API wrapper,
as this does not give access to the desired information) was a quick and
accurate way to get the location. The advantage of sending requests over
using the API is due to the website redirecting to the relevant article
on the institution even when misspellings are present or multiple ways
to spell the university exist. Requests were sent using \emph{urllib3},
and \emph{beautifulsoup} was used to parse them get the coordinates
present in the article. Regretfully not all institutions, even well
known ones such as Tokyo University, have their coordinates listed on
Wikipedia. To get the remaining locations we used geopy, which is a
python client for geocoding web services. These results however are more
error prone as geopy will always return a result. It is thus recommended
to verify the correctness of the results. To get the country we then
again used geopy. The resulting data containing paper identifier, the
authors email, its associated domain, order in which the author was
cited, the institution, country, longitude and latitude were saved to a
csv file. We used the paper identifier and author order to associate
this data with the metadata to link information about authors with the
papers they authored.

    \hypertarget{keyword-extraction}{%
\subsection{Keyword extraction}\label{keyword-extraction}}

    \hypertarget{text-preprocessing}{%
\subsubsection{Text preprocessing}\label{text-preprocessing}}

    The pdf parser returns txt files that are encoded using a variety of
conventions, thus when extracting the strings from the text files, we
were met with non-ascii characters, which had to be dealt with. We
preprocessed the text by applying the following steps to it:

\begin{itemize}
\item
  We used \emph{beautiful soup} and \emph{string} libraries to only keep
  sections of the text that we needed and remove special characters.
\item
  We applied \textbf{\emph{regular expression}} to remove numbers
  because numbers were not important for our analysis.
\item
  We put all words to lower case
\item
  We removed words that were connecting parts of a sentence rather than
  showing subjects, objects or intent (\textbf{\emph{stopwords}}). For
  the purpose of this task these words were not important and were only
  going to increase the bag of words length
\item
  In some methods we applied \textbf{\emph{lemmatization}} and
  \textbf{\emph{stemming}} to the words. In other methods we did not
  because we implement a mapping with \textbf{\emph{GloVe}} vocabulary
  instead, which we will explain later on.
\end{itemize}

\textbf{\emph{Lemmatization}} is a word variations to the root of the
word (e.g.~working, works, worked changed to work).
\textbf{\emph{Stemming}} is the process of reducing inflected and
derived words to their word stem, base or root form---generally a
written word form. As the use of the stop words to reduce the BOW (bag
of words) size we also used lemmatization and stemming to be able to
reduce the number of words without taking off any valuable information.

\textbf{\emph{Lemmatization}} and different forms of
\textbf{\emph{Stemming}} were only used with \textbf{\emph{TF-IDF}}
while doing the Keywords extraction, otherwise we prefered keeping the
words as they were because we were doing a mapping with a pretrained
Stanford Word Embedding \cite{GloVe_web} and sometimes root forms were
not in the dictionnary of words of the pre-trained words-vectors.

For the Keywords extraction we used three methods : The first one is a
trivial approach that consists of searching in the pdf itself for
keywords and extract them, this method is the most accurate but we found
out that for 492 papers we had no keywords included. So we deciced to
use other two methods for keywords extraction, namely one based on
\textbf{\emph{TF-IDF}} and a second one based on \textbf{\emph{Rake}}.
\textbf{\emph{TF-IDF}} is one of the most popular term-weighting schemes
today, in information retrieval, tf--idf or TFIDF, short for term
frequency--inverse document frequency, is a numerical statistic that is
intended to reflect how important a word is to a document in a
collection or corpus. It is often used as a weighting factor in searches
of information retrieval, text mining, and user modeling. The tf--idf
value increases proportionally to the number of times a word appears in
the document and is offset by the number of documents in the corpus that
contain the word, which helps to adjust for the fact that some words
appear more frequently in general \cite{Tf-idf}. With TF-IDF we sorted
for each document the words with the highest score and then selected the
7 first words of each document.

    \textbf{\emph{Rake}}: Rake stands for Rapid Automatic Keyword
Extraction. It is an existing python implementation that uses the NLTK
toolkit for the score calculations.

\textbf{\emph{How the method works}} :

\begin{itemize}
\tightlist
\item
  It splits the text into sentences and generates some words as
  candidates.\\
\item
  Various punctuation signs will be treated as sentence boundaries.
\item
  All words listed in the stopwords file will be treated as phrase
  boundaries. This helps generate candidates that consist of one or more
  non-stopwords. However, it won't work in cases where the stopword is
  part of the phrase. For example, `new' is listed in RAKE's stopword
  list. This means that neither `New York' nor `New Zealand' can be ever
  a keyword.
\item
  For each keyword candidate generated, the algorithm computes the score
  of the pretended keyword, which is the sum of the scores for each of
  its words (if it's a composed word). The words are scored according to
  their frequency and the typical length of a candidate phrase in which
  they appear.
\item
  The last step is ranking the scores and selecting the ones with the
  highest score
\end{itemize}

    \textbf{\emph{Keywords in text}}: Select from the text the keywords
already given by the authors of the paper.

Once we have all the keywords from the three methods, we tried to select
the most accurate keywords. So we created a method that selected the
intersection of TF-IDF keywords and the Rake keywords with the keywords
of the authors (if there are any keywords in the text), if the
intersection is empty and we have the keywords in the text we only take
the keywords of the text. If the paper does not contain any keywords, we
take the intersection of the TF-IDF and the Rake outputs. If again, the
intersection is empty then we only take the TF-IDF keywords because they
were closer to the text keywords than the Rake Keywords.

Once we have our final keywords, we create a method that goes into the
metadata and assign to every document a publication year.


    Since we don't have the same number of documents for each year we needed
to scale over the years and use the percentage of appearence of a word.
So after groupping the words by year and scaling them we found the graph
below \ref{fig1}.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_21_0.png}\end{center}
        \caption{Trends of keywords over the years}
        \label{fig1}
    \end{figure}
    
    We wanted then to know what are the trends of words for each specific
year, so we selected distinct words for each year. In other words it
means that we selected for each year the words that were only present
for this specific year and never appeared in the others. We get the
figure below. As we can see from the graph \ref{fig2}, the most used
word in the papers, more than 10\% of 2015 papers used the word `peer',
more than 6\% of the papers used the words `building', `embodied' in
2016. In 2017 the most frequent keyword is `reponsive' and in 2018
`tools' is present with more than 6\%.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_23_0.png}\end{center}
        \caption{Distinct keywords over the years}
        \label{fig2}
    \end{figure}
    
    \hypertarget{methods-for-data-analysis}{%
\section{Methods for Data Analysis}\label{methods-for-data-analysis}}

    \hypertarget{document-clustering}{%
\subsection{Document Clustering}\label{document-clustering}}

To cluster the documents we decided to use two different approaches. The
first one is based on Natural Langage Processing (NLP) and the second
one is rooted in network analysis. We fin that network clustering using
the references graph is most accurate.

\hypertarget{approach-1-natural-langage-processing-nlp}{%
\subsubsection{Approach 1 : Natural Langage Processing
(NLP)}\label{approach-1-natural-langage-processing-nlp}}

\hypertarget{baseline---tf-idf-combined-with-k-means}{%
\paragraph{Baseline - TF-IDF combined with
K-means}\label{baseline---tf-idf-combined-with-k-means}}

With the natural langage processing, we started with a naive method
which consisted of clustering the documents using only TF-IDF. Each
document has a vector representation with values for each word present
in the document depending on the frequency of the word in this document
regarding its frequency in all the other documents. So if a word is a
keyword to the document and is not a common word than its value will be
more important than the other words in the vector of the document. Once
we obtained our vector representation, we selected the max features
parameter as 2500, it means that we only considered the top
max\_features ordered by term frequency across the corpus. On this word
representation we run K-means.

\textbf{\emph{K-means}} clustering is a method used for clustering
analysis, especially in data mining and statistics. It aims to partition
a set of observations into a number of clusters (k), resulting in the
partitioning of the data into Voronoi cells. It can be considered as a
method of finding out which group a certain object really belongs to.

This method did not give good results as we can see it in the graph
below \ref{fig3}. All the vectors are together there is no clear
separation between the documents. So we decide to use other methods
using words embeddings combined with properties of TF-IDF.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_26_0.png}\end{center}
        \caption{Baseline model document clusters}
        \label{fig3}
    \end{figure}
    
    \hypertarget{second-method-average-of-glove-vectors-combined-with-k-means}{%
\paragraph{Second method : Average of GloVe vectors combined with
K-means}\label{second-method-average-of-glove-vectors-combined-with-k-means}}

The second method was to use pre-trained words embeddings. We decided to
use \emph{GloVe} library pre-trained on Wikipedia dataset.

\textbf{\emph{GloVe}}, coined from Global Vectors, is a model for
distributed word representation. The model is an unsupervised learning
algorithm for obtaining vector representations for words. Training is
performed on aggregated global word-word co-occurrence statistics from a
corpus (Wikipedia), and the resulting representations showcase
interesting linear substructures of the word vector space. It is
developed as an open-source project at Stanford.\cite{GloVe} The goal of
GloVe is to group words with similar meaning together in a new vector
space.

We constructed a dictionnary from the words embeddings mapping each word
of a document to its vector representation, then we averaged all the
vectors of each document. Since each words has its own representation in
space, the average of all the words for each document will lead to a
vector representing a document.

Once we obtained a document vector representation, we applied K-means to
it. The results were slightly better, but we had only two clear
separated clusters. If we increase the number of clusters, then there is
no clear separation between clusters visible anymore. The plot in figure
\ref{fig4} reprensents the documents belonging to each cluster.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_28_0.png}\end{center}
        \caption{Cluster using Average of GloVe vectors combined with K-means }
        \label{fig4}
    \end{figure}
    
    \hypertarget{third-method-weighted-average-of-tf-idf-glove-combined-with-k-means}{%
\paragraph{Third method : Weighted average of TF-IDF + GloVe combined
with
K-means}\label{third-method-weighted-average-of-tf-idf-glove-combined-with-k-means}}

With this method, we did not use the parameter max features of the
TF-IDF vector, we took all the vocabulary generated by all the documents
and we created a method that sorts the document frequency vector. We
selected the 500 first words and computed a weighted average of the
every word embedding vector with the its respectively TF-IDF score.

Once we get the weighted average vector, since we were using a words
embedding of 50 dimentions, we decided to use feature reduction
technique to reduce the number of dimensions and represent our vectors
in an other vector space where all the features are orthogonal to each
others.

To apply feature reduction to our vectors we used
\textbf{\emph{Principal Component Analysis (PCA)}} algorithm.

\textbf{PCA} is a statistical procedure that uses an orthogonal
transformation to convert a set of observations of possibly correlated
variables (entities each of which takes on various numerical values)
into a set of values of linearly uncorrelated variables called principal
components. If there are \textbf{\emph{n}} observations with
\textbf{\emph{p}} variables, then the number of distinct principal
components is \textbf{\emph{min(n-1,p)}}. This transformation is defined
in such a way that the first principal component has the largest
possible variance (that is, accounts for as much of the variability in
the data as possible), and each succeeding component in turn has the
highest variance possible under the constraint that it is orthogonal to
the preceding components. The resulting vectors (each being a linear
combination of the variables and containing n observations) are an
uncorrelated orthogonal basis set. \cite{PCA}.

Once we get our new vector representation, we applied K-means and we
obtained the best separation, with a clear separation of 4 clusters as
we can see it in figure \ref{fig5}.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_30_0.png}\end{center}
        \caption{Cluster using Weighted average of TF-IDF + GloVe  combined with K-means }
        \label{fig5}
    \end{figure}
    
    \hypertarget{approach-2-references-network-analysis}{%
\subsubsection{Approach 2 : References Network
Analysis}\label{approach-2-references-network-analysis}}

For the references graph we chose to construct 4 different graphs to be
able to extract as much information as possible. Firstly, to be able to
cluster the documents that are about the same subject and secondly to
gain more insights about the most influencial papers and trends of
citations.

    \hypertarget{first-reference-graph-intersection-between-referenced-documents}{%
\paragraph{First reference graph : Intersection between referenced
documents}\label{first-reference-graph-intersection-between-referenced-documents}}

Our first graph is constructed using the references of each document. It
is constructed as follows: for each two documents, if there is an
intersection between the references of both documents, then there is a
link conneting the two documents. The weights of the edges are the
length of the intersection set. The intuition behind it is that the
higher the weight, the more the documents are likely to be about the
same subject, since they are citing the same documents. From this graph,
we create a subgraph of it. We filter the edges keeping only those who
have a weight strictly higher than 3. This means that, each node of the
subgraph, is related to at least one other node with 4 same documents
referenced.



    The resulting graph, plotted in figure \ref{intersection}, is an
undirected graph with 805 nodes and 6699 edges. On this graph we
extracted two main features:

\begin{itemize}
\tightlist
\item
  Strongly connected components to be able to detect clusters and
  related documents.
\item
  The in-degree in order to check if there are documents that are
  related to a lot of documents and if those documents are more general
  than others, and will not necessarly belong to one specific cluster.
\end{itemize}


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_36_0.png}\end{center}
        \caption{Intersection between references graph}
        \label{intersection}
    \end{figure}
    
    \hypertarget{second-graph-relations-between-references}{%
\paragraph{Second graph: Relations between
references}\label{second-graph-relations-between-references}}

The second graph relates more to the referenced documents and is not
about the documents themselves. The aim of this graph is to see if there
are documents that are more likely to be cited together. If this is the
case, then these documents are likely to be about the same subject. This
weighted graph is constructed as follows: for each document, we create a
link between the references of the document. We create a dictionnary of
links, for each new document, if two references of the latter have
already a link in the dictionnary then we increase the weight of the
link. For instance, if two references are present together in the 5
distinct documents, then the weight of the edge would be of 5. We
obtained a graph of 10845 nodes and 148594 edges but we filtered the
edges that have a weight higher than 4 and we get the graph below
represented in this figure \ref{fig6}




    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_40_0.png}\end{center}
        \caption{Relations between references filtered}
        \label{fig6}
    \end{figure}
    
    We can directly select from the interactive graph, which can be found in
the jupyter notebook corresponding to this report, the nodes with degree
higher than 100. These nodes represent the most cited documents, but we
will construct a subgraph to specificly tackle the most important
referenced documents. On this graph, to be able to detect similar
documents we also run the Strongly connected components algorithm. On
top of this, we inspected the documents with higher weights. We will
explain more about what we extracted from this graph in the result
section.

    \hypertarget{third-graph-exploratory-references-graph}{%
\paragraph{Third graph: Exploratory references
graph}\label{third-graph-exploratory-references-graph}}

With this graph we aimed to find the most cited documents, the documents
that are the most influencial and how citations evolve over time. This
is a directed graph constructed in the following way: for each document,
if the document cites another document, then there is a link between the
document and the cited one. We get a graph of 11650 nodes and 13243
egdes. We also constructed a subgraph of this graph, which is a
co-citations in conferences, but do to a lack of data, we found only
three documents that were cited by other documents.

On this graph, we run \textbf{\emph{PageRank}} algorithm to be able to
extract the most influential documents.

\textbf{PageRank (PR)}* is an algorithm originally used by Google Search
to rank web pages in their search engine results. In our case, PageRank
is a way of measuring the importance of referenced papers. Compared to
the in-degree analysis we get almost the same results. This is because
almost all nodes have a zero out degree.



    \hypertarget{co-authorship}{%
\subsection{Co-authorship}\label{co-authorship}}

To get insight into collaborations within the community we built graphs
based on co-authorship of papers. To build the graph structure and
visualize it the python package \emph{networkx} was used. When
constructing the graph, we associate qualitative features --such as the
name and institution of the author that is represented by a node-- to
the nodes and edges of the graph. We also associate the number of
collaborations between different parties to the graphs as the weight of
the edge.

\hypertarget{graph-construction}{%
\subsubsection{Graph Construction}\label{graph-construction}}

First, we built a co-authorship graph based on individual authors. Each
node represented an author, and two nodes were connected if the authors
collaborated on a paper. Other works \cite{cheong2009social},
\cite{hesford2006management} have used a directed graph that only adds a
link from the first author of the paper to all other authors. However,
as we were more interested in collaboration patterns and less in finding
the most influential first author, we built an undirected graph with
edges between all co-authors. On the downside, it yielded a very dense
graph. Hence, we pruned the initial graph by removing nodes that did not
satisfy additional conditions. Intuitively it is clear that a singular
collaboration on a paper does not necessarily indicate that two people
even know each other. A first condition was to link two nodes together
only if the authors corresponding to each node appeared as co-authors in
more than one conference. The graph produced by this approach was much
more sparse and contained much fewer authors. Additionally, all
connected components contained fewer than 35 authors, making it possible
to consider each author individually. Such a component is shown in
figure \ref{fig_plot}.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_46_0.png}\end{center}
        \caption{One of the connected components of the co-authorship graph}
        \label{fig_plot}
    \end{figure}
    
    A second alternative approach was to consider the strength of bounds. In
this graph authors are only linked if they have written at least 2
papers together. This method yielded a much larger component
--containing 441 authors-- as this second condition was weaker. To get
an understanding of author to author relations in this component we
needed to further prune away parts of the graph that were not
insightful. When we looked at individual author within the community, we
were interested in collaborative individuals. Hence we removed nodes
that had fewer than 3 connections to other nodes. The resulting graph
can be seen in figure \ref{author_collab_large}. Details creating such a
plot can be found in the section dedicated to this below.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_48_0.png}\end{center}
        \caption{Largest Component}
        \label{author_collab_large}
    \end{figure}
    
    Focusing in on the authors is not the only way to interpret the
co-authorship graph. One should also consider the overall structure of
the graph. As the co-authorship graph is not connected, and contains one
large main component with 1141 authors and 208 very small components
with less than 25 authors in them we focused on analyzing the structure
of the main component of the graph. First we aimed to detect
sub-communities within the graph. Modularity is one measure used to find
such communities. It measures the connection strength within a community
relative to the strength of outgoing connections. Networks with high
modularity have dense connections between the nodes within the
subcommunites but sparse connections between nodes in different
subcommunites. A non-parametric algorithm for community detection is the
Louvain Method \cite{blondel2008fast}. We used the implementation of
Thomas Aynaud \cite{louvain}. This gives a good partitioning of the
graph into 26 communities of median size 42.84. This is quite a large
number of communities, so we considered an alternative way to yield
bigger partitions. For this we looked at normalized cuts, which
partition the nodes into groups with many within-group connections and
relatively few between-group connections. The advantage of this method
is that we can choose how many cuts to make, and we are thus able to
define the size of subcommunities. Finding normalized cuts in a graph
can be done using \emph{sklearns} spectral clustering algorithm by
passing it the adjacency matrix of the graph. With this method we are
able to neatly separate the main component graph in three large
communities, as can be seen in figure \ref{normalized}

    To further analyze these subcommunities we looked at the distribution of
nationalities and institutions over the years. Additionally, to
understand differences in the research being conducted, we analyzed the
frequency of keywords used for the papers that were writing within that.

To get a better overview of how co-authorship happens across countries
and institutions, we built graphs where every node represented a
country, respectively an institution. Two countries or institutions were
linked if two authors from these two institutions collaborated on a
paper together. The country graph is simple enough that it does not have
to be pruned. The institution graph on the other hand is again not
comprehensible without further processing. Thus, similar to the first
approach on pruning presented, we only considered institutions who
collaborated in at least two conferences. This the main component of
this graph is shown in figure \ref{unigraph}.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_51_0.png}\end{center}
        \caption{Different formats used to reference institution}
        \label{unigraph}
    \end{figure}
    
    \hypertarget{numerical-analysis-of-graphs}{%
\subsubsection{Numerical analysis of
graphs}\label{numerical-analysis-of-graphs}}

To analyze the structure of the graphs themselves we calculated the
following measures:

\begin{itemize}
\tightlist
\item
  Diameter: longest shortest path in the graph.
\item
  Average clustering coefficient: measure of how much the nodes in a
  graph tend to cluster together. Average over all clustering
  coefficients. The clustering coefficient of a node is the fraction of
  triangles prassing through the node over all possible triangles
  through that node \cite{watts1998collective}.
\item
  Density: the number of edges in the graph divided by the maximum
  number of possible edges for a graph with the same number of nodes
\end{itemize}

    \hypertarget{plotting-of-co-authorship-graphs}{%
\subsubsection{Plotting of co-authorship
graphs}\label{plotting-of-co-authorship-graphs}}

Graphs were visualised with the built in plot functions and graph
layouts of networkx. The force directed layout was used as it clusters
together nodes with strong connections. As this layout is produced by
iterations and based on random initial conditions, we set a fix random
seed to be able to reproduce the positioning. The size of each node was
changed based on the position of the node in the graph. Central nodes
with a lot of connections should be larger, while nodes with fewer
connections should be smaller. To achieve this we used the measures of
degree centrality for graphs with few nodes and edges and betweenness
centrality for larger graphs. Degree centrality measures the fraction of
degrees (number of connections) a node has with respect to the total
number of connections in the graph. Betweenness centrality of a node is
the sum of the fraction of all-pairs shortest paths that pass through it
\cite{borgatti2014social}. We found that degree centrality is more
appropriate for smaller graphs as their structure is still easy to
interpret. As by construction betweenness takes into consideration the
degrees of it neighboring nodes, it better structures the nodes large
graphs into central and less central nodes. The width of the edge is
based on the weight of the edge, which in turn reflects the number of
collaboration between two nodes.

    \hypertarget{creating-interactive-visualizations}{%
\subsection{Creating Interactive
Visualizations}\label{creating-interactive-visualizations}}

To better understand and interact with the data we created interactive
graphs on top of the static graphs. Many libraries exist to allow for
interaction. While working on the project, we worked with \emph{bookeh},
\emph{holoviews}, \emph{mpld3} and \emph{plotly}. For some time
implementing a custom visualization using \emph{D3} was also
contemplated. However, functionality provided by plotly is most general
and allows to easily build interactive figures, while not being as heavy
as bookeh and holoviews. Additionally, it is the most well maintained.
D3 requires a running server to be viewed and has a hard time dealing
with large graphs. To plot figures we created them in networkx to grab
the positioning of the nodes. Then we passed this positioning to plotly
to plot.

    \hypertarget{results}{%
\section{Results}\label{results}}

In this section we present first insights gathered during the
Bibliographic analysis.



    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_57_0.png}\end{center}
        \caption{Global spread of community. Size of dot represent number of participants from each institution}
        \label{global_com}
    \end{figure}
    
    \hypertarget{community-composition-structure}{%
\subsection{Community Composition \&
Structure}\label{community-composition-structure}}

Participants from 372 institutions participated over the last 4 years,
with a median of 128 institutions per year. The number of institutions
participating increased each year, with a noticeable peak in 2018, with
participants from 261 institutions. This is a considerable increase from
the last conference ICLS in 2016 which had 135 participating
institutions. Most participants are from the US, with a average of 50\%
of all participants being from the US. We could also observe a trend of
increase in US participants with only 35\% of US participants in 2015
and 51\% in 2017, followed by 64\% in 2018. Overall, participants from
41 nations have participated over the 4 years, with an average of 28
nations represented at each conference. After the US, participants of
Germany, Canada and Singapore make up the biggest proportion of
participants \ref{figrep}.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_59_0.png}\end{center}
        \caption{Number of Contributiors by coutry, excluding the US}
        \label{figrep}
    \end{figure}
    
    \hypertarget{collaboration}{%
\subsubsection{Collaboration}\label{collaboration}}

\textbf{International Collaboration:} The U.S. again stands out as a
major player, with most collaborations happening between the U.S. and
other countries. Out of 851 papers, 153 were international
collaborations. Out of these 153, 108 involved at least one U.S.
participant. Excluding US participation we find the collaboration graph
presented in figure \ref{figcollab}. We note that some countries, such
as Japan, are not present in this graph. This means that all
international collaboration of these countries, in this case japanese
authors, also involves American authors. Additionally we observed that
many Asian nations seem to collaborate almost exclusively with each
other or Canada.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_61_0.png}\end{center}
        \caption{Collaborative papers between countries excluding US}
        \label{figcollab}
    \end{figure}
    
    \textbf{Inter-institution collaboration}: In regards to collaboration
between institutions, 318 out of the 815 papers were written in
collaboration of authors from different institutions. The most
collaborative university is the University of Indiana at Bloomington,
followed by the University of Toronto. Considering the graph of
university collaboration, we saw that none of the nodes is solitary -
all institutions in the conference have collaborated with another
institution at least once. There is a large main component and 6 other
smaller clusters of universities. The cluster of main institutions
collaborating in two conferences can be seen in figure \ref{unigraph}.
We see that the topology of the cluster is rather odd, and that while
some strong links exist, most links are weak.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_63_0.png}\end{center}
        \caption{Subcommunities via normalized cuts in largest component of co-authorship graph}
        \label{normalized}
    \end{figure}
    

    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_64_0.png}\end{center}
        \caption{Subcommunities by paricipation in CSCL. Blue: no participation in main compnents of CSCL. Red and orange each mark participation in one of the two main components of CSCL}
        \label{CSCL_components}
    \end{figure}
    
    \textbf{Author collaboration}: As the graph yielded by only considering
repeat collaboration during different years is sparse, containing
connected components of size at most 31, we deduce that repeat
collaboration over the years is rather infrequent. As the graph
conditioned on multiple collaboration is more dense (having a connected
component with more than 400 authors), we deduce that when multiple
collaborations with the same authors happen, they yield work submitted
at the same time.

The fact that normalized graph cuts will split the community into three
promted further investigation. As will be discussed in the next section,
the graph of authors that participanted in CSCL yields two large
connected components. By coloring the authors contained in the two CSCL
components different colors, and authors in neither a third color, we
find a similar partitioning of the graph, as can be observed in figure
\ref{CSCL_components}. This is somewhat trivial, as the graph is built
based on collaboration in these conferences. However, the analysis of
nationalities present in the graph yielded an interesting results. All
Asian participants from nations other than Singapore are contained in
the same cluster. We also note that one of the CSCL clusters is closer
to the group of ICLS-only participants than the other. Additionaly,
through normalized splits, we find that the section of the normalized
cut containing ICLS only authors is overwhelmingly american.

Considering the wordclouds in figure \ref{c1}, \ref{c2}, \ref{c3} of the
most common keywords in each of the large splits, we find a first
indication that the splits also reflect some thematic differences in
research.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_66_0.png}\end{center}
        \caption{Keywords of the blue Community}
        \label{c1}
    \end{figure}
    

    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_67_0.png}\end{center}
        \caption{Keywords of the red community}
        \label{c2}
    \end{figure}
    

    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_68_0.png}\end{center}
        \caption{Keywords of the orange community}
        \label{c3}
    \end{figure}
    
    \hypertarget{icls-and-cscl}{%
\subsubsection{ICLS and CSCL}\label{icls-and-cscl}}

In our analysis, we could find many structural differences between the
two conferences. Firstly, ICLS is much larger and american dominated
than CSCL. We also observed differences in the most cited papers and the
average number of authors of a paper. By constructing the graph based on
collaboration of authors in each conference, we find that while ICLS
contains one giant component, CSCL contains two large main components.
All these results are however to be taken with a grain of salt, as for
each conference we only have data for two years, and the year 2018 is a
marked outlier. Additionaly, we are unable to find a strong difference
between the metrics of the graphs, that is diameter, density, clustering
coefficient and average path length.

    \hypertarget{movement-of-particpants}{%
\subsubsection{Movement of particpants}\label{movement-of-particpants}}

Using the data we have of participants over the years, and knowing their
affiliation, we can now find members that change institutions over the
years. Over the years of recording we find 14 participants that have
changed countries and 47 that have moved institutions. We can see that
most movement occured within the US respectively Europe, see figure
\ref{fig_move}.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_71_0.png}\end{center}
        \caption{Movement of participants across institutions}
        \label{fig_move}
    \end{figure}
    
    \hypertarget{changes-over-the-years}{%
\subsection{Changes over the years}\label{changes-over-the-years}}

We can observe a growth of the number of participants over the years,
and by building co-authorship graphs for each year of the conference, we
observe that the density and clustering coefficient of the graph
indicate a steady decrease, while the diameter and average shortest path
length tend to increase. This hints that the community is attracting new
member that do not collaborate with the established group of
participants. But, as the years 2018 shows a massive increase of
participation, in number of participants and number of institutions
represented, as well as a large decrease in density of the graph --which
is reduced by more than half from the previous ICLS conference-- it is
impossible to draw any statisically significant conclusions.

    \hypertarget{clustering-and-referecences-graphs-results}{%
\subsection{Clustering and referecences graphs
results}\label{clustering-and-referecences-graphs-results}}

The distribution of referenced publications years are plotted in the
graph \ref{count_ref_docs}. We can see from the graph that there is a
kind of a trend, each two, three years there is a pick of documents
referenced followed by a two or three years of stagnancy. We can also
notice that throughouts the year each pick is higher than the older one,
which can be explained by the fact that the International Society of the
Learning Sciences community is investigating more and more tools to
facilitate the learning over the years.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_74_0.png}\end{center}
        \caption{Distribution of the number of referenced documents belonging to each year}
        \label{count_ref_docs}
    \end{figure}
    
    Since the number of documents is increasing over the years, we also
wanted to investigate more if the relevance of all the documents
published is increasing too. Using Pagerank algorithm and the In-degree
of each nodes to get the most influencial documents referenced of the
exploratory citations graph that we constructed before. We filtered the
first 100 documents and then contructed a histogram of the counts by
year. We obtain the figure \ref{fig7}:


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_76_0.png}\end{center}
        \caption{Distribution of the 100 most referenced documents belonging to each year}
        \label{fig7}
    \end{figure}
    
    We can see that in the top 100 cited documents more than 12\% are from
2006, 8\% are from 2013, 6\% from 2014\ldots{} We can not say that we
have the same trend as before, and the relevance of the documents is
increasing in the same way than with the number of publications but what
we can say is that more than 50\% of the most cited documents are from
2006 until 2016, which is 10 years. The other half is between 1977 until
2005 which represents more than 25 years. On top of that, 2016 is the
most important year in term of publications and maybe some documents did
not have enough time to be cited yet.

    The top 10 of the most influencial and most important referenced papers
are represented in table \ref{indegree}:


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_79_0.png}\end{center}
        \caption{In-degree count for each document}
        \label{indegree}
    \end{figure}
    
    We also noticed from the first graph that some documents were more
likely to cited together. We can say that the more two documents are
likely to be cited together the more it is probable that both documents
are about the same subject. From the table \ref{fig8} we can also notice
that some papers are more likely to cite distinct publications of the
same author, which is logic in a sense, because if some authors always
treat about the same subject then they are experts in that field and a
lot of their publications are relevant and can be cited together.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_81_0.png}\end{center}
        \caption{Count of documents cited together}
        \label{fig8}
    \end{figure}
    
    Using this graph, we also want to extract if there are some clusters of
papers cited together. As we explained before, we run strongly connected
algorithm to check if some cited documents are more likely to be cited
together, and this will give us clusters of cited documents that are
more likely to be about the same subjects. We can visualise one cluster,
the most important one in figure \ref{fig12}.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_83_0.png}\end{center}
        \caption{Table of the documents belonging to the same clusters}
        \label{fig9}
    \end{figure}
    


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_85_0.png}\end{center}
        \caption{Fully connected references graph}
        \label{fig12}
    \end{figure}
    
    The largest fully connected component is the composed of 8 nodes and 56
edges plotted in figure \ref{fig12}. These documents are all cited
together more than 4 times. You can investigate the edges and the
weights in the interactive graph constructed above. We wanted all the
documents to be linked to each others to be more precise, but if we only
look to giant connected component, we will get a network of 27
referenced paper with 142 egdes represented in this graph \ref{fig15}.


    In the table \ref{fig10} below are the documents contained in the giant
connected component represented in the interactive graph \ref{fig15}.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_89_0.png}\end{center}
        \caption{Giant component graph}
        \label{fig15}
    \end{figure}
    

    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_90_0.png}\end{center}
        \caption{Documents belonging to the giant component}
        \label{fig10}
    \end{figure}
    
    We wanted to apply the same logic with the papers in-conferences. So we
focused on the analysis of the first graph that uses the intersection
between the referenced documents of each two by two papers. The length
of the intersection set had to be more than 4.

So if more than 4 documents are referenced by two distinct documents
than the latter are more likely to be about the same subect. In the
table \ref{fig11} we can find some of the papers that belong to the same
clusters.


    \begin{figure}
        \begin{center}\adjustimage{max size={\linewidth}{0.4\paperheight}}{paper_files/paper_92_0.png}\end{center}
        \caption{Documents in conferences belonging to the same clusters}
        \label{fig11}
    \end{figure}
    
    When applying this kind of clustering we found that most of the clusters
contains almost the same authors with different papers, which is logic
because if they are specialists in one topic and write several papers on
it, they might reuse some references in several documents.

    \hypertarget{conclusions}{%
\section{Conclusions}\label{conclusions}}

In summary, we were able to extract how the dominant some institutions
and countries are within the conference. Some initiall collaboration
patterns between countries, instututions and authors were extracted.
Subcommunities found within the network of co-authorship reflect the
division between conferences and within the conference. Using the
analysis of the affiliation over the years we followed the migration of
participants accross institutions. Natural language preprocessing and
metadata parsing enabled the extraction of trends of keywords over the
years

Overall, due to lack of data, we were often unable find non trivial
results; This lack of data is exasperated by the fact that the
conference of 2018 presents as an outlier in the dataset. Additionally,
taking document clustering for instance as an example, network analysis
of references were more accurate than(NLP) regarding documents and
references clustering.

We thus strongly recommend adding additional conferences to the dataset.
\newpage

    % Add a bibliography block to the postdoc
    
    
\bibliographystyle{unsrt}
\bibliography{references}

    
    \end{document}
