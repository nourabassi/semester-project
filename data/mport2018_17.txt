Toward a Taxonomy of Team Performance Visualization ToolsZachari Swiecki, University of Wisconsin, Madison, swiecki@wisc.eduDavid Williamson Shaffer, University of Wisconsin, Madison, dws@education.wisc.eduAbstract: Research on teams has become increasingly important due in part to the status ofcollaborative problem solving as a vital 21st century skill. Much of this research has focused onfactors that affect team processes and outcomes, such as the use of team performancevisualization tools. Such tools are also valuable to researchers who make inferences from teamdata or educators who assess teams and plan interventions. This variety of users suggests thatstudying these tools requires a user-centered approach focusing on affordance relationships. Inthis paper, we use Epistemic Network Analysis to create a visual representation of the space ofaffordance relationships for extant team performance visualization tools. We use this space tocompare tools, and to demonstrate empirically the dimensions along which they differ. Thesedimensions suggest a preliminary taxonomy of tools in terms of their affordance relationshipsfor different users.IntroductionTeams are collections of individuals who work together to learn, solve problems, make decisions, and designproducts. Recently, research on teams has become increasingly prevalent due to their ubiquity and growingimportance in society. Moreover, collaborative problem solving—and collaboration more generally—has beenrecognized as a vital 21st century skill (Griffin & Care, 2014). Much of the extant research on teams has focusedon factors that affect team performance, which may refer to either team outcomes or the processes teams take toachieve those outcomes. One such factor is how tool use by teams affects their performance. In particular, agrowing body of research has focused on how visualizations of team performances affect team outcomes andprocesses (e.g., Fiore & Wiltshire, 2016; Janssen & Bodemer, 2013).Team performance visualizations are not only important for teams, however. Given the complexity andvolume of team interaction data that can be collected, visualizations also play an important role in how researchersinvestigate teams and how educators monitor and assess them. This variety of users, and their differing goals,suggests that those who study team performance visualizations should take a user-centered approach. Such anapproach would consider both the properties of visualization tools and those who use them—in other words, itwould consider the affordance relationships that exist between these tools and their users (Norman, 2013).Several prior studies have investigated features of team performance visualization tools, such asawareness tools and dashboards (e.g., Bodily & Verbert, 2017; Janssen & Bodemer, 2013; Verbert, Duval, Klerkx,Govaerts, & Santos, 2013). However, these studies are lacking either because they do not focus on team dataspecifically, they ignore certain user groups, they ignore large classes of tools, or they do not consider how theaffordances of tools relate to one another. To address this gap, we conducted an empirical investigation of extantteam performance visualization tools and derived a preliminary taxonomy in terms of their affordancerelationships with three user groups: teams, researchers, and educators. Such a taxonomy should help to betterdistinguish tools and provide guidance to different users. To analyze and visualize our data, we used EpistemicNetwork Analysis, or ENA, (Shaffer, Collier, & Ruis, 2016) a method for analyzing the connections betweenfeatures of interest in data, and thus a powerful technique for investigating the collections of affordances that existin tools.TheoryIt is widely recognized that the tools we use affect how we think and act (Hutchins, 1995; Vygotsky, 1978).Several fields, including the learning sciences, social psychology, and organizational psychology, haveinvestigated tools in relation to teams. Much of this work has focused on the effects of different visualization toolson team performance. For example, those who study Computer Supported Collaborative Work have investigatedhow shared representations, such as digital whiteboards, affect team performance (Fiore & Wiltshire, 2016). Andin Computer Supported Collaborative Learning, researchers have studied how different visualization tools affectcommunication (Munneke, Andriessen, Kanselaar, & Kirschner, 2007), the construction of joint problem spaces(Roschelle & Teasley, 1995), and participation (Janssen & Bodemer, 2013).Team performance visualization tools—and tools more generally—affect teams through what Norman(2013) calls affordances, or particular relationships between the properties of tools and the capabilities of theirusers. Several authors have investigated the affordance relationships that exist between team performanceICLS 2018 Proceedings144© ISLSvisualization tools and teams. For example, Janssen and Boedmer (2013) argue that cognitive group awarenesstools, which provide information about the distribution of team knowledge, help to coordinate social and cognitiveactivity by facilitating communication and the development of shared mental models. Similarly, social groupawareness tools, which provide information about the activities of team members such as communication levelsor contributions to tasks, help teams to coordinate action and information in situations where they cannot directlysee one another’s behavior. These awareness tools may also allow team members to identify and directly comparetheir performance to that of their teammates, creating opportunities for social comparison. Other tools mayocclude individual contributions, instead showing metrics of team performance only at the team level (Kimmerle& Cress, 2008; Kimmerle, Cress, & Hesse, 2007).While it is clear that tools affect how teams perform, they also affect how researchers measure teamperformance. To understand and make claims about teams, researchers design assessments to collect evidencethat warrant those claims (Mislevy, 1996). However, teams are dynamic and complex systems. They have multilevel structure, individuals interact with one another and with tools, and these interactions change over time. Thecombination of these features means that evidence elicited from teams is both vast and complex. For example,many researchers study teams who work together in virtual environments. Such environments log data oninteractions within the environment, including mouse clicks, verbal or chat communications, and productsubmissions. The complexity of these data has led many researchers to use visualization tools in order to makemeaning from the evidence they collect. For example, researchers have used a variety of network tools to visualizeboth the structure of team interactions and the semantic content of those interactions (e.g., Dawson, Tan, &McWilliam, 2011; Sha, Teplovs, & van Aalst, 2010). Temporal visualization tools, such as CORDTRA, havebeen used to analyze the sequence and simultaneity of team interactions (Hmelo-Silver, Jordan, Liu, &Chernobilsky, 2011). And dependency visualizations, such as those derived from state transitions, Markovmodels, and process models, have been used to investigate the likelihood that team actions follow or depend onone another (Reimann, P., Frerejean, J., & Thompson, K., 2009).Furthermore, given the volume and complexity of team data, educators have come to rely on tools thatconvey information about teams succinctly in real-time. Consequently, learning analytic dashboards have beendesigned to provide educators with information about team activity and performance in condensed views (Verbertet al., 2013). Many of these dashboards, such as the Process Tab (Shaffer, 2017) , which uses network diagramsto represent the connections teams make between concepts as they collaborate, allow educators to access the dataunderlying the visualizations to see more detailed information on demand.Each of the properties described above—knowledge, activity, member identification, structure,semantics, temporality, dependence, and details—are features of team performance visualization tools that afforddifferent actions for different users. Teams use these affordances to facilitate communication and compare theirperformance; researchers use them to test hypotheses and build theories; and educators use them to assess studentsor plan interventions. Importantly, the affordances of any given tool do not exist in isolation. For example, conceptmaps, one kind of cognitive group awareness tool, represent information about team knowledge and informationabout the structure of that knowledge—that is, how knowledge components relate to one another. In addition,dashboards typically contain a variety of visualizations of team performance. One dashboard, Collaid (MartinezMaldonado, Kay, Yacef, & Schwendimann, 2012), contains at least five visualizations, including radar graphs,pie charts, and time series, which simultaneously provide information about how team activities change over time.In other words, team performance visualization tools are interactive systems in which different affordances relateto and affect one another. Thus, understanding and describing the affordance relationships between different toolsand their users requires a method of analysis focused on how affordances connect to one another. One such methodis Epistemic Network Analysis, or ENA (Shaffer, Collier, & Ruis, 2016).ENA measures connections among relevant features in data and represents those connections asnetworks. ENA was developed to model cognitive networks, or networks that represent associations betweenelements of complex thinking. This method has been used to study the patterns of connections that teams oflearners make as they solve problems in simulations of professional practice (e.g., Shaffer et al., 2016). Thus,ENA itself is a team performance visualization tool. However, ENA is also a general method for investigatingdata where complex patterns of relationships are thought to exist. In this study, we use ENA to create a visualrepresentation of the space of affordances for extant team performance visualization tools. Using this space, wecompare tools, and also demonstrate empirically the dimensions along which they differ.Several prior studies have investigated the affordances of team visualization tools for different usergroups. However, each case has important limitations. For example, Verbert and colleagues (2013) conducted areview of over 20 learning analytics dashboards. These authors classified each dashboard according features suchas target user (teacher or student) and data source (social interaction, time spent, and so on). This review accountedICLS 2018 Proceedings145© ISLSfor two kinds of users; however, it was limited to one particular kind of visualization tool and did not specificallytarget dashboards that represent team performance data.Taking a team focus, Janssen and Bodemer (2013) reviewed the literature on group awareness tools.These authors identified the kind of data represented by each tool, how the data was collected, and characteristicsof the visualizations used. This work explicitly looked at tools designed for teams as the target user, and itconsidered several affordances of these tools for teams. However, other aspects of the visualizations consideredtended to focus only on surface level properties and not the interactions they afforded. Moreover, this review didnot consider tools designed for researchers or educators.Each study above contributes toward understanding the affordance relationships between teamperformance visualization tools and their users. However, they either ignore certain users, ignore large classes oftools, or do not focus on teams. Work discussing team performance visualization tools for researches is alsodeficient in that it often only implicitly demonstrates their affordances. That is, this work shows what these toolscan do in terms of providing insights into team performance or building theory, but they do not explicitly discussthe properties of the visualizations that make these insights possible. And while some work is more explicit inthis regard (e.g., Siebert-Evenstone et al., 2016), it is rare that authors discuss a variety of these visualizationstogether in one place.In this paper, we address the limitations of prior work by conducting an empirical investigation of theaffordances of extant team performance visualization tools. In particular, we examine the literature on these toolsand ask the following two research questions: (1) what are the affordances of extant tools? And, (2) how do theseaffordances differ across user groups? This analysis provides some first steps toward a taxonomy of tools thatshould guide future work and help users select appropriate tools given their goals.MethodsDataTo address the research questions above, we first collected a representative sample of papers that described teamperformance visualization tools and their affordances. To collect this sample, we consulted a panel of three domainexperts who have extensive experience in measuring and visualizing team performance. These experts suggestedspecific papers in which team performance visualization tools were described, as well as topics for a literaturesearch. Using these papers, their citations, and literature search topics, we collected a sample of 41 papers andbook chapters, which were reviewed and approved by the expert panel. From this sample, we identified 48 teamperformance visualization tools for this study.CodingTwo raters classified the tools described in each paper into one of six types: (1) social group awareness tools, (2)cognitive group awareness tools, (3) dashboards, (4) networks, (5) dependency visualizations, and (6) temporalvisualizations. The distribution of tools by type is shown in table 1.Table 1: Counts of team performance visualization tools by typeTypeSocial Group AwarenessCognitive Group AwarenessDashboardsNetworksDependency VisualizationsTemporal VisualizationsCount171111432Next, the two raters coded each tool for their intended user and affordances. The user categories includedfour codes: (1) team, (2) educator, (3) team and educator, and (4) researcher. Tools coded for the first threecategories were described in papers that explicitly mentioned the end user of the tool. Tools coded for theresearcher category either did not mention the end user of the tool, or only reported on the analysis of teamperformance data using a given tool. In total, 31 tools were coded for teams as the intended user, 3 for educators,5 for teams and educators, and 9 for researchers. We developed the visualization affordance codes from agrounded analysis (Glaser & Strauss, 1967) of the data. We define these codes in table 2.ICLS 2018 Proceedings146© ISLSTable 2: Visualization affordance codesCodeActivityKnowledgeStructureSemanticsDependenceTemporalityMemberIdentificationDetailsDefinitionRepresents information about the activity or participationof the team, e.g., how much they are communicating orcontributing to the task.Represents information about the knowledge, opinions, orunderstandings held by the team.Represents the structure of teams, their interactions, ortheir knowledge, e.g., subgroups, communication patterns,or connections between concepts.Represents the content of team interactions orknowledge—rather than, or in addition to—quantity orstructure.Represents information about how team actions depend onother team actions, e.g., the likelihood that one actionfollows another.Represents information about the simultaneity, sequence,or change of team actions or knowledge over time.Contributions from each individual team member areidentifiable within a single visualization in the tool.Allows users to access the data underlying thevisualization.ExampleCollaid dashboard: MartinezMaldano et al., 2012Concept maps: Engelmann &Hesse 2011Social network graphs: Dawson& McWilliam, 2011KSV: Sha et al., 2010.Dependency graphs: Reimannet al., 2009.CORDTRA: Hmelo-Silver etal., 2011.Social awareness tool:Kimmerle et al., 2007.Process Tab: Shaffer, 2017.Two raters coded each visualization tool for the type, user, and affordance codes described above. Giventhe relatively small sample size, both raters coded the entire data set together, resolving differences as they coded,and arriving at a single decision for each code. Thus, the kappa statistic for this coding was 1.AnalysisAfter coding, we analyzed the data using ENA. ENA measures connections among codes in data and representsthem as networks. In this study, we used ENA to construct network models for each of the 48 team performancevisualization tools. Here, the units of analysis are the tools, the network nodes correspond to the affordance codesin table 2, and a network for a given tool represents its collection of affordances.The process of creating ENA models is described in more detail elsewhere (Shaffer et al., 2016), but inbrief, ENA creates adjacency matrices for each unit of analysis that quantify the co-occurrences of codes withinthe unit. Next, the adjacency matrices are normalized and represented as vectors in a high dimensional space,where each dimension corresponds to a co-occurrence of codes. A dimensional reduction via singular valuedecomposition is then performed to project the vector for each unit of analysis into a lower dimensional spacethat maximizes the variance accounted for in the data. Finally, the nodes of the networks are placed in the spaceusing an optimization algorithm such that the center of mass for a given unit’s network closely corresponds tothat unit’s location in the lower dimensional space. Importantly, these nodes placements are fixed, meaning thatthe nodes of each network are in the same place for all units in the analysis. This fixed set of node positions allowsfor meaningful comparisons between units in terms of their connection patterns and allows us meaningfullyinterpret the dimensions of the space.The final result is two coordinated representations for each unit of analysis: (1) a plotted point, whichrepresents the location of that unit’s network in the low-dimensional space, and (2) a weighted network graph.Because the location of any plotted point corresponds to the center of mass of its corresponding network, we canuse the weighting of the network to explain that point’s location. Thus, plotted points located on the right handside of the space will have networks whose strongest, or most heavily weighted, connections appear on the right.Similarly, plotted points located on the left will have networks whose strongest connections appear on the left,and so on. Using ENA, we can also group plotted points by metadata and calculate the mean position in the spacefor a group. This mean can be plotted in the same metric space and has a corresponding network graph whichrepresents the average connections between codes for that group. In this analysis, we grouped the visualizationtools in two ways: by type of tool (table 1) and by user. In both cases, to make comparisons between groups weused their mean position in the space, their mean network diagrams, and network difference graphs, which subtractthe edge weights of two networks to show the strongest connections in each group.ICLS 2018 Proceedings147© ISLSResultsIn this study, we used ENA to investigate (1) the affordances of extant team performance visualization tools and(2) how those affordances differ across user groups. With regard to the first, in the figures below we see the resultsof the ENA analysis for the six types of tools described above. Fig 1 shows the mean plotted points (coloredsquares) for each type of tool, as well as the overall mean point, in ENA space. In the figure, we have also includedthe network for the overall mean, which helps to interpret the dimensions of the space. For the mean network infigure 1, the thickness and saturation of a given connection reflects the average number of tools coded for bothcodes defining the connection.Fig 1. Mean plotted points and overall mean network for the six tool types. Legend on the right.Because of the relationship between network representations and plotted points in ENA, we can use thisnetwork layout to interpret the dimensions of the space, and in turn, the meaning of each point’s location. Thus,the Y dimension separates visualization tools in terms of their focus on more complex data transformations, suchas showing temporality, dependence, or structure (top), versus less complex transformations like showingindividual contributions or underlying data (bottom). On the other hand, the X dimension separates visualizationtools in terms of their focus on social information about each team member, such as their activity levels (left),versus more cognitive information, such as the knowledge held by the team (right).The interpretation of the dimensions above helps to explain the location of tools in the space, as well asthe differences between them. For example, in the figure, we see that the mean location of social and cognitiveawareness tools in the space is quite different. To highlight this difference, in figure 2 we constructed a differencegraph showing which connections, on average, are stronger in one kind of tool versus the other. In particular, thisgraph shows that cognitive awareness tools (red) are toward the right and lower down in the space due to theirfocus on representing team knowledge, underlying data, and the contributions of all members on the team.Contrastingly, social awareness tools (blue) are toward the left and higher up due to their focus on representingthe social activity of each team member and how that activity changes over time.Fig 2. Network difference graph for social (blue) and cognitive (red) awareness tools.ICLS 2018 Proceedings148© ISLSPage limitations do not allow us to include the network diagrams like those in figures 2 for each type oftool; however, the defining affordances of each tool type can be inferred from the location of their mean plottedpoints and the interpretation of dimensions described above. For a more detailed account, we examined theseparate networks for each type of tool and found that network visualizations (purple, figure 1) are defined bymore complex representations of the structure and semantics of team activities and knowledge. Similarly,dependency visualizations (in orange) are defined by the representation of those same features, with the additionof information about how team activities or knowledge components depend on one another. Temporalvisualizations are defined by representations of temporality and the semantic content of team activity andknowledge. Finally, dashboards are defined by representations of how the activity of team members change overtime.With regard to our second research question, figure 4 below shows the results of the ENA analysis ofvisualization tools grouped by their intended user. Thus, the colored squares in this figure correspond to the meanplotted point positions of tools grouped by user, whereas in figure 1 above, the colored squares correspond to themean plotted point positons of the tools grouped by visualization type. Here again, we include the overall meannetwork diagram to aid in interpretation. Because only the groupings of tools have changed, the interpretation ofthe dimensions remains the same as above.Fig 3. Mean plotted points and overall mean network for tools by user. Legend on the right.Figure 3 shows that, on average, visualization tools designed for teams (in blue), focus on representinginformation about the activity and knowledge of each individual on the team, as well as the data underlying thevisualizations. On the other hand, tools designed for use by researchers (in red) focus on representing morecomplex information such as temporality, structure, and dependence.Tools designed for educators or teams and educators (green and purple respectively) are located belowand to the left of researcher tools, meaning that they focus more on the activity of team members and underlyingdata, rather than more complex team performance data. However, their location on the Y axis is higher than toolsdesigned for teams, meaning that they focus relatively more on complex data than team tools, on average.DiscussionThe ENA analysis above suggests a preliminary taxonomy of team performance visualization tools in terms oftheir affordances. The dimensions of the ENA space distinguish between those tools that focus on representationsof social versus cognitive information on the X axis, and those tools that focus on more or less complextransformations of team performance data on the Y axis. These dimensions define four broad categories withwhich we can classify the different tools: complex/cognitive, complex/social, simple/social, simple/cognitive.Thus on average, network, dependency, and temporal visualizations fall into the complex/cognitive category;dashboards fall into the complex/social category; social awareness tools into the simple/social category (thoughjust barely); and cognitive awareness tools into the simple/cognitive category. These same categories also applyto the tools when grouped by user: researcher tools tend to be complex/cognitive; educator tools, complex/social;educator and team tools, complex/social; and team tools, simple/social.The taxonomy described above has at least two important implications, one practical and one theoretical.First, it allows potential users to make clear distinctions between tools in terms of their affordances. ThisICLS 2018 Proceedings149© ISLStaxonomy could help users to distinguish between broad types of visualization tools such as temporal versusnetwork tools or to make more fine-grained comparisons between individual tools. These distinctions may alsohelp users understand which tools complement one another, and guide their decisions about which tools to usegiven their goals. For example, researchers interested in visualizing complex cognitive information about teamsover time may find temporal visualizations useful. However, if they are also interested in the structure of teamcognition, or how team cognitive components depend on one another, then they may want to supplement theiranalyses with network or dependency tools.Second, this taxonomy suggests potential areas of further inquiry and visualization tool development.For example, the discrepancy between tools designed for researchers and those designed for teams suggests thatmore complex and cognitively focused tools have yet to be developed for teams. A potential explanation for thelack of more complex tools for teams may have to do with the constraints associated with them. Just as tools haveaffordances that make certain interactions possible, their constraints place limits on those interactions. Theresearcher visualization tools are constrained in the sense that they may require more knowledge or expertise tounderstand, and more time to interpret. Teams that lack this knowledge and time may not benefit from such toolsunless they are carefully designed for their contexts. For now, the effect of such tools on team performanceremains an open question.This study has several obvious limitations. First, our data selection procedure was limited in that it reliedon the suggestions of a small panel of experts. Although these selections were supplemented by citation andliterature searches, a more systematic search of the literature would likely have added to our sample. Future workwill explore more systematic data selection methods. Second, in the majority of cases, our coding process waslimited to investigating descriptions or images of the visualization tools rather than the tools themselves. Thus, itis possible that some important features were missed if they were not clearly described in their write-ups. Finally,the sample sizes for some types of visualization tools, such as temporal tools, were relatively small. Thenormalization feature of ENA should mitigate such differences in sample size; however, future work will targetliterature searches around tools that were less represented here. Despite these limitations, this work provides apreliminary taxonomy of team performance visualization tools based on an empirical analysis. Investigating moretools will test the stability of the ENA model and taxonomy presented here, and should improve its utility forteams, educators, and researchers.ReferencesBarron, B. (2003). When Smart Groups Fail. Journal of the Learning Sciences, 12(3), 307–359.Bodily, R., & Verbert, K. (2017). Review of research on student-facing learning analytics dashboards andeducational recommender systems. IEEE Transactions on Learning Technologies, 1–1.Dawson, S., Tan, J. P.-L., & McWilliam, E. (2011). Measuring creative potential: Using social network analysisto monitor a learners’ creative capacity.Engelmann, T., & Hesse, F. W. (2011). Fostering sharing of unshared knowledge by having access to thecollaborators’ meta-knowledge structures. Computers in Human Behavior, 27(6), 2078–2087.Fiore, S. M., & Wiltshire, T. J. (2016). Technology as Teammate: Examining the Role of External Cognition inSupport of Team Cognitive Processes. Frontiers in Psychology, 7.Glaser, B. G., & Strauss, A. L. (1967). The discovery of grounded theory: Strategies for qualitative research.Aldine de Gruyter.Griffin, P., & Care, E. (2014). Assessment and teaching of 21st century skills: Methods and approach. Springer.Hmelo-Silver, C. E., Jordan, R., Liu, L., & Chernobilsky, E. (2011). Representational tools for understandingcomplex computer-supported collaborative learning environments (pp. 83–106). Springer.Hutchins, E. (1995). Cognition in the wild. Cambridge, MA: MIT Press.Janssen, J., & Bodemer, D. (2013). Coordinated Computer-Supported Collaborative Learning: Awareness andAwareness Tools. Educational Psychologist, 48(1), 40–55.Kimmerle, J., & Cress, U. (2008). Group awareness and self-presentation in computer-supported informationexchange. International Journal of Computer-Supported Collaborative Learning, 3(1), 85–97.Kimmerle, J., Cress, U., & Hesse, F. W. (2007). An interactional perspective on group awareness: Alleviating theinformation-exchange dilemma (for everybody?). International Journal of Human-Computer Studies,65(11), 899–910.Martinez Maldonado, R., Kay, J., Yacef, K., & Schwendimann, B. (2012). An Interactive Teacher’s Dashboardfor Monitoring Groups in a Multi-tabletop Learning Environment. In S. A. Cerri, W. J. Clancey, G.Papadourakis, & K. Panourgia (Eds.), Intelligent Tutoring Systems (Vol. 7315, pp. 482–492). Berlin,Heidelberg: Springer Berlin Heidelberg.ICLS 2018 Proceedings150© ISLSMislevy, R. (1996). Evidence and inference in educational assessment. Los Angeles: National Center for Researchon Evaluation, Standards, and Student Testing.Munneke, L., Andriessen, J., Kanselaar, G., & Kirschner, P. (2007). Supporting interactive argumentation:Influence of representational tools on discussing a wicked problem. Computers in Human Behavior,23(3), 1072–1088.Norman, D. (2013). The design of everyday things: Revised and expanded edition. Basic Books (AZ).OECD. (2017). PISA 2015 collaborative problem‑solving framework. In PISA (pp. 131–188). Organisation forEconomicCo-operationandDevelopment.Retrievedfromhttp://www.oecdilibrary.org/content/chapter/9789264281820-8-enReimann, P., Frerejean, J., & Thompson, K. (2009). Using process mining to identify models of group decisionmaking in chat data. In Proceedings of the 9th international conference on Computer supportedcollaborative learning-Volume 1 (pp. 98–107). International Society of the Learning Sciences.Roschelle, J., & Teasley, S. D. (1995). The Construction of Shared Knowledge in Collaborative Problem Solving.In C. O’Malley (Ed.), Computer Supported Collaborative Learning (pp. 69–97). Berlin, Heidelberg:Springer Berlin Heidelberg.Sha, L., Teplovs, C., & van Aalst, J. (2010). A visualization of group cognition: Semantic network analysis of aCSCL community. In Proceedings of the 9th International Conference of the Learning Sciences-Volume1 (pp. 929–936). International Society of the Learning Sciences.Shaffer, D. W. (2017). Quantitative ethnography. Madison, WI: Cathcart Press.Shaffer, D. W., Collier, W., & Ruis, A. R. (2016). A tutorial on epistemic network analysis: Analyzing thestructure of connections in cognitive, social, and interaction data. Journal of Learning Analytics, 3(3),9–45.Siebert-Evenstone, A.L., Arastoopour Irgens, G., Collier, W., Swiecki, Z., Ruis, A.R., & Shaffer, D.W. (2017).In search of conversational grain size: Modelling semantic structure using moving stanzawindows. Journal of Learning Analytics, 4(3), 123–139.Verbert, K., Duval, E., Klerkx, J., Govaerts, S., & Santos, J. L. (2013). Learning analytics dashboard applications.American Behavioral Scientist, 57(10), 1500–1509.Vygotsky, L. S. (1978). Mind in society: The development of higher psychological processes (14th ed.).Cambridge, MA: Harvard University Press.AcknowledgmentsThis work was funded in part by the National Science Foundation (DRL-0918409, DRL-0946372, DRL-1247262,DRL-1418288, DRL-1661036, DRL-1713110, DUE-0919347, DUE-1225885, EEC-1232656, EEC-1340402,REC-0347000), the MacArthur Foundation, the Spencer Foundation, the Wisconsin Alumni ResearchFoundation, and the Office of the Vice Chancellor for Research and Graduate Education at the University ofWisconsin-Madison. The opinions, findings, and conclusions do not reflect the views of the funding agencies,cooperating institutions, or other individuals.ICLS 2018 Proceedings151© ISLS