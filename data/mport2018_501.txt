Towards a Framework for SmartClassrooms that Teach Instructors to TeachDavid Gerritsen, Carnegie Mellon University, dgerrits@cs.cmu.eduJohn Zimmerman, Carnegie Mellon University, johnz@cs.cmu.eduAmy Ogan, Carnegie Mellon University, aeo@andrew.cmu.eduAbstract: Teaching Assistants (TAs) play a major role in higher education; however, theyreceive little if any training on how to teach. Quality training requires access to groundedfeedback and relevant suggestions for improvement. We developed a framework for usingfeatures of a smart classroom. This work reframes the instructor as the learner. It providestraining on discursive practices with feedback based on the instructor’s in-class behaviors. Webuilt and deployed a system based on this framework to five STEM TAs as part of a largerstudy. This paper: discusses the action-reflection-planning framework we used, providesevidence for how the framework addresses TA learning goals, and discusses how otherresearchers might make use of the framework.IntroductionTeaching Assistants (TAs) teach up to 26% of college classes (Friedman, 2017). Interestingly, most receive nopedagogical training (Ellis et al., 2016), nor do they receive regular feedback on their performance (Austin, 2002).Smart classrooms now offer many sensors that can detect various behaviors that occur during class (Blanchard etal., 2016). While this technology to date has focused mostly on students, it could also sense instructors' behaviors,recasting the instructor as a learner. Smart classrooms provide a new opportunity for training such noviceinstructors to teach using their own data as a form of input, creating opportunities for instructional feedback thathave been difficult to achieve at scale.We developed a framework for training instructors to teach that incorporates reflection on practice andplanning for upcoming classes that is distributed over time within cycles of teaching. In addition, we prototypeda system to train instructors on discursive practices, simulated a smart classroom with research assistants loggingbehaviors, provided feedback in the form of data visualizations, and prompted instructors to both reflect on theirperformance and plan for the next class. We conducted a field trial with a small number of TAs and observedmovement towards better teaching practices and some shift in beliefs around student-centered learning.Related workSupport for TAs is rare. Many schools provide no training. When available, many TAs choose not to participatefor reasons including the need to dedicate many consecutive hours to the typical workshop format of such training(Ellis et al. 2016). However, professional development (PD) research on faculty and TAs shows that they needhelp to notice their own poor practices (Henderson et al., 2011) and for setting goals (Brinko, 1993). Regular,data-supported feedback on teaching would address some of these needs (Brinko, 1993).Researchers have previously offered digital feedback to instructors, typically as dashboards showingwhat students do or know (Jivet et al., 2017). Research on instructor feedback systems shows that instructors musthave pre-existing expertise to interpret them. Currently, most TAs have no prior teaching experience nor accessto feedback. Our work investigates opportunities for addressing this lack of expertise through automated feedbackon what TAs actually do in the classroom combined with online training.System designOur socio-technical training system (STTS) is meant to simulate a future smart classroom that can provide trainingand feedback, and it is meant to scaffold reflection and planning. Our system aims to help TAs develop the use ofdiscursive teaching tactics (Rocca, 2010) to reduce non-interactive lectures and shallow questioning strategies;actions which harm student attention, focus, and learning (Chi & Wylie, 2014; Freeman et al., 2014). It introducessimple research-proven strategies for eliciting student participation and describes how increased participationimproves student learning (Rocca, 2010). It trains TAs on the use of discursive techniques such as askingquestions, asking content questions, pausing after asking questions, and calling on students by name.We developed a framework (Figure 1) of the TA as a learner based on our pilot studies of how TAsprepare for teaching. It is also influenced by Clark and Hollingsworth’s interconnected model of professionalgrowth (2002). This functioned as an overarching guide for the design of the STTS. The STTS directs TAs throughan iterative process of reflection, planning, and acting (i.e., teaching their class).1779We simulated a smart classroom by having research assistants witha logging tool capture in-class behaviors, such as the length and order of TAand student speech turns, the number and types of questions TAs asked, andthe pauses between speech turns. We prototyped the training aspect of ourSTTS using the commercial survey service, Qualtrics. We used this to delivertraining materials on discursive practices, to present data visualizations ofteaching behaviors, and to scaffold reflection on recent teaching and planningfor an upcoming class.Our design organizes activities into conceptual Units that aredelivered over about ten days via three subsection, called modules (M). Theyare M.1: training and reflecting on performance to notice teaching behaviors,M.2: planning for use of new tactics, and M.3: reviewing and reflecting onthe results of attempting those tactics. M.1 provides instruction on the value Figure 1. Diagram of frameworkof student participation and asks TAs to recall details of their most recent for sensing and training to alignwith TA activities.class. It shows them data visualizations from that class, revealing theirpatterns of teaching. TAs respond to reflection prompts immediately after viewing these data. M.2 supports goalsetting and guides TAs through practical tactics (e.g., “Write questions up on the board rather than just sayingthem out loud”). TAs commit to trying new tactics in their next class. After the next class, M.3 reveals updatedsets of visualizations comparing prior and current performance, and the outcomes of their selected strategies.In the current field trial, we deployed two content Units, each containing these three modules. Unit Atrains TAs to ask more questions and to ask questions about the course content, as opposed to simply asking if thestudents have "any question?". Unit B trains TAs to wait at least 3 seconds after asking a question before speaking.MethodWe conducted a qualitative study of our STTS to examine how well it promotes the features of the framework.We wanted to understand how our design might impact TAs’ in-class actions, post-class reflections, and pre-classplanning. We recruited 5 TAs in an American university (one undergraduate and four PhD students) whovolunteered due to interest in improving their teaching. They were all male. They taught either a recitation sectionor a stand-alone course in math, engineering, or accounting. Average attendance per class was about 20 students.We observed a total of 80 class sessions totaling 89 hours of observation across the 5 TAs.Before deploying our STTS, we observed a baseline of about 6 sessions per participant. Participants theninteracted with the STTS seven times over the three-week trial. Each TA’s interaction with the tool was short (nomore than 10 minutes per session). Sessions that included class data were available within 1 hour of teaching.TAs were prompted with email messages containing links to the Qualtrics forms. TAs were prompted to plan 2days before their next class.Following the trial, we conducted semi-structured interviews with each TA for 1 hour, followed by adeep review of the conversation by the research team. We followed a style of contextual inquiry interviews thatproduces line by line segments of subject sentiments, each of which we used to produce inductive sentimentthemes across participants (Beyer & Holtzblatt, 1997).Data analysisTo assess the framework, we reviewed three data sources: interview sentiments, interactions with the STTS, andin-class behaviors. For Action, we looked at the use of suggested discursive strategies, and whether or not TAschanged behaviors following the use of the STTS. From the class logs, we calculated discursive actions at theclass level (e.g., number of questions asked per session) and used exploratory data analysis to uncover behavioraltrends at an individual TA level, both across sessions and between baseline and instructional sub-units. ForReflection, we reviewed STTS interactions and interview sentiments to discern whether TAs engaged inreflection, including assessments of their ability to perform critical self-evaluation. For Planning, we reviewedthe strategies TAs selected to try, and we judged whether they enacted those tactics in class.FindingsIn general, the TAs all used our STTS as designed. We observed them all engage in a structured cycle of action,reflection, and planning. None abandoned the system. They typically responded to each emailed prompt to usethe system within one or two days at which point they completed the module. All participants described the systemas “useful” for improving their teaching. P1, P3, and P4 all exhibited positive change over the semester. P2 andP5 exhibited ineffective teaching strategies from the start, and these persisted throughout the semester. Below, wecharacterize the TAs' interactions with the STTS based on our Action, Reflection, and Planning framework.ICLS 2018 Proceedings1780© ISLSReflection (M.1 & M.3)We designed two distinct types of reflection: before practice and after practice. Before practice reflection relatesto module M.1, where participants received training and saw visualizations of their behavior from the previousclass. After practice reflection relates to module M.3, where participants viewed visualizations of theirperformance after they had previously created a plan to try something new for their next class.With respect to before practice reflection, all TAs expressed curiosity and surprise when viewingvisualizations of in-class behaviors. They generally seemed to appreciate the objective nature of the data. Weobserved P1 and P4 rationalizing what they viewed as negative aspects of their data by explaining some challengethey faced in class. In answering the reflection prompts, P1 focused on a high number of unanswered questions.He shared that he had a “bad habit” of “chaining” several questions together, which seemed to not offer thestudents the time or opportunity to answer. P4 focused on his first visualization, which showed that he talked for75% of class time and the students only talked for 5% of class time. He explained that this might be happeningbecause he tried to “prioritize delivering all the materials on time...” He felt more pressure to cover all of thecontent as opposed to creating time for students to participate.With respect to after practice reflection, all TAs acknowledged the validity of the data and tried toexplain some aspect of it. In M.3 TAs were asked to review their attempts to enact new tactics. Some TAsexpressed positive self-evaluations of themselves as instructors, pointing to increases in studentparticipation/responses or describing their skill with a new tactic. For example, P4 shared that preparing questionsbefore class helped him ask more content questions than he had previously been asking. In the interview, he sharedthat he continued this practice throughout the rest of the semester. P1 shared that he “paid more attention to what[he] was saying question-wise,” as he began to notice going “on autopilot.” P4 reflected on the fact that he wasnot able to increase the percentage of student talk. Interestingly, he gave himself credit for trying. “I set goals formyself after seeing the data, like when I saw the data where I waited too short. So, I set the goal to wait longer.”TAs did not all feel they improved. When prompted to recall their goals, neither P2 or P5 could recallthe techniques they had planned to try. When asked to reflect on his goal attainment in Unit A/M.3, P5 stated hecould not remember the class in question. Our system prompts TAs to review their data soon after teaching.However, in these cases, P2 waited 3 days before logging in, and P5 waited 9. Later, in Unit B, P2 reflected thatwaiting longer after asking questions, “Did not seem to make any particular difference, as students could notanswer the question anyway.” P5 reflected that he should probably wait longer in the future, despite waiting longerthan the target of 3 seconds. He seemed to fail to notice a larger issue, that he asked very few questions.Planning (M.2) and ActionWe report Planning and Action together to support evaluations of TAs’ attempts to use their selected tactics inclass. Overall, TAs were responsive to goal-setting. When planning their upcoming class, TAs saw a list ofstrategies they might try in order to increase the content questions they asked, e.g., preparing questions in advanceof class, or putting questions up on slides. In baseline observations, most TAs relied on lecture and shallowquestioning strategies. We observed each TA select at least one improvement strategy in Unit A/M.2. Followingthe strategy selection, they all increased the number of content questions they asked in the following class.P1 selected the most strategies, setting goals to slow down during question asking, ask a wider varietyof question types, and wait longer before giving hints or answering a question. P4 set a more conceptual goal,describing how the questions should support the need to cover material. P3 said he planned to ask, “a few morequestions than last time.” After planning, P1 avoided rapidly repeating/rephrasing questions, and his studentsresponded to a greater ratio of the questions he asked. P4 increased his use of content questions and reduced noncontent questions. P3 increased his use of content questions, and this persisted for the rest of the semester.Alternatively, P2 and P5 showed only isolated teaching improvement. P2 jumped from 1 content questionat the beginning of Unit A to 19 following planning. P5 went from 6 to 15. Both then reverted to very low numbersof content questions after a single class. Goal-setting for these TAs seemed vague or disconnected from theirpractice. P5’s only goals in Unit A/M.2 were to “Ask better questions” and “Give students more time to thinkabout questions,” (despite already averaging over 3 seconds of wait time). P5 never explained the dramatic riseand drop of content questions. Importantly, during the class where he asked 15 content questions, he averagedonly 1 second of wait time after each question, and no students responded to any of the questions.DiscussionThe framework seemed to promote some reflection and planning for all TAs. Three engaged in substantive selfcritique, and these instructors seemed to improve. Each TA made plans, and all of them attempted at least onenew strategy. However, they did not all maintain the use of these strategies. Each TA who made concrete plansand performed more thoughtful reflection seemed to continue challenging themselves as the course progressed.ICLS 2018 Proceedings1781© ISLSAlthough two of the TAs did not engage in meaningful reflection, concrete planning, or beneficialchanges to their teaching, they did still engage with each stage of the STTS. Deeper analysis of their interactionswith the system and possible changes in their beliefs may provide some insight into what went wrong and revealspecific needs to address to make the system more successful. Our analysis leads us to conclude that a fullydeployed system would function better if it accounted for individual differences in instructors. We suspect thatpaying attention to beliefs in teacher-centered learning versus student-centered learning is a good place to start.A deployed system might also collect self-efficacy measures and adapt to instructors more individually.For others attempting to operationalize this framework, we point out that it is critical to considerconsistency in user engagement. Parts of the framework require specific timing constraints, such as planningbefore class begins and active reflection very soon after teaching. If learners fail to review their data whenprompted, it will likely reduce the system’s impact. Missing sessions of planning or reflection can create gaps inthe training cycles where instructors lose track of their goals. Future systems should consider how the system canmotivate instructors to quickly engage.The design of this kind of teacher-as-learner system benefits from information on what drives theinstructor. Instructors may embrace training due to a genuine motivation to change, or they might be compelledif this becomes a requirement of their job. They may avoid training because they are not interested in improving,do not feel it is worth the time, or because they do not believe that the strategies actually work. Future systemscould use social motivations, such as connecting instructors to a community of practice. Or they could follow amore extrinsic approach, such as hinting that training could enhance the instructor's CV.This field study of our STTS that combines smart classrooms that sense instructors’ actions withpersonalized feedback and training shows promise, and it offers a new approach to instructional PD. Beyondproducing actionable, scalable methods for TA training, we believe this data-rich approach to interacting withnovice instructors may reveal interesting new insights into how people learn to teach.ReferencesAustin, A. E. (2002). Preparing the Next Generation of Faculty: Graduate School as Socialization to the AcademicCareer. The Journal of Higher Education, 73(1), 94–122.Beyer, H., & Holtzblatt, K. (1997). Contextual design: Defining customer-centered systems. Elsevier.Blanchard, N., Donnelly, P. J., Olney, A. M., Samei, B., Ward, B., Sun, X., & D’Mello, S. K. (2016). SemiAutomatic Detection of Teacher Questions from Audio in Live Classrooms. In Proceedings of the 9thInternational Conference on Educational Data Mining (pp. 288–291).Brinko, K. T. (1993). The Practice of Giving Feedback to Improve Teaching: What Is Effective? The Journal ofHigher Education, 64(5), 574.Chi, M. T. H., & Wylie, R. (2014). The ICAP Framework: Linking Cognitive Engagement to Active LearningOutcomes. Educational Psychologist, 49(4), 219–243.Clark, D., & Hollingsworth, H. (2002). Elaborating a model of teacher professional growth. Teaching andTeaching Education, 18, 947–967.Ellis, J., Deshler, J., & Speer, N. M. (2016). Supporting instructional change: A two-pronged approach related tograduate teaching assistant professional development. Proceedings of the 19th Annual Conference onResearch in Undergraduate Mathematics Education, (February).Freeman, S., Eddy, S. L., McDonough, M., Smith, M. K., Okoroafor, N., Jordt, H., & Wenderoth, M. P. (2014).Active learning increases student performance in science, engineering, and mathematics. Proceedings ofthe National Academy of Sciences of the United States of America, 111(23), 8410–5.Friedman, J. (2017, February). 10 Universities Where TAs Teach the Most Classes. Retrieved fromhttp://www.usnews.com/Henderson, C., Beach, A., & Finkelstein, N. (2011). Facilitating change in undergraduate STEM instructionalpractices: An analytic review of the literature. Journal of Research in Science Teaching, 48(8), 952–984.Jivet, I., Scheffel, M., Drachsler, H., & Specht, M. (2017). Awareness is not enough: Pitfalls of learning analyticsdashboards in the educational practice. In Lecture Notes in Computer Science (Vol. 10474, pp. 82–96).Rocca, K. A. (2010). Student Participation in the College Classroom: An Extended Multidisciplinary LiteratureReview. Communication Education, 59(2), 185–213.AcknowledgmentsThe research reported here was supported in part by a training grant from the Institute of Education Sciences(R305B090023), NSF grants IIS-1747997 and IIS-1464204, and the Jacobs Foundation. Opinions expressed donot represent the views of the U.S. Department of Education.ICLS 2018 Proceedings1782© ISLS