Learning to Argue: The Role of Peer AssessmentShiyu Liu, The Pennsylvania State University, liux0631@gmail.comAbstract: The present study explored how peer assessment may influence the development ofstudents’ skills in constructing written arguments. Twenty-two college freshmen participated inthis qualitative research to provide feedback on their peer’s written arguments about popularpsychology topics. Constant comparative analysis of multiple data sources revealed three maincategories of feedback that students provided: cognition-based, metacognition-based, andaffection-based. While receiving cognition-based feedback had the most impacts on howstudents would later evaluate others’ work, those who had previously provided metacognitionbased feedback were more likely to make observed progress in constructing arguments. Thiswork adds to our understanding about the complex nature of peer assessment and proposes apotentially effective approach to facilitate students’ skills in written argumentation.Keywords: peer assessment, argumentation, feedbackIntroductionPeer assessment is an educational arrangement where students evaluate their peer’s performance and providefeedback to each other (Topping, 1998). Fostering a culture of collaborative learning, engagement in peerassessment allows students to be in charge of their own learning (Kollar & Fischer, 2010). This not only motivatesthem to be more actively involved in the classroom but also helps them to be more reflective on their learningperformance. Ultimately, students become more proficient in self-assessment and metacognition (Cho &MacArther, 2010). Considering its various learning benefits, peer assessment has been broadly employed in highereducation and incorporated into the classrooms in different disciplines (Strijbos & Sluijsmans, 2010).However, not all peer assessment provides positive learning effects. Several main factors may contributeto the learning outcomes of peer assessment, including the frequency and levels of detail in the feedback (Gibbs& Simpson, 2004), the nature of the tasks to accomplish (Topping, 2009), and instructional support provided (vanden Berg, Admiraal, & Pilot, 2006). While in recent years a significant number of studies have evaluatedapproaches to structure peer assessment (e.g., Cho & Shunn, 2007; Orsmond, Merry, & Reiling, 2002), furtherresearch is needed to expand the scope of this line of research to explore how peer assessment may affect variousaspects of learning (van Zundert, Sluijsmans, & van Merriënboer, 2010).The present study explores how incorporating peer assessment may influence students’ learning ofargumentation. Argumentation is a social and verbal means of trying to resolve a conflict or difference that existsbetween two or more parties (Sampson & Clark, 2008). Constructing arguments requires individuals to analyzedata and rationalize its use as evidence for a claim (Sandoval & Millwood, 2005). Previous research has shownthat students experience difficulty in generating evidence-based arguments in both the sciences and social sciences(e.g., Acar, Turkmen, & Roychoudhury, 2010). As students tend to overlook the importance of argumentation andare often confined to only one perspective, engaging students in productive argumentation in the classrooms isparticularly challenging (McNeil, 2008). With an overarching goal of exploring the role of peer assessment inwritten argumentation, the present study investigates three research questions:1. What types of feedback do students provide when assessing peer’s written arguments?2. How does engagement in peer assessment relate to the development of argumentation skills?3. What is students’ perceived effectiveness of peer assessment?MethodsContext and participantsTwenty-two college freshmen at a large public university in the Midwestern U.S. participated in this study (15females and 7 males, Mage=18.20). They were all from at-risk backgrounds (5 African American, 4 Hispanic, 13Hmong) and enrolled in the university’s TRiO program, which provided extensive academic support for underrepresented students to navigate through college. The study was conducted in an introductory psychology coursethat focused on developing students’ cognitive and metacognitive skills. The curriculum was primarily centeredon collaborative learning activities to foster students’ critical thinking and self-regulated learning. None of theparticipants had taken any psychology courses prior to this course.ICLS 2016 Proceedings910© ISLSAs part of the curriculum, written assignments were designed for students to critically evaluate differentsources of information, construct evidence-based written arguments, and engage in peer assessment (see Table 1).Instructional support was developed to help students understand and practice writing arguments as well asevaluating other’s writing. In addition, they were asked to reflect on their writing processes and experience in peerassessment in a journal every week. A whole-group discussion was led by the instructor at the end of every otherweek to help students further reflect on their learning experience.Table 1: Timeline of instructional design on written argumentation and peer assessmentContentIntroduction to written argumentation and peer assessmentTopic 1. Is development gradual and continuous, or abruptly in separate stages?Topic 2. Which form of learning, conditioning and observational learning, ismore effective in scenarios such as classroom management and business?Topic 3. Do you think we have one general intelligence or multipleintelligences?Topic 4. Which psychological theory do you think best characterizes ourpersonality?Summary and debriefTimelineWeek 1Week 2Week 3Week 4Week 5Week 6Week 7Week 8Week 9Week 10ActivityGroup practicesWritten Assignment 1Peer assessment 1Written Assignment 2Peer assessment 2Written Assignment 3Peer assessment 3Written Assignment 4Peer Assessment 4Group discussionA detailed rubric was provided to guide students’ own writing and evaluation of others’ work regardingboth the general quality of their writing as well as the content and structure. Rather than adopting a single-assessorapproach, this study was designed to have each student receive feedback from two peers for more learning benefits(Cho & Shunn, 2007). To reduce the potential negative influence on learning outcomes, the grades studentsreceived from their peers were not counted toward their course grade.Data collection and analysisData sources included four components: feedback that students provided to their peers; feedback that studentsreceived from their peers; students’ written arguments; and students’ weekly reflection journals. A qualitativeapproach, constant comparative analysis (Strauss & Corbin, 1990), was employed to integrate the differentsources of data and identify main themes that may emerge. Moodle, an online course management system, wasused for students to submit their own work and evaluate others’. All data were exported from Moodle and enteredinto NVivo 10 for analysis. Data analysis underwent an iterative process to establish credibility.FindingsResearch question 1Three main categories were identified to capture the feedback that students provided: cognition-based,metacognition-based, and affection-based (see Table 2). In particular, cognition-based feedback entailed threelevels: corrective, confirmatory, and suggestive. While corrective feedback mainly identified grammatical errorsand conceptual incorrectness, confirmatory feedback included comments that reiterated the agreement betweenthe assessor and the author. Suggestive feedback, in contrast, was the most cognitively demanding: it entails bothdiagnosis of misconceived knowledge and constructive suggestions for improving the quality of the writtenarguments. The second main category, metacogntion-based feedback, illustrated how the assessor reflected ontheir own work and would make improvements if they were to rewrite it. Last, affection-based feedback primarilyconsisted of encouraging and complimentary comments on the work being assessed.Based on in-depth qualitative analysis, this categorization aligned with the typology that Topping (1998)proposed. More importantly, it shed light on how existing characterization of peer feedback should be extendedgiven the diverse types of tasks. Constructing written arguments is a challenging task and thus rather cognitivelydemanding. Students who are new to this task may find themselves less prepared for either completing the task orcommenting on other’s work. As a result, affection-based feedback may be more prevalent in this context.However, affection-based feedback may not necessarily appear in other less challenging situations. Futureresearch should explore how different types of task may yield variations in the categorization of peer feedback.ICLS 2016 Proceedings911© ISLSTable 2: Categorization of feedback participants provided in peer assessmentMain CategorySubcategoryCorrectiveCognitionbasedConfirmatorySuggestiveMetacognitionbasedAffectionbasedDefinitionFocused on basic aspects such as thelength and grammatical issues of theargumentsExample“A couple of times you said ‘change’when it was supposed to be‘changes’ ”Discussed agreement with the author inaspects such as conceptual understandingor personal beliefs“The information in the summaryreveals a correct understanding of theconcept”Focused on the quality of the argument,such as evidence used, the strength ofjustification in the arguments, and so on;constructive suggestions were made forfurther improvementSelf-reflective comments on others’work, often discussing how they wouldimprove their own workEncouraging and/or complementarycomments on others’ work, usuallyfocusing on its overall impression ratherthan its content“You believe that development is inseparate stages, but throughout yourargument, I feel as if you got lost. Fora strong argument try to giveexamples that support your claim”“I never thought of using thisexample.”“Great job!”“I really liked the way you organizedthe paragraphs.”Research question 2Students’ written arguments were evaluated based on the rubric they were provided with. Considering the natureof this investigation and the limited sample size, the relationship between peer assessment and student learning ofwritten argumentation was investigated qualitatively and three patterns were identified to illustrate the nature ofthis relationship. First, the feedback students provided was mostly affection-based during the first peer assessment,with few students using both affection-based and cognition-based feedback. More cognition-based andmetacognition-based feedback emerged during the third and fourth peer assessments. Second, the more cognitionbased feedback one received, the more likely they would later provide feedback with similar focus. For example,when a student received corrective comments on their grammar, they tended to also evaluate others’ argumentsin this aspect the next time. Last, changes in students’ own arguments were observed to occur mostly when theyhad previously received or made metacognition-based comments during peer assessment.This finding, while limited in its generalizability, implied the complex nature of peer assessment andhow it may influence students’ learning outcomes. It revealed a potential alternative to understand the mixedfindings in the literature regarding the effectiveness peer assessment. As the types of feedback one may provideand receive vary largely, it is critical that future studies investigate this topic in more depth at a micro level. Suchefforts will not only enrich our understanding of peer assessment but also provide implications for improvingstudents’ skills in self- and peer assessment.Research question 3Students’ reflection journals were analyzed to obtain an understanding of their perceived experience in peerassessment. While it was a common concern among students that they would not be able to provide their peerswith helpful feedback, most students valued the opportunities to read their peer’s written work. At the same time,students mentioned several drawbacks of engaging in peer assessment. First, it was time-consuming as theyneeded to carefully read through the writing before providing any feedback, and without much previousexperience, this process took them much longer than expected. Second, when there was inconsistence betweentheir own understanding of a concept and that of the peer’s, students felt lack of confidence to make a mark butalso did not necessarily resort to others for clarification. Additionally, being aware of the quality of others’ workmay lead students to be less committed to completing their assignments with high quality. Students reportedhaving trouble to devote the same levels of efforts to their own work after reading a peer’s sloppy writing.Students usually need to see the value and relevance of assessing their peers to make the most out of thisexperience (Hanrahan & Isaacs, 2001). The current findings indicated that when exposed to a relativelychallenging task, such as writing a quality argument, students may find peer assessment valuable, yet overlydifficult. Therefore, how to incorporate peer assessment so that students are more motivated and actively engagedremains to be further studied. Nonetheless, as shown in previous research, novices tend to view their peer’sICLS 2016 Proceedings912© ISLSevaluation more understandable and acceptable in comparison with experts’ (Cho & MacArthur, 2010). Receivingpeer feedback is more likely to lead one to revisit their work and make further revisions (e.g., Gielen, Peeters,Dochy, Onghena, & Struyven, 2010). Hence, in this study, students’ perception of the value of peer assessmentmay also help to explain the observed progress in their learning of written argumentation.ConclusionsThe present study explored how engaging in peer assessment may influence students’ learning of writtenargumentation. Our findings suggested that providing and receiving peer feedback may enhance the quality of thearguments students generate, but this effect varied depending on the type of the feedback. By exploring thepotential benefits of peer assessment, this study adds to our understanding of how collaborative learning activitiesmay facilitate students’ performance in generating evidence-based arguments. At the same time, the discussion ofstudent argumentation also extends the scope of previous research on peer assessment, as it proposes an alternativeapproach to evaluate the learning outcomes of peer assessment.Given the nature of the course that served as the study context in this work, the sample size was relativelysmall and interpretation of the findings should be carried out with cautions. However, the in-depth qualitativeanalysis of multiple data sources in this study helped to gain a deep insight into how peer assessment may affectstudents’ learning processes. The current findings invite further research to investigate the impact of peerassessment on students’ argumentation skills. In all, this research proposes a potentially effective approach tofacilitate student learning in written argumentation and constitutes first steps in enhancing under-representedstudents’ academic performance and helping them succeed in college.ReferencesAcar, O., Turkmen, L., Roychoudhury, A. (2010). Student difficulties in socio-scientific argumentation anddecision-making research findings: Crossing the borders of two research lines. International Journal ofScience Education, 32 (9), 1191-1206.Cho, K., & MacArthur, C. (2010). Student revision with peer and expert reviewing. Learning and Instruction, 20,328-338.Cho, K., & Shunn, C.D. (2007). Scaffolded writing and rewriting in the discipline. Computers and Education, 48,409-426.Falchikov, N., & Goldfinch, J. (2000). Student peer assessment in higher education: A meta-analysis comparingpeer and teacher marks. Review of educational research, 70(3), 287-322.Gibbs, G., & Simpson, C. (2004). Conditions under which assessment supports students’ learning. Learning andteaching in higher education, 1(1), 3-31.Gielen, S., Peeters, E., Dochy, F., Onghena, P., & Struyven, K. (2010). Improving the effectiveness of peerfeedback for learning. Learning and Instruction, 20(4), 304-315.Hanrahan, S.J. & Isaacs, G. (2001). Assessing self- and peer-assessment: The students’ view. Higher EducationResearch and Development, 20(1): 53–70.Kollar, I., & Fischer, F. (2010). Peer assessment as collaborative learning: A cognitive perspective. Learning andInstruction, 20(4), 344-348.McNeil, K. (2008). Teachers’ use of curriculum to support students in writing scientific arguments to explainphenomena. Science Education, 93(2), 233-268.Orsmond, P., Merry, S., & Reiling, K. (2002). The use of exemplars and formative feedback when using studentderived marking criteria in peer and self-assessment. Assessment & Evaluation in HigherEducation, 27(4), 309-323.Sampson, V., & Clark, D. (2008). Assessment of the ways students generate arguments in science education:Current perspectives and recommendations for future directions. Science Education, 92 (3), 447-472.Sandoval, W., & Millwood, K. (2005). The quality of students’ use of evidence in written scientific explanations.Cognition and Instruction, 23(1), 23-55.Strijbos, J., & Sluijsmans, D. (2010). Unrevelling peer assessment: Methodological, functional, and conceptualdevelopments. Learning and Instruction, 20, 265-269.Topping, K. J. (1998). Peer assessment between students in colleges and universities. Review of educationalResearch, 68(3), 249-276.Topping, K. J. (2009). Peer assessment. Theory into practice, 48(1), 20-27.Van den Berg, I., Admiraal, W., & Pilot, A. (2006). Design principles and outcomes of peer assessment in highereducation. Studies in Higher Education, 31(3), 341-356.Van Zundert, M., Sluijsmans, D., & Van Merriënboer, J. (2010). Effective peer assessment processes: Researchfindings and future directions. Learning and Instruction, 20(4), 270-279.ICLS 2016 Proceedings913© ISLS