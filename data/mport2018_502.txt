Augmenting Formative Writing Assessment With LearningAnalytics: A Design Abstraction ApproachSimon Knight, University of Technology Sydney, simon.knight@uts.edu.auAntonette Shibani, University of Technology Sydney, antonette.shibani@uts.edu.auSimon Buckingham Shum, University of Technology Sydney, simon.buckinghamshum@uts.edu.auAbstract: There is increasing interest in use of learning analytics and technologies underpinnedby artificial intelligence in the support of learning. In implementing and integrating thesetechnologies there are challenges both with regard to developing the technologies themselves,and in aligning them with existing, or transformable, practices. In this paper we argue that by‘augmenting’ formative tasks with learning analytics, we can achieve impact both through theintegration of the tools, and through the support of existing good practices. We exemplify ourapproach through its application to the design of tasks for the key skill of learning how to writeeffectively. The development of these designs and their abstractions holds significant potentialin bridging research and practice, by supporting the sharing and interrogation of designs in away that is intimately tied to both practice (practical applied contexts), and research (throughtheorized and empirically supported designs targeting particular learning outcomes).Keywords: writing analytics, learning analytics, learning design, conjecture maps, design patterns,technology enhanced assessment, technology enhanced learning, learning sciencesIntroductionLearning how to write analytically is a core skill in higher education and professional contexts (NationalCommission On Writing, 2003; OECD & Statistics Canada, 2000). However, its teaching is challenging,(Ganobcsik-Williams, 2006) with students often judging their work by more superficial criteria than the analyticalstandards that educators apply (Andrews, 2009; Lea & Street, 1998; Lillis & Turner, 2001; Norton, 1990). Thismay be considered a specific case of a common misalignment between people’s perceptions of their own learningand reality (Bjork, Dunlosky, & Kornell, 2013). To address this misalignment, students should be inducted intoacademic practice to develop ‘evaluative judgement’, their ability to apply normative standards to assess thequality of work, in order that they are better able to self-assess their own work and thus to improve it (Boud,2000).To develop this evaluative judgement, there is a need to both implement tasks that target the learning ofwriting, and to research the learning of writing, in classroom contexts (Graham & Harris, 2014). In their 12recommendations for designing high quality writing intervention research, Graham and Harris (ibid) note theimportance of interventions that are “well-founded and designed”, “representative of the real world context”, andinclude “a series of studies to refine and test the writing intervention” (Graham & Harris, 2014, p. 96). Taskdesigns, and the sharing of these, that support students in learning to write are important.In this paper, we describe an approach to ‘augment’ learning designs for writing pedagogy, through theaddition of automated writing feedback to writing tasks. As we describe in the following section, there is growinginterest in the use of techniques drawn from the fields of learning analytics, educational data mining, or artificialintelligence as applied to education. The integration and implementation of these techniques to support learningin authentic contexts is an ongoing challenge. In the section ‘Empirical Case Study’ we describe a set of tasks thatwere designed specifically to develop student writing skills. These tasks are described through the lens of designabstractions that highlight the relationship between features of the design and the theorized pedagogic intent(described in ‘A Design Based Approach’). By exemplifying this ‘augmentation’ technique, we highlight itspotential to support communities such as AIED and ISLS to develop methods that support both research andimplementation.Writing analytics to support student writingOne means through which to support the learning of writing is through the provision of feedback to students thatsupports them in building their evaluative judgement – i.e., feedback that helps students learn to recognize whatis good, and what needs improving, particularly in their own work. Such feedback might be provided throughautomated means, and indeed a body of work has investigated this potential in automated essay scoring (AES),which have been successfully deployed in summative assessment contexts (see, e.g., discussions throughoutShermis & Burstein, 2013). However, these AES systems are not without detractors (Ericsson & Haswell, 2006),ICLS 2018 Proceedings1783© ISLSand much less attention has been given to the challenge of providing formative feedback, which to be actionableby the learner, requires more than the calculation of a grade. There is a growing recognition of the potential ofcomputational approaches and natural language processing to be applied to formative feedback on student work,to support them in understanding where to improve, and develop their writing (see, for examples, BuckinghamShum et al., 2016; Knight, Allen, Gibson, McNamara, & Buckingham Shum, 2017; Passonneau, McNamara,Muresan, & Perin, 2017; Zhang, Hwa, Litman, & Hashemi, 2016).However, the design and implementation of such systems should be conducted with regard to theirpotential as tools for intervention, and as Graham and Harris note, that has implications for design. Specifically,in developing learning analytics as a form of formative assessment (Knight, Buckingham Shum, & Littleton,2014) on writing, they should be tested in real world contexts that provide for incremental improvement of thetool alongside impact on learning. To deploy such emerging technologies in formative contexts, we can conceiveof two approaches. One approach would use technology to shift the focus of assessment to new kinds of processand product, for example towards choice based assessments (Schwartz & Arena, 2013) or ‘performanceassessment’ (Linn, Baker, & Dunbar, 1991) that investigate the decisions students make in completing tasks. Analternative approach would investigate how existing systems might be ‘augmented’ by learning analytics, toprovide novel feedback in existing high quality pedagogy (Knight, Forthcoming). While the potential of theformer is transformative, its challenges include: the need to gather new data types, possibly using noveltechnology; and the expense and research required in order to develop new kinds of assessments. As Baker (2016)noted with regard to intelligent tutoring systems, adoption of artificial intelligence technologies has not beenwidespread, and thus our attention may be best focused on how to better use such intelligent systems alongsideeducators and students in flexible ways. Indeed, the need for pedagogically aligned learning analytics interventiondesigns has been highlighted as key in incorporating learning analytics within educational systems to improveexisting teaching and learning practices (Wise, 2014). Given that for innovations to be taken up by educators,their distance from existing culture, practice, and technologies must be considered (Ferguson et al., 2014; Zhao,Pugh, Sheldon, & Byers, 2002), a focus on augmenting (rather than revolutionizing) existing practice may bothsupport and enhance that practice, alongside raising awareness regarding – and implementation of – the role ofthese technologies for learning.A design based approachThis paper, then, adopts a perspective of ‘augmentation’, by taking a design approach to analyzing teaching andlearning contexts, to investigate where existing good practice might be augmented by learning analytics, furtherstrengthening that practice. In our view, this approach is likely to increase adoption both of the analytics, and ofthe underlying practices, thus driving forward implementation of such learning designs, and the potential toresearch them. A similar call has been made in learning analytics applications for researchers to capture theirpedagogical intents by aligning learning analytics to learning design. In this way, Learning Analytics can help totest the assumptions of learning designs by providing the necessary data, methodologies and tools to support thelearning design in lieu of self-reported measures (Lockyer, Heathcote, & Dawson, 2013). In turn, knowledge ofthe pedagogical context that gives rise to the data is critical to its interpretation. The use of design abstractionscan support this alignment, as we outline in this paper.In taking a design approach, we specifically focus on a particular set of goal oriented designrepresentations to support learning (Goodyear, 2005). Design abstractions that represent features of the design forlearning thus aim to bring into alignment theory and practice to improve both (for example, Goodyear, de Laat,& Lally, 2006; Sandoval, 2014). These representations aim to share the general principles of a task design andpurpose, specifics of a practical implementation, and design configurations targeted at particular outcomes. Forexample, ‘design patterns’ are abstractions that help in sharing existing practice in a way that supports its adoptionacross an array of learning tasks and contexts (Goodyear & Retalis, 2010). Good patterns foster research andadoption by: capturing existing practices that solve existing, complex, problems; abstract at the right level, neithertoo concrete or specific, or too general and divorced from context; gives insight into how it works, making clearwhy we should value it as a solution to the problem; links to other patterns in a structured way; and follows ageneralized representation: a problem statement, the solution pattern, and a rationale for how the solutionaddresses the problem (Goodyear et al., 2006).With a similar aim, conjecture mapping (Sandoval, 2014) involves understanding the ways that learningtasks make conjectures about how the learning should happen; these become testable, improvable, conjecturesand designs (Sandoval, 2014).Tasks are thus analyzed for the design and theoretical conjectures they are basedin. In common across conjecture mapping and design patterns is the desire to flag a key, high-level problem beingaddressed (a learning need). Across both, particular designs then embody or manifest the principles. Thus, asection of a pattern describes empirical background or evidence, and examples of the pattern’s manifestations,ICLS 2018 Proceedings1784© ISLSwhile conjecture maps describe a design’s embodiment through tools and materials, task structure, participantstructures, and discursive practices into mediating processes of observable interactions and participant artefacts;these mediating processes are then mapped to learning outcomes. Design conjectures thus describe assumptionsabout “how embodied elements of the design generate mediating processes,” such that, “if learners engage in thisactivity (task + participant) structure with these tools, through this discursive practice, then this mediating processwill emerge” (Sandoval, 2014, p. 22). Theoretical conjectures, then, describe “how those mediating processesproduce desired outcomes”…in the form “if this mediating process occurs it will lead to…”.In the work described in this paper, we have undertaken a design process to:1. Understand existing patterns of writing support in a particular institutional context, and developabstractions of these2. Augment these abstractions with additional – learning analytics based – abstractions (in this case,patterns) that complement the original designs3. Evaluate the implementation of these patterns, to understand relations among them and the developmentof a larger pattern-set that can be augmented with learning analytics (that are described by patterns intheir own right)Empirical case studyIn this section, we explain an empirical case study of our approach in the context of a design for writing instruction.The key components of the design are represented in a conjecture map shown in Figure 1. The specific sectionsof the task design which were augmented by analytics are displayed with a ‘gear’ icon. The elements of the designand the augmentation will be explained in detail below.Figure 1. Conjecture map of a learning design for writing instruction, augmented by analytics.The box on the left conveys the theoretically principled high-level conjecture, specifically, that in order to inductstudents into disciplinary practice, they must learn to write using the rhetorical structures that make upargumentative forms. This conjecture is then applied through its embodiment within a designed learningenvironment that uses three main elements: Tools/Materials, Task Structures and Participant Structures. Thewriting instruction is driven by the materials defined by the instructor (in tools/materials) and tasks described inthe task structures, listed in the order that students complete them.The current design was developed over three design iterations, described below. These iterationsintroduced changes around the use of analytics (tools/materials), and a dyadic task design (participant structures).Conjecture maps developed for each iteration can help in tracking the trajectory of the research by capturing therationale behind the changes made to the design over the iterations. A key feature of the conjecture map is that itICLS 2018 Proceedings1785© ISLSseparates the logic of the design from the specific instantiations, and provides a clear perspective on pedagogicsites at which learning analytics can augment the design towards the conjecture. In this case, a tool called AWATutor (the Academic Writing Analytics Tutor) was developed that guided students through the tasks, and collecteddata for research, by integrating Writing Analytics in pedagogic contexts (Shibani, 2018; Shibani, Knight,Buckingham Shum, & Ryan, 2017). This tool provides an example case for the use of abstraction to developresearch and implementation understanding of a learning context.Writing analytics tools that make use of computational techniques produce feedback which is almostimmediate. One such tool – “Academic Writing Analytics” (AWA) – uses natural language processing techniques,and has been embedded in an AWA-Tutor tool which allows students to submit their drafts and receive immediatefeedback to make further revision in their texts. This enables students to assess their revisions based on thefeedback and encourage further revisions upon assessment. This immediate feedback, made possible by analytics,can aid reflection and encourage improvement in student revisions on their drafts. While the current design isbased on an automated tool which provides feedback on rhetorical structures in the text, the design can also beextended for tools that provide feedback on other text features.Because the tasks were developed within an online tool, a separate evaluation question was also builtinto the structures, as indicated in the conjecture map. The artifacts highlighted in this conjecture map includespecifically those artefacts about which learning conjectures were made. Since the artifacts from observablestudent interactions in the tasks could be useful proxies for students’ learning, they are of interest to researchersand practitioners. Indeed, the artificial intelligence in education (AIED), learning analytics, and other communitieshave strong interest in analysis of such data. In the context of our design iterations, this interest is further developedin not only describing how conjectures may be made regarding the trace obtained through use of an online tooldesigned with pedagogic principles in mind, but also how learning analytics may augment these task designs.Here, we describe the final map with reference to its development, describing each design, and its analysis forresearch and implementation purposes. Drawing on (though not directly using) Alexandrian design patternprinciples, here we highlight the core evolution of the tasks, in relation to the conjectures made, including throughthe use of computer aided augmentation.DESIGN 1: Benchmarking and Automated Writing AnalyticsProblem: We wanted students to engage with exemplars and their assessment, in order that they have anactivity that (1) prompts them to critically apply the assessment criteria, (2) prompts them to engage activelywith exemplars, (3) provides us as researchers with information regarding their ability to appropriately assesstexts.Task: The initial base task (task 2) consisted of a task in which students were provided with three exemplarsof varying quality, and asked to assess those exemplars using the assessment criteria. The application of theassessment criteria involves a mediating process of evaluative judgement in the application of assessmentcriteria, which in turn should produce the outcome of improved self-assessment ability.Tools/materials and participant structures: This task was designed for individual completion, making useof the instructor’s rubric, and both high and low quality exemplars.Iterations and Augmentation: The task design was modeled on an existing common practice at theinstitution. To augment this with writing feedback, in the initial iteration of the task, students were providedwith texts that had been marked up using writing feedback (from either a tool for feedback on rhetoricalstructures in writing, or one focusing on spelling and grammar, or from the instructor). With the intent offoregrounding salient features of the texts through the provision of NLP-derived feedback in the form ofAnalysis (unpublished) of this activity indicated that – as in previous iterations of the task design – the studentsappreciated access to the exemplars, and criteria. However, there were no clear differences between the ability ofstudents to appropriately apply the criteria in the group with, or without, the automated feedback. In addition, themode of interaction with both the exemplars (in the original task), and the automated feedback (in the ‘augmented’version) is rather shallow. Thus, we sought to develop the task to provide opportunity for deeper interaction.ICLS 2018 Proceedings1786© ISLSDESIGN 2: Benchmarking, Text-Revision, and Automated Writing AnalyticsProblem: We wanted students to critically consider how specific features in the text instantiate responses tothe assessment criteria, and to develop the student’s interaction with the application of the criteria forbuilding their understanding of how to – practically – improve a text.Task: The initial task (task 2) was amended, and an additional task was added (task 1). Task 1 consisted of atask in which the students were asked to match excerpts from a text to the criteria that they addressed (forexample, a sentence providing background information aligns with the criterion “Identification of relevantissues”, while a sentence providing evaluation or analysis of a claim or piece of evidence aligns with thecriterion “Critical analysis, evaluation, original insight”. The revised task 2 involved students assessing a singleexemplar text using the assessment criteria, and being specifically asked how they would suggest improvingthe text. In task 3, then, the students were asked to edit the text they were provided with (in an editable window,see Figure 2), and (task 4) to evaluate the improvements that they had made (i.e., to provide a new assessmentof the quality of the text). Following task 4 the students were provided with their own text revisions, and thoseof an instructor on the same text, providing a ‘good’ exemplar to demonstrate the improvements made. Whilethe original task (above) was intended to produce a mediating process of evaluative judgement, the revisiontask was – in addition – designed to produce a mediating process of revision strategy application, to producethe outcome of increased capacity and motivation to revise, and improved self-assessment ability. The firsttask was specifically designed to develop evaluative judgement through understanding of the assessmentcriteria, and thus to improve self-assessment through understanding of rhetorical structures.Tools/materials and participant structures: As in design 1, this task was designed for individual completion,making use of the instructor’s rubric, and in task 2 a lower quality exemplar, with task 4 providing the higherquality comparator. The instructor’s rubric and the lower-quality exemplar drive the first and second-to-fourthtasks from the task structures list respectively.Iterations and Augmentation: This task design developed from that described in design 1. As in that case, abetween-subjects design was used to provide some students with instructor-based (static) feedback, others withdynamic feedback from AWA, and others with no feedback. Prior work has been conducted to establishconceptual relations between the instructor’s criteria, rhetorical structures, and their specific instantiation inAWA (Knight, Buckingham Shum, Ryan, Sándor, & Wang, 2017). These relationships were foregrounded tothe AWA group through static highlights flagging the AWA moves on the sentences to be aligned with thecriteria. Then, the revision task was also augmented by AWA, with feedback provided on-request (via a button)to students as they revised the draft they were provided with.Figure 2. Sample screenshot from the revision task with an editable text (left) and automated feedback (right).Analysis of this activity (preliminarily reported in, Shibani et al., 2017) indicated that across groups (with andwithout augmentation) the students generally found the tasks useful, and performed at equivalent levels whentheir text revisions were assessed. Feedback in places indicated that students would find it helpful to gain multipleperspectives on the texts and their revisions. Indeed, in conducting the work, it was observed that in some groupsICLS 2018 Proceedings1787© ISLSstudents engaged in peer discussion as a part of the process. Moreover, prior research has indicated the benefitsof peer discussion of assessment criteria and exemplars (Hendry, Armstrong, & Bromberger, 2012; Hendry &Jukic, 2014; Payne & Brown, 2011). Thus, peer discussion may provide a further mediating process. In addition,it provides a further site for possible augmentation (of the peer discussion), and – in contexts where feedback isprovided on the exemplars and revisions – a site to investigate how that feedback is understood and discussed inapplication.DESIGN 3: Benchmarking, Text-Revision, Peer-Discussion, and Automated Writing AnalyticsProblem: Building on the previous designs, we additionally wanted students to engage with each otheraround the application of assessment criteria, to further develop their evaluative judgement, and ability toexplain and justify their judgements of texts and their revisions.Task: The initial base tasks in design 2 were adapted, such that in in one group of students they were asked towork as dyads, submitting a single revised text, and in the other group they worked individually.Tools/materials and participant structures: In this design, the participant structure varied by group, withsome working in pairs and others individually. When students work in dyads, they involve in discussionconsisting of reflection and critique on the structure of essays and the application of automated feedback. Thematerials and tool for this design are the same as those in design 2.Iterations and Augmentation: This task design developed from that described in design 2. A key concern inthis design was that peer discussion may mediate the understanding and use of the augmented feedbackprovided by AWA; that is, this task may develop students’ abilities to – critically – use such feedback, and thatthrough observation of this dialogue research and implementation data is obtained. A further alternative designiteration (to be implemented in 2018) consists of asking students to work individually first (with, or without,augmentation), and then to work in dyads (or not) to create a hybrid revised text to submit.Preliminary analysis of a pilot of this task indicated that across the paired vs individual and AWA vs instructorgroups there were no differences in their reported ‘usefulness’ of the task, or quantity of revisions made (althoughno analysis of the quality of text revisions has yet been conducted). The instructor has reported a preference forpeer discussion in the revision as an authentic practice. Further research is being developed to analyse both theoutputs of the paired tasks, and the dialogue that students engage in and how it might mediate the automatedfeedback received (and vice-versa).Conclusions and implicationsArtificial intelligence and learning analytics hold unmet potential to address learning challenges. One meansthrough which to increase their impact is through the augmentation of tasks grounded in practice, supported byresearch. Design abstractions, such as design patterns and conjecture maps, can support such augmentation. Inthis paper we have exemplified this approach by using abstractions to (1) understand the existing design ofpractices to support writing within a particular institutional context; (2) support the augmentation of these existingdesigns with additional – learning analytics based – design features that complement the original designs; (3)describe and develop the evaluation of the implementation of these patterns, in order to understand relationsamong the design features. In doing so, we indicate how larger task designs can be developed that augmentformative tasks with learning analytics. This approach makes effective use of practitioner strategies for addressingpedagogic problems, thus grounding the use of learning analytics in tasks that are well established, andtheoretically grounded. While this paper focuses on one example – writing instruction – the approach describedhas broad application. The approach described, then, addresses the general concern that innovation must addressthe needs of existing practice (Ferguson et al., 2014; Zhao et al., 2002), and the specific concern that writinginterventions must be “well-founded and designed”, “representative of the real world context”, and include “aseries of studies to refine and test the writing intervention” (Graham & Harris, 2014, p. 96).By taking this approach, we intend to support existing good practice, develop implementation modelsfor analytics, and provide opportunities for research that are grounded in practice and theory. The technologiesdeveloped for this particular task are used not only to engage students in those tasks, but also to gather dataregarding student interaction with the tools, and provide opportunity for exploring how students engage withdifferent kinds of (automated) feedback. By developing further design patterns, we can extend this work. Forexample, further patterns might augment the student text-revision with feedback on the revisions that they make,automate the allocation of peers based on ability or topical interest, or provide feedback to students on how wellICLS 2018 Proceedings1788© ISLSthey assess texts that they are provided with in the benchmarking. Each of these builds on a practice that can bedescribed in terms of design patterns, each of which could be augmented with learning analytics designs.In this paper, conjecture maps provide the ‘argumentative grammar’ that shows the broad design logic.However, these maps omit contextualized features of the designs across contexts, and detailed descriptions of taskspecifics and their relationships to design conjectures. Design patterns can address this concern by setting out indetail specific elements of a design, in such a way that the pattern may be adapted across contexts. By using theseapproaches we bridge the gap between research and practice, to represent and share designs in a way that is usefulfor both audiences.ReferencesAndrews, R. (2009). Argumentation in Higher Education: Improving practice through theory and research.Routledge.Baker, R. S. (2016). Stupid tutoring systems, intelligent humans. International Journal of Artificial Intelligencein Education, 26(2), 600–614. Retrieved from http://link.springer.com/article/10.1007/s40593-0160105-0Bjork, R. A., Dunlosky, J., & Kornell, N. (2013). Self-regulated learning: Beliefs, techniques, and illusions.AnnualReviewofPsychology,64,417–444.Retrievedfromhttp://www.annualreviews.org/doi/abs/10.1146/annurev-psych-113011-143823Boud, D. (2000). Sustainable assessment: rethinking assessment for the learning society. Studies in ContinuingEducation, 22(2), 151–167. https://doi.org/10.1080/713695728Buckingham Shum, S., Knight, S., McNamara, D., Allen, L., K. ., Betik, D., & Crossley, S. (2016). CriticalPerspectives on Writing Analytics (pp. 481–483). Presented at the 6th ACM Learning Analytics andKnowledge Conference, Edinburgh, UK: ACM. https://doi.org/10.1145/2883851.2883854Ericsson, P. F., & Haswell, R. H. (2006). Machine scoring of student essays: Truth and consequences. Utah StateUniversity Press.Ferguson, R., Clow, D., Macfadyen, L., Essa, A., Dawson, S., & Alexander, S. (2014). Setting learning analyticsin context: overcoming the barriers to large-scale adoption. In Proceedings of the Fourth InternationalConference on Learning Analytics And Knowledge (pp. 251–253). ACM. Retrieved fromhttp://dl.acm.org/citation.cfm?id=2567592Ganobcsik-Williams, L. (Ed.). (2006). Teaching Academic Writing in UK Higher Education. Basingstoke, UK:Palgrave Macmillan.Goodyear, P. (2005). Educational design and networked learning: Patterns, pattern languages and design practice.AustralasianJournalofEducationalTechnology,21(1).Retrievedfromhttp://ascilite.org.au/ajet/submission/index.php/AJET/article/view/1344Goodyear, P., de Laat, M., & Lally, V. (2006). Using pattern languages to mediate theory–praxis conversationsindesignfornetworkedlearning.Taylor&Francis.Retrievedfromhttp://www.tandfonline.com/doi/full/10.1080/09687760600836977Goodyear, P., & Retalis, S. (2010). Technology-enhanced learning. Rotterdam: Sense Publishers. Retrieved fromhttps://www.sensepublishers.com/media/1037-technology-enhanced-learning.pdfGraham, S., & Harris, K. (2014). Conducting High Quality Writing Intervention Research: TwelveRecommendations. Journal of Writing Research, 6(2), 89–123. https://doi.org/10.17239/jowr2014.06.02.1Hendry, G. D., Armstrong, S., & Bromberger, N. (2012). Implementing standards-based assessment effectively:Incorporating discussion of exemplars into classroom teaching. Assessment & Evaluation in HigherEducation,37(2),149–161.Retrievedfromhttp://www.tandfonline.com/doi/abs/10.1080/02602938.2010.515014Hendry, G. D., & Jukic, K. (2014). Learning about the Quality of Work That Teachers Expect: Students’Perceptions of Exemplar Marking versus Teacher Explanation. Journal of University Teaching andLearning Practice, 11(2), 5. Retrieved from http://eric.ed.gov/?id=EJ1040741Knight, S. (Forthcoming). Augmenting Assessment with Learning Analytics. In P. Dawson, D. Boud, & M.Bearman (Eds.), Re-imagining Assessment in a Digital World. Springer.Knight, S., Allen, L. K., Gibson, A., McNamara, D., & Buckingham Shum, S. (2017). Writing analytics Literacy- Bridging from Research to Practice. Presented at the 7th ACM Learning Analytics and KnowledgeConference, Edinburgh, UK: ACM.Knight, S., Buckingham Shum, S., & Littleton, K. (2014). Epistemology, assessment, pedagogy: where learningmeets analytics in the middle space. Journal of Learning Analytics, 1(2), 23–47.http://dx.doi.org/10.18608/jla.2014.12.3ICLS 2018 Proceedings1789© ISLSLea, M. R., & Street, B. V. (1998). Student writing in higher education: An academic literacies approach. StudiesinHigherEducation,23(2),157–172.Retrievedfromhttp://www.tandfonline.com/doi/abs/10.1080/03075079812331380364Lillis, T., & Turner, J. (2001). Student writing in higher education: contemporary confusion, traditional concerns.TeachinginHigherEducation,6(1),57–68.Retrievedfromhttp://www.tandfonline.com/doi/abs/10.1080/13562510020029608Linn, R. L., Baker, E. L., & Dunbar, S. B. (1991). Complex, Performance-Based Assessment: Expectations andValidationCriteria.EducationalResearcher,20(8),15–21.https://doi.org/10.3102/0013189X020008015Lockyer, L., Heathcote, E., & Dawson, S. (2013). Informing Pedagogical Action: Aligning Learning AnalyticsWithLearningDesign.AmericanBehavioralScientist,0002764213479367.https://doi.org/10.1177/0002764213479367National Commission On Writing. (2003). Report of the National Commission on Writing in America’s Schoolsand Colleges: The Neglected “R,” The Need for a Writing Revolution. College Board. Retrieved fromhttp://www.collegeboard.com/prod_downloads/writingcom/neglectedr.pdfNorton, L. S. (1990). Essay-writing: what really counts? Higher Education, 20(4), 411–442. Retrieved fromhttp://link.springer.com/article/10.1007/BF00136221OECD, & Statistics Canada. (2000). Literacy in the Information Age - Final Report of the International AdultLiteracy Survey. OECD. Retrieved from http://www.oecd.org/edu/skills-beyond-school/41529765.pdfPassonneau, R. J., McNamara, D., Muresan, S., & Perin, D. (2017). Preface: Special Issue on MultidisciplinaryApproaches to AI and Education for Reading and Writing. International Journal of Artificial Intelligencein Education, 27(4), 665–670. https://doi.org/10.1007/s40593-017-0158-8Payne, E., & Brown, G. (2011). Communication and practice with examination criteria. Does this influenceperformance in examinations? Assessment & Evaluation in Higher Education, 36(6), 619–626. Retrievedfrom http://www.tandfonline.com/doi/abs/10.1080/02602931003632373Sandoval, W. (2014). Conjecture Mapping: An Approach to Systematic Educational Design Research. Journal ofthe Learning Sciences, 23(1), 18–36. https://doi.org/10.1080/10508406.2013.778204Schwartz, D. L., & Arena, D. (2013). Measuring what matters most: Choice-based assessments for the digitalage. Boston Massachusetts: MIT Press.Shermis, M. D., & Burstein, J. (2013). Handbook of Automated Essay Evaluation: Current Applications and NewDirections. Routledge.Shibani, A. (2018). AWA-Tutor: A Platform to Ground Automated Writing Feedback in Robust Learning Design.In 8th International Learning Analytics and Knowledge Conference. Sydney, Australia: Society forLearning Analytics Research (SoLAR) Companion Proceedings.Shibani, A., Knight, S., Buckingham Shum, S., & Ryan, P. (2017). Design and Implementation of a PedagogicIntervention Using Writing Analytics. In W. Chen, J.-C. Yang, A. F. Mohd Ayub, S. L. Wong, & A.Mitrovic (Eds.), 25th International Conference on Computers in Education (pp. 306–315). Christchurch,New Zealand: Asia-Pacific Society for Computers in Education.Wise, A. F. (2014). Designing Pedagogical Interventions to Support Student Use of Learning Analytics. InProceedings of the Fourth International Conference on Learning Analytics And Knowledge (pp. 203–211). New York, NY, USA: ACM. https://doi.org/10.1145/2567574.2567588Zhang, F., Hwa, R., Litman, D., & Hashemi, H. B. (2016). Argrewrite: A web-based revision assistant forargumentative writings. In Proceedings of the 2016 Conference of the North American Chapter of theAssociation for Computational Linguistics: Demonstrations (pp. 37–41). San Diego, California:Association for Computational Linguistics. Retrieved from http://www.aclweb.org/anthology/N16-3008Zhao, Y., Pugh, K., Sheldon, S., & Byers, J. L. (2002). Conditions for classroom technology innovations. TeachersCollegeRecord,104(3),482–515.Retrievedfromhttp://crcsalon.pbworks.com/f/Conditions+for+Classroom+Technology+Innovations.pdfAcknowledgmentsOur thanks to the academics and students who have supported our work, in particular Dr Philippa Ryan at theFaculty of Law. Thanks too to Agnes Sandor at Naver Labs Europe, whose work has been instrumental in thedevelopment of our learning analytics tools. Research of the designs described has been supported by internalteaching and learning grants.ICLS 2018 Proceedings1790© ISLS