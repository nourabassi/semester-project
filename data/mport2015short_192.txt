Assessment and Collaborative Inquiry:A Review of Assessment-Based Interventions inTechnology-Enhanced K-14 EducationYuqin Yang, The University of Hong Kong, yqyang@hku.hkJan van Aalst, The University of Hong Kong, vanaalst@hku.hkAbstract: We provide a conceptual review of the research on assessment in technologyenhanced collaborative inquiry in K-14 education, published between 1994 and 2013. The 57studies that satisfied our search criteria were coded using a framework that considered thenature of the assessment intervention, the purposes of the assessment intervention, and therole of technology in student learning. This allowed us to identify four types of assessmentinterventions. The findings indicated that only interventions in the immersion orientationseemed to help students learn how to learn. Such assessment practices enabled students tolearn actively and to learn both disciplinary substance and metacognitive/regulative skills.However, relatively few studies clearly integrated assessment and learning. The maincontributions of this study are the coding framework and the four patterns of assessmentinterventions. Together, they provide a new way of thinking about the design of practice. Thereview provides guidance for the shift of assessment practice to scaffold learning of this field.Keywords: assessment, learning, technology, collaborative inquiry, scaffoldingIntroductionIn the last few decades, collaborative inquiry has emerged as an important research area in computer-supportedcollaborative learning (Stahl, 2002). Despite substantial development in learning theories, assessment, and theuse of educational technology, it remains unclear whether assessment practices have changed in classroomswhere educational technology is used to support collaborative inquiry. Theoretical developments suggest thatassessment should be used to both measure students’ achievements and to scaffold future learning; furthermore,an increased emphasis on collaboration in learning requires assessment procedures that consider both individualand shared achievements (van Aalst, 2013). The role of students in assessment also continues to be neglected.Educational technologies often store large amounts of information about the learning process, which could beused to enhance learning. Thus, assessments that involve educational technology are of particular interest. Theuse of assessment data to improve learning is gaining some momentum in the policy discourse on education(Datnow & Park, 2014).Most reviews of the assessment literature have focused on higher education; relatively few havefocused on assessment in K-14 education. At the same time, there is a lack of systematic empirical evidence onhow assessment practices can be designed to guide learning. This study reviews the latest developments inassessment in technology-enhanced K-14 education, with a view to articulating an agenda for further researchand development. The question that drove the review is How are assessment interventions designed to scaffoldstudents’ learning? In this study, assessment refers to any evaluative practice that is “part of everyday practiceby students, teachers and peers that seeks, reflects upon and responds to information from dialogue,demonstration and observation in ways that enhance ongoing learning” (cited in Klenowski, 2009, p. 268). Thedefinition is used to distinguish this type of assessment practice from others that focus on measuring students’academic achievement through standard tests, or final exams.In the following section, we present the method and procedures used in the literature review, followedby a detailed analysis of the reviewed studies. We then discuss the key issues and consider the implications ofthis study for assessment practice and further research.MethodsCriteria for inclusionThe following five criteria were established to select studies for inclusion in the analysis. (1) Empirical andpeer-reviewed journal articles published in English between 1994 and 2013. Non-empirical literature, reviewarticles, opinion articles, conference papers, dissertations, book chapters, and books were selected in the initialstages of the literature search to serve as sources of relevant research. However, they were not included in theCSCL 2015 Proceedings190© ISLSfinal analysis. (2) Studies conducted in K-14 education, excluding medical education. We chose K-14 ratherthan K-12 education because there is considerable variation in when postsecondary education starts. Forexample, the content of college courses can be similar to that in the final year of high school and many highschools offer university-level courses in the final year. (3) Studies that focused on educational and formativeassessment practices. Studies that focused on assessment practices that facilitate and transform learning, ratherthan those used for measurement or educational evaluation purposes were included.(4) Studies that usedtechnology to enhance student learning and assessment activities. We only included studies that involvedtechnology in some part of the learning and assessment activities, such as providing information and feedbackon performance, creating authentic learning contexts to increase learners’ engagement, creating opportunitiesfor collaboration, reflection, and self-regulation, and providing advice before and during the assessmentactivities. (5) Studies that provided a clear description of the methodological characteristics necessary for ouranalysis. We only included articles with a clear description of the following variables: the definition ofassessment, data source, sample sizes, treatment, research design, and outcome. If the outcomes were notsufficiently well defined or measured for us to assess the accuracy of the results, then the study was excluded.Search procedureThe literature search was conducted in a three-step process. First, an exhaustive search of peer-reviewedjournals was conducted using the EBSCOhost, ERIC, and PsycINFO databases. The following combinations ofdescriptors were used: formative assessment, self-assessment, peer-assessment, embedded and transformativeassessment, and reflective assessment. This search retrieved 123 studies based on the examination of the titles.Second, to locate other relevant studies, an exhaustive search of the major journals that publish research in thelearning sciences, specialize in assessment in education, and specialize in reviews of educational researchstudies was conducted. The literature search was conducted in learning sciences journals because this is anemerging interdisciplinary field that aims to improve formal and informal education by designing complexlearning environments and studying how learning is accomplished in them.These two steps retrieved 250 studies. After applying the selection criteria to the titles, abstracts, andresearch designs of these 250 studies, 46 studies were retained. This significant reduction was due to the limitednumber of studies conducted in K-14 educational contexts and the extended use of the term formativeassessment to refer to any kind of assessment adopted in the learning process. However, in most of these studies,assessment was only used summatively and students did not act as active agents who generated feedback,monitored their learning, and made further plans based on feedback/information to further transform theirlearning. In the third step of the literature search, ancestral and decadency searches were conducted byreviewing the references in the previously identified 46 articles and in relevant opinion and review articles toidentify additional relevant research studies. An additional 11 studies that satisfied the inclusion criteria wereidentified at this stage, increasing the number of studies to 57.Emergence of a coding frameworkTo determine how the design of assessment interventions can encourage guide (scaffold) learning and howscaffolds are used to make assessment practices work in a productive way, it was necessary to consider thedetailed documentation of each study and to analyze the core characteristics of the assessments. A threedimensional framework was developed to capture the core characteristics and features of such practices. By corecharacteristics we mean the goal/purpose of an intervention, the processes and activities planned to realize thegoal, the role of technology in the process and activities, and the evidence collected to demonstrate success inachieving the goal. The three-dimensional framework was determined on the basis of a preliminary analysis ofan initial sample of the articles, followed by further refinement after the preliminary analysis was presented toour research team.The constant comparative method (Strauss, 1987; Strauss & Corbin, 1998) was used to create thiscoding framework. This is an iterative coding approach (Bogdan & Biklen, 1998; Miles & Huberman, 1994)that usually involves examining each individual article, forming various categories, comparing categories, andachieving category saturation. On the basis of this iterative process, three dimensions with subcategories wereidentified. The three dimensions were presented to our research team, and researchers with experience of codingprovided feedback on the conceptual framework and the coding procedures. The conceptual framework for thereview was reconceptualized and reframed accordingly, and the coding process went through a second iterativeprocess. Finally, the following three dimensions with subcategories were identified: the nature of the assessmentintervention, the purposes of the assessment intervention, and the role of technology in student learning (seeTable 1). Each study received only one code in each of the three dimensions.CSCL 2015 Proceedings191© ISLSTable 1: Coding categories and descriptionsNature of assessment interventionCulminating activityContinuous assessmentDynamic assessmentIntrinsic component of learningDescriptionOne-time assessment activities that are connected or disconnected fromcurriculum or activities; the assessment requires student application of predetermined criteria or construction of assessment criteria to evaluate theirown or other’s work after initial content instruction or after completing thewhole or part of a scientific inquiry.Assessment activities in which students improve their work based oncontinuous feedback or information from teachers, peers, technologies, orthemselves. The feedback or information aims to narrow/close the gapbetween the present state and the desired goal (Bell & Cowie, 2001a).Activities in which players (e.g., researcher and student) interact in a guidedlearning situation in which the more experienced participant selects,focuses, and provides feedback on an environmental experience in such away as to create appropriate learning sets (Magnusson et al., 1997).Participants’ knowledge is assessed in the context of mediated learningsituations that attempt to foster conceptual change in a specific domain.Assessment activities are incorporated into a holistic learning and teachingframework, in which learners perform the assessment throughout theinquiry and learning process.PurposeAcademic performanceDisciplinary substanceLearning how to learnAssessment is used to facilitate content retention and learning, with content asa body of correct information.Assessment is used to assess and enhance students’ domain-specific ideas,thinking, and reasoning.Assessment is used to scaffold students to develop self-regulative andmetacognitive skills such as planning, monitoring, evaluation, andreflection.The role of technologyProviding/facilitating assessment andfeedbackMaking learning/assessmentactivities/tasks authenticCreating opportunities/spaces forgroup workProviding information/opportunities to encourage learnerreflection/self-regulationOffering regular and formative online tests, providing rapid and detailedfeedback/prompts; supporting teachers/tutors to write assessments andfeedback in a timely manner.Providing an authentic context in which learning and assessment activities areperformed; creating an assessment that is authentic to the concepts orcompetence being assessed or tested.Creating opportunities/space to facilitate collaboration and peer assessment;encouraging learners to stimulate and scaffold each other’s learning; avirtual learning community to enable learners to collaborate with eachother.Providing learners with information or opportunities that scaffold them toregulate their learning process toward an assessment goal, such as decodingthe feedback message, internalising it, and evaluating and modifying theirwork with it; offering information or opportunities to help students developreflective skills.ResultsOur review of the 57 studies indicated that assessment interventions were implemented in a variety of ways tosupport collaborative inquiry in technology-enhanced K-14 classrooms. The interventions ranged from thosethat used assessment as an added element of learning and aimed to improve academic performance by providingstudent feedback to those that emphasized assessment as an integral part of learning that helped students learnhow to learn. Within this variation, four orientations were evident: interventions as an instrument to improvestudents’ academic performance (outcome), interventions to facilitate collaboration in learning processes(collaboration), interventions as an instrument to help students learn disciplinary substance (disciplinarysubstance), and interventions that immerse students in the inquiry process to help them learn how to learn(immersion) (See Appendix).CSCL 2015 Proceedings192© ISLSInterventions to improve academic performance: Outcome-orientedIn eight studies (14%), interventions were used to enhance students’ academic achievement. These interventionsseemed to be guided by a tacit presumption that learning content consisted of a body of correct information,centered on terminology and measureable skills that was selected in advance as a learning objective. Theinterventions were used to narrow or close the gap between students’ present performance and some targetedoutcome. Interventions in this orientation facilitated assessment activities through scaffolds such as assessmentrubric, direct instruction, structuring the tasks, prompts, and assessment strategies. For example, Hume andColl (2009) reported on students’ use of rubric-referenced assessment to rate peers’ work. These assessmentrubrics, which had specific evaluating dimensions (e.g., format, timing, and reporting requirements) scaffoldedstudents to quantitatively rate peers’ work. Similar directional scaffolds were provided by a Web-basedassessment system (Wang, Wang, Wang, & Huang, 2006; Wang, 2007).Interventions often provided explicit instruction or working examples to foster students’ activeinvolvement in assessment activities. In the study by Fontana and Fernandes (1994), direct instruction coupledwith task structuring was used to facilitate students’ assessment activities. For instance, in regular selfassessment activities, students were instructed to understand both the learning objectives and the assessmentcriteria, and were given opportunities to choose and use the learning tasks that provided them the scope toevaluate their learning outcomes. In Ozogul, Olina, and Sullivan (2006), working examples combined withassessment criteria were used to scaffold students’ lesson plan writing.Facilitating collaboration in assessment activities: CollaborationThe second orientation, collaboration, helped students to develop collaborative skills or to facilitate productivecollaboration in assessment activities (eight studies, 14%). The intention was markedly different from theprevious orientation’s, and could be broadly described as focusing on collaboration rather than academicperformance. One strategy used to scaffold student collaboration in peer assessment was reducing problemssuch as carelessness and favoritism. For instance, Lai and Lan (2006) described the “negotiated agreement”approach and the use of computer agents to minimize subjective judgments and unfair assessments. Kao (2013)reported the use of peer assessment with positive interdependence (PAPI) to engage students in productivecollaboration. PAPI, which integrates the two approaches of positive interdependence and personalaccountability into the assessment process, was designed to reduce or eliminate carelessness and favoritism inpeer assessment and to improve the overall quality of peer reviews.Increasing peer interaction was another strategy used to support collaboration. Kwok and Ma (1999)applied an approach involving collaborative assessment in which GSS supported students and teachers’collaborative construction of evaluating schemes. Lin and Lai (2013) used social network awareness (SNA) topromote the opportunities of peer interaction and collaboration. The social network awareness, visualized in thesocial network awareness for a formative assessments system (SNAFA) system, enabled students to seek peeronline help and supported information sharing and co-construction of knowledge by keeping students aware ofpeers’ social and knowledge context. Roschelle et al. (2010) reported the use of group-level feedback coupledwith worked examples of productive collaboration to promote students’ collaborative behavior, such asdiscussion, explanation, cooperative negotiation, and group-level evaluation and feedback, scaffolded by asoftware program.Interventions to facilitate disciplinary substance learning: Disciplinary substanceTwenty-three (40%) interventions adopted the disciplinary substance orientation, which seemed to be guided bythe notion that assessment involves genuine engagement with disciplinary ideas, thinking, and reasoning. Theseinterventions engaged students in progressively constructing scientific theories (explanations) and/or developingdisciplinary thinking and reasoning skills in investigative and collaborative contexts. They facilitated students’engagement with ideas through scaffolds such as assessment rubric/assessment worksheets, prompts, creation ofinquiry contexts/tasks, and structuring learning tasks/activity.Interventions that used scaffolds such as assessment rubrics, assessment worksheets, and assessmentinstruments were used to enhance students’ development of disciplinary ideas, thinking, and reasoning. Forexample, Lin, Hong, and Lee (2011) described the use of worked examples of scientific explanations, and areflective peer assessment instrument containing six open-ended questions with competing theories, to supportstudents’ collaborative argumentation and conceptual understanding. Similar interventions were reported in Li,Liu, and Steckelberg (2010) and Toth, Suthers, and Lesgold (2002). Creating inquiry context or tasks wasanother strategy used to support student learning. For example, Chin and Teou (2009) used concept cartoons tocreate an inquiry situation to encourage students to discuss, articulate, question, evaluate, and reflect on theirown and their peers’ ideas, and to elicit their ideas, including misconceptions and argumentation, about theCSCL 2015 Proceedings193© ISLSscientific concepts. Etkina et al. (2010) designed conceptual design tasks, supported by the InvestigativeScience Learning Environment, which integrated cognitive apprenticeship and ongoing assessmentsupplemented by reflection to help students develop their scientific abilities. Students used assessment rubrics toself-assess their inquiry process and guide their experimental design and report writing.Provision of working examples and prompts was a third strategy. For example, Woo, Chu, and Li(2013) used online writing prompts to guide students’ group writing. Treagust, Jacobowitz, Gallagher, andParker (2001) engaged students in learning by asking them to respond to various questions during the activities.Structuring learning activities or tasks was a fourth strategy, used to direct students’ attention to disciplinarysubstance. For instance, in Taasoobshirazi, Zuiker, Anderson, and Hickey (2006) and Anderson, Zuiker,Taasoobshirazi, and Hickey (2007), a four-step review routine was used to foster students’ understanding ofastronomy and to develop their reasoning skills. The four-step review routine asked students to explain andcompare each answer, reach an initial consensus, review the explanation of the answer, and then confirm thegroup’s understanding. Students were encouraged to provide claims and use data to justify them.Immersion in inquiry process to help students learn how to learn: ImmersionThe final type of orientation (15 studies, 26%) used assessment to help students learn how to learn by improvingtheir metacognitive/self-regulative awareness and their abilities to monitor, evaluate, reflect, and re-plan. In thisorientation, assessment interventions were an intrinsic component of students’ inquiries; assessment wasembedded in the inquiry process and further transformed their learning. Interventions in this orientationfacilitated assessment activities using scaffolds such as assessment rubrics/principles/forms, models/examples,and prompts.The interventions that used assessment rubrics, assessment principles, and assessment forms weredesigned to help students learn how to learn. In a series of studies, Chang (2008) and Chang and Tseng (2009,2011) reported on the use of assessment forms embedded in a Web-portfolio system to foster students’metacognitive activities, such as self-/peer-assessment and reflection. The Web-based portfolio assessmentsystem itself provided students with a metacognitive model that fostered their engagement in a series ofmetacognitive activities, such as setting learning goals, writing reflections, creating their own portfolios,viewing and emulating peer projects, conducting self-/peer assessment, providing feedback, and continuouslyimproving their work. In another series of studies, Lee, Chan, and van Aalst (2006) and van Aalst and Chan(2007) described the use of a set of principles combined with e-portfolios to help students learn how to learn.These principles worked both as a conceptual framework to scaffold students’ inquiry, and as assessmentcriteria to guide their reflections on the quality of their work in preparing and reflecting on the e-portfolios.Through assessment, the students monitored, evaluated, and re-planned their inquiry processes and products.The provision of metacognitive models/working examples was another strategy used to help studentslearn how to learn. For example, Kostons, van Gog, and Paas (2012) used modeling examples to help studentsacquire self-assessment and task selection skills and further used the obtained skills to support students’ selfregulated learning. White and Frederiksen (1998) used a metacognitive model of research (the Inquiry Cycle)and a metacognitive process (reflective assessment) in a computer-supported curriculum to engage middleschool students in learning about and reflecting on their inquiry process as they constructed and appliedincreasingly complex causal models of force and motion. Prompts were a third strategy for engaging students inlearning how to learn. For instance, Wang (2011) described the use of five strategies (adding answer notes,stating confidence, reading peer answer notes, recommending peer answer notes, and querying peers’recommendations on personal answer notes) provided by a peer assessment system to help students foster selfregulated awareness and skills.The assessment orientations were derived from the nature of the interventions. The outcome orientationemphasized the improvement of students’ academic performance, as the assessment interventions were used asinstruments to narrow or close the gap between actual results and expected goals. The collaboration orientationtended to foster productive collaboration in assessment activities and to help students develop collaborativecapacity. The disciplinary substance orientation engaged students in the development of disciplinary substancein collaborative and investigative environments that mediated their learning. The immersion orientationincluded assessment activities that facilitated students’ development of metacognitive/self-regulative awarenessand skills for learning how to learn such as monitoring, evaluation, reflection, and re-planning. Three studies(5%) could not clearly be grouped into any of the four orientations.Discussion and conclusionsThis review explored how assessment practices in K-14 education are currently used to scaffold students’learning, with a view to articulating an agenda for research and development. We reviewed technologyCSCL 2015 Proceedings194© ISLSenhanced, cognitively oriented research on assessment in K-14 classrooms published between 1994 and 2013.The selected articles were coded using a three-dimensional coding framework that considered the nature of theassessment intervention, the purposes of the assessment intervention, and the role of technology in studentlearning. This framework was developed to capture the characteristics of the assessment interventions, and toprovide data for further analysis of how assessment interventions scaffolded student learning and whichstrategies made assessment activities work to scaffold learning.Documenting the core characteristics of each intervention in this way clarified how the outcome ofeach study was determined by its overall design rather than by a single core characteristic. The documentedcharacteristics also revealed how scaffolds, strategies, and methods were used to make a particular assessmentpractice work in a productive way in each study. This method differed from that of previous reviews (e.g., Blackand William, 1998), which reported either the results or the effect size of the assessment interventions, withlittle description of the assessment processes or assessment activities that the students were involved in.Furthermore, most of the previous reviews focused on higher education, whereas this review focused on the K14 educational context, which has received little attention. In addition, previous reviews of the literature havegenerally focused on interventions in which teachers played a central role in initiating assessment tasks andproviding feedback. However, this review analyzed a body of research dealing with assessment practices inwhich students, with technological support, assessed (through peer- or self-assessment), managed, evaluated,monitored, and reflected on their own collaborative inquiry.The review found that assessment interventions ranged from those that used assessment to providefeedback that could improve academic performance to those that helped students learn how to learn using aninquiry process focused on learning disciplinary substance; others used technology to provide information orfeedback and some integrated technology into the inquiry process. All of the interventions to some extentscaffolded students’ learning. However, only interventions in the immersion orientation seemed to fully helpstudents to learn how to learn. This approach seems the most promising for facilitating students’ agency in theinquiry process and to further transform their learning. The review also found that requiring students to monitor,evaluate, and reflect on their progress in light of criteria and principles related to learning goals had positiveeffects on their ability to learn actively, and to learn both disciplinary substance and metacognitive/regulativeskills. However, not many assessment interventions engaged students in metacognitive activities.The review should facilitate further research on the use of assessment to scaffold students’ learning.The findings provide evidence suggesting that some promising features (e.g., students’ engagement inmetacognitive activities in assessment practice) contribute to both students’ disciplinary substance learning andlearning how to learn. However, the limited assessment designs and findings may not be sufficient forresearchers and teachers to design strong and successful assessment activities to scaffold students’ learning. Wehope that more educational researchers will work toward this goal.References* References marked with an asterisk indicate studies included in the review (Not a full list of all of thereviewed studies).* Anderson, K. T., Zuiker, S. J., Taasoobshirazi, G., & Hickey, D. T. (2007). Classroom discourse as a tool toenhance formative assessment and practise in science. International Journal of Science Education, 29,1721-1744. doi:10.1080/09500690701217295Bogdan, R. E., & Biklen, S. K. (1998). Qualitative research for education: An introduction to theory andmethods. Boston: Allyn & Bacon.* Chang, C.-C., & Tseng, K.-H. (2011). Using a web-based portfolio assessment system to elevate project-basedlearning performances. Interactive Learning Environments, 19, 211-230.doi:10.1080/10494820902809063* Chin, C., & Teou, L. Y. (2009). Using concept cartoons in formative assessment: Scaffolding students’argumentation. International Journal of Science Education, 31, 1307-1332.doi:10.1080/09500690801953179Datnow, A., & Park, V. (2014). Data-driven leadership. San Francisco, CA: Jossey-Bass.* Etkina, E., Karelina, A., Ruibal-Villasenor, M., Rosengrant, D., Jordan, R., & Hmelo-Silver, C. E. (2010).Design and reflection help students develop scientific abilities: Learning in introductory physicslaboratories. Journal of the Learning Sciences, 19, 54-98. doi:10.1080/10508400903452876* Fontana, D., & Fernandes, M. (1994). Improvements in mathematics performance as a consequence of selfassessment in Portuguese primary school pupils. British Journal of Educational Psychology, 64, 407417. doi:10.1111/j.2044-8279.1994.tb01112.xCSCL 2015 Proceedings195© ISLS* Kim, M., & Ryu, J. (2013). The development and implementation of a web-based formative peer assessmentsystem for enhancing students’ metacognitive awareness and performance in ill-structured tasks.Educational Technology Research and Development, 61, 549-561. doi:10.1007/s11423-012-9266-1Klenowski, V. (2009). Assessment for learning revisited: An Asia-Pacific perspective. Assessment in Education:Principles, Policy and Practice, 16, 263-268.* Lee, E. Y. C., Chan, C. K. K., & van Aalst, J. (2006). Students assessing their own collaborative knowledgebuilding. International Journal of Computer-Supported Collaborative Learning, 1, 57-87.doi:10.1007/s11412-006-6844-4* Lin, H. S., Hong, Z. R., & Lee, S. T. (2011). Using reflective peer assessment to promote students’ conceptualunderstanding through asynchronous discussions. Educational Technology & Society, 14, 178-189.* Magnusson, S. J., Templin, M., & Boyle, R. A. (1997). Dynamic science assessment: A new approach forinvestigating conceptual change. Journal of the Learning Sciences, 6, 91-142.doi:10.1207/s15327809jls0601_5Miles, M. B., & Huberman, A. M. (1994). Qualitative data analysis: An expanded sourcebook (2nd ed.).Thousand Oaks, CA: Sage Publications.* Roschelle, J., Rafanan, K., Bhanot, R., Estrella, G., Penuel, B., Nussbaum, M., & Claro, S. (2010).Scaffolding group explanation and feedback with handheld technology: Impact on students’mathematics learning. Educational Technology Research and Development, 58, 399-419.doi:10.1007/s11423-009-9142-9Stahl, G. (2002). Computer support for collaborative learning: Foundation for a CSCL community. Mahwah,NJ: Lawrence Erlbaum Associates, Inc.Strauss, A. L. (1987). Qualitative analysis for social scientists. London: Cambridge University Press.Strauss, A. L., & Corbin, J. (1998). Basics of qualitative research: Techniques and procedures for developinggrounded theory (2nd ed.). Thousand Oaks, CA: Sage Publications.* Toth, E. E., Suthers, D. D., & Lesgold, A. M. (2002). “Mapping to know”: The effects of representationalguidance and reflective assessment on scientific inquiry. Science Education, 86, 264-286.doi:10.1002/sce.10004* Trautmann, N. M. (2009). Interactive learning through web-mediated peer review of student science reports.Educational Technology Research and Development, 57, 685-704. doi:10.1007/s11423-007-9077-y* Tseng, S. C., & Tsai, C. C. (2007). On-line peer assessment and the role of the peer feedback: A study of highschool computer course. Computers & Education, 49, 1161-1174. doi:10.1016/j.compedu.2006.01.007van Aalst, J. (2013). Assessment in collaborative learning. In A. O'Donnell, C. A. Chinn, C. K. K. Chan & C.Hmelo-Silver (Eds.), International Handbook of Collaborative Learning (pp. 280-296). New York, NY:FalmerRoutledge.* van Aalst, J., & Chan, C. K. K. (2007). Student-directed assessment of knowledge building using electronicportfolios. Journal of the Learning Sciences, 16, 175-220. doi:10.1080/10508400701193697* White, B. Y., & Frederiksen, J. R. (1998). Inquiry, modeling, and metacognition: Making science accessibleto all students. Cognition and Instruction, 16, 3-118. doi:10.1207/s1532690xci1601_2CSCL 2015 Proceedings196© ISLSAppendix:Patterns of assessment interventions in technology-enhancedK-14 education	  CSCL 2015 Proceedings197© ISLS