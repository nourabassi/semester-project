Detecting Collaborative Dynamics Using Mobile Eye-TrackersBertrand Schneider, Stanford University, schneibe@stanford.eduKshitij Sharma, EPFL, kshitij.sharma@epfl.chSébastien Cuendet, EPFL, sebastien.cuendet@epfl.chGuillaume Zufferey, EPFL, guillaume.zufferey@epfl.chPierre Dillenbourg, EPFL, pierre.dillenbourg@epfl.chRoy Pea, Stanford University, roypea@stanford.eduAbstract: Prior work has successfully described how low and high-performing dyads ofstudents differ in terms of their visual synchronization (e.g., Barron, 2000; Jermann, Mullins,Nuessli & Dillenbourg, 2011). But there is far less work analyzing the diversity of ways thatsuccessful groups of students use to achieve visual coordination. The goal of this paper is toillustrate how well-coordinated groups establish and sustain joint visual attention by unpackingtheir different strategies and behaviors. Our data was collected in a dual eye-tracking setupwhere dyads of students (N=54) had to interact with a Tangible User Interface (TUI). Weselected two groups of students displaying high levels of joint visual attention and comparedthem using cross-recurrence graphs displaying moments of joint attention from the eye-trackingdata, speech data, and by qualitatively analyzing videos generated for that purpose. We foundthat greater insights can be found by augmenting cross-recurrence graphs with spatial and verbaldata, and that high levels of joint visual attention can hide a free-rider effect (Salomon &Globerson, 1989). We conclude by discussing implications for automatically analyzingstudents’ interactions using dual eye-trackers.Keywords: joint visual attention, collaborative learning, dual eye-trackersIntroductionJoint Visual Attention (JVA) is a central construct for any learning scientist interested in collaborative learning.It is defined as "the tendency for social partners to focus on a common reference and to monitor one another’sattention to an outside entity, such as an object, person, or event" (Tomasello, 1995, pp. 86-87). Without jointattention, groups are unlikely to establish a common ground, take the perspective of their peers, build on theirideas, express some empathy or solve a problem together. As an example, autistic children are known to havedifficulties in coordinating their attention with their caregivers (Mundy, Sigman, & Kasari, 1990), which in turnis associated with social disabilities. Joint visual attention is a prerequisite for virtually all of social interactions.Previous work has highlighted the importance of joint attention in small groups of students: Barron(2000), for instance, carefully analyzed how groups of students who failed to achieve joint attention were morelikely to ignore correct proposals and not perform as well as similar groups. Richardson & Dale (2005) found thatthe degree of gaze recurrence between individual speaker–listener dyads (i.e., the proportion of times that theirgazes are aligned) was correlated with the listeners’ accuracy on comprehension questions. Jermann, Mullins,Nuessli and Dillenbourg (2011) conducted a similar analysis with a dual eye-tracking setup. They used crossrecurrence graphs (Fig. 1) to separate “good” and “bad” collaborative learning groups. They found that productivegroups exhibited a specific pattern (shown in Figure 1’s middle graphs), and that less productive groups produceda fundamentally different collaborative footprint (right graph of Figure 1). In a different study, Mason, Pluchinoand Tornatora (2015) showed that 7th graders who could see a teacher’s gaze when reading an illustrated textlearn significantly more than students who could not. Schneider & Pea (2013), using two synchronized eyetrackers in remote collaborations, showed that dyads who could see the gaze of their partner in real time on acomputer screen outperformed their peers in terms of their learning gains and quality of collaboration. Thisintervention was highly beneficial to students since they could monitor the visual activity of their partner,anticipate their contribution and more easily disambiguate vague utterances. There are many more examples ofstudies finding joint visual attention to be associated with interactions of higher quality. Due to space constraintswe will not conduct an exhaustive review of this line of research. The take away message is that JVA plays afundamental role in organizing social interactions, and measuring its frequency using a dual eye-tracking setupcan serve as a proxy for evaluating a group’s collaboration quality.This line of research has received renewed attention with the advent of mature eye-tracking systems. Themain focus of this paper is 1) to use dual eye-tracking data to understand how well-functioning dyads of studentsICLS 2016 Proceedings522© ISLSinteract; 2) to use cross-recurrence graphs to guide qualitative analyses; and 3) a foray into studying the varietyof ways that students use to maintain high levels of joint visual attention.Dual eye-tracking settings and cross-recurrence graphsPrevious work done by Jermann and his colleagues used cross-recurrence graphs to make sense of dual eyetracking datasets. A cross recurrence graph (shown on Fig. 1) is a visual representation of a dyad’s visualcoordination: the x-axis represents time for the first participant, the y-axis represents time for the secondparticipant, and black pixels show moments of joint attention (for a given time slice and distance between twogazes). Thus, a black diagonal signifies that the dyad was continuously looking at the same area of interest (e.g.,the middle graph in Fig. 1); the absence of a dark diagonal means an absence of joint visual attention (e.g., theright graph on Fig. 1). In other words, good quality interaction exhibits higher recurrence rates of JVA comparedto low quality interaction, and this difference can be visually detected using cross-recurrence graphs.Figure 1. Reproduced from Jermann, Mullins, Nuessli & Dillenbourg (2011): on the left, a schematic crossrecurrence graph; in the middle picture, a cross-recurrence graph from a productive group; on the right side, agraph from a less productive group.We use a similar methodology in this paper, with three new contributions: first, the data comes from astudy that looked at co-located interactions. We captured students’ gazes using two mobile eye-trackers, whereasprior work has solely looked at remote interactions where students were looking at two different computer screens.This methodological development is a significant improvement in ecological validity, because so much ofcollaborative learning is among co-located individuals. Second, we augmented cross-recurrence graphs withspeech data and spatial information to uncover students’ strategies when working on a problem-solving task. Thisprovided us with compelling visualizations that guided our qualitative analysis. Third, we contrasted two groupsthat each exhibited high levels of joint visual attention (in contrast with comparing a productive versus a nonproductive group, as Jermann and his colleague did). Our goal is to illustrate the multitude of ways that studentsuse to successfully establish joint visual attention. We found that dual eye-tracking datasets can uncover particulartypes of collaborations.In the next sections, we summarize the methods we used to collect our data. We then describe our twodyads of interest, and compare their cross-recurrence graphs. Finally, we provide quotes to illustrate crucialdifferences in the ways that those two dyads collaborated. We conclude with thoughts on the implications of thiswork, and future directions for studying joint visual attention in small collaborative learning groups.MethodsExperimental design, subjects and materialThe goal of our previous study was to conduct an empirical evaluation of a Tangible User Interface designed forstudents following vocational training in logistics. Our goal was to compare the affordances of 2D, abstractlooking interfaces (Fig. 2, left side) with 3D, realistic-looking interfaces (Fig. 2, right side) The system used inthis study, the TinkerLamp, is shown on Figure 2 (bottom row): it features small-scale shelves that students canmanipulate to design a warehouse layout. A camera detects their location using fiducial markers and a projectorenhances the simulation with an augmented reality layer. Fifty-four apprentices participated in the study (28 inthe “3D” condition, mean age = 19.07, SD = 2.76; 26 in the “2D” condition, mean age = 17.96, SD = 1.56). Theactivity lasted an hour and the goal for students was to uncover good design principles for organizing warehouses.The core of this reflection involves understanding the trade-off between the amount of merchandise that can beICLS 2016 Proceedings523© ISLSstored in a warehouse and how quickly one can bring an item to a loading dock (i.e., a lot of shelves make itdifficult to efficiently load/unload items, while few shelves limit the size of available stock). To help studentsunderstand this relationship, we followed the Preparing for Future Learning framework (Bransford & Schwartz,1999). We designed a set of contrasting cases (Fig. 2, first row) and asked students to analyze three layouts basedon the following criteria: in which warehouse they would prefer to work on a daily basis (prompt 1); whichwarehouse maximized space (prompt 2); and finally, which warehouse minimized the distance from each shelf tothe docks (prompt 3). At the end, the experimenter revealed the numbers for those two metrics (referred to as the“reveal” on Fig. 3). The contrasting cases were set up to highlight good design principles (e.g., orienting theshelves toward the docks makes them more accessible, placing them back to back so that only one side isaccessible frees up space, and so on). Students were supposed to use those principles in the next activity (Fig. 2,second row), where we asked them to build two warehouses: one, where they had to put as many shelves aspossible in a given layout, and a second one where they had to minimize the average distance to the docks givena certain number of shelves. Before and after those two activities, we gave students a pre- and post-test on paperwhere we asked them to slightly change the layout of several warehouses in order to optimize them. Thus we havetwo main dependent variables: how well they designed the two warehouses, and how well they answered thelearning test. During those two activities, students wore two mobile eye-trackers (SMI Eye-Tracking Glasses withbinocular pupil tracking at 30Hz) and we used the scene cameras (1280x960 pixels) to record videos for postprocessing. In a previous publication, we explain in detail how we used the fiducial trackers detected by the scenecamera to remap the dyad’s gazes onto a ground truth and determine whether two students were jointly lookingat the same location at the same time. We will not delve deeper in the general results of this study here; theinterested reader can learn more about it in Schneider et al. (2015). In the next section, we conduct a more indepth analysis of two dyads of students who achieved high levels of visual coordination, as measured by themobile eye-trackers.2D ConditionGREENBLUE3D ConditionREDGREENBLUEConstruction (~10 min)Analysis (~10 min)REDFigure 2. The two experimental tasks of interest: 1st row the contrasting cases students had to analyze; 2nd rowshows the Tangible Interface for the construction task. The “red”, “green”, “blue” labels are used to color-codeadditional analyses below (e.g., in the cross-recurrence graphs in Fig. 3)AnalysesWe chose to focus on groups 13 and 20 because they had the highest levels of joint visual attention. They alsoperformed above the average on the problem-solving tasks (but not on the learning test). Table 2 summarizes keyinformation about the two dyads:Table 2: Data of group 13 and 20.Group 13Group 20Task 113 shelves18 shelvesICLS 2016 ProceedingsTask 27.47 meters7.39 metersLearning gains7.5 points-0.5 points524JVA24 %29%Speech409 seconds540 seconds© ISLSM=13.5M=7.6M=1.3 SD=3.2 M=15.7M=405.7 SD=245.7SD=2.9SD=1.9SD=8.0As a reminder, the goal of Task 1 was to place as many accessible shelves as possible on a given layout. The goalof Task 2 was to minimize the average distance from each shelf to the docks, given a certain number of shelves.We computed learning gains by subtracting pre-test scores from the post-test scores, and then averaged them atthe group level. After running the study, we found that the post-test was harder than the pre-test. This explainswhy some learning gains are negative (e.g., group 20 in Table 1). JVA scores were computed by counting howmany times two gazes were in a radius of 70 pixels (i.e., the width of a shelf) within +/- 2 seconds of lag(Richardson & Dale, 2005); we then divided this count by the total number of data points in the eye-trackingdatasets. This gave us a number that reflects the percentage of time that two students were jointly looking at thesame location. Finally, we extracted speech information from the audio data to collect the number of seconds thateach student spoke. JVA and speech data are displayed in Fig. 3 for groups 13 and 20: on the top, we first showthe traditional black and white cross-recurrence graphs used in prior work (e.g., Jermann, Mullins, Nuessli &Dillenbourg, 2011); on the bottom, we show our cross-recurrence graphs augmented with spatial information (i.e.,which warehouse the two students were jointly looking at) and speech data (who spoke when during the analysistask).TotalAugmenting cross-recurrence graphsThe graphs of Fig. 3 provide us with several interesting bits of information about groups 13 and 20. First, we cannotice that the traditional cross-recurrence graphs are ideal for identifying moments of joint visual attention (blacksquares along the diagonal). Based on those graphs, one would predict that group 20 has a better visualcoordination than group 13: the diagonal is darker, with more well-defined squares, meaning that this group jointlylooked at the same area on the table more often and had longer moments of joint attention. Prior work suggeststhat an unfilled cross recurrence graph is likely to indicate a poor collaboration between group members. However,we would like to demonstrate that a “good” cross-recurrence graph (i.e., with a dark diagonal) is not necessarilyindicative of a good learning group. We will illustrate this difference by exposing some of the diversity that existsbetween groups of highly visually-coordinated students.Our augmented cross-recurrence graphs (middle section of Fig. 3) color-code each pixel to show whichwarehouse groups 13 and 20 were looking at (red pixels represent JVA on the 1st warehouse, green on the secondone, and blue on the last one). We also added dotted squares to show when the experimenter gave various promptsto the groups (i.e., prompt 1 = “in which warehouse would you prefer to work”, prompt 2 = “which warehousemaximizes space, and why”, prompt 3 = “which warehouse minimizes the average distance from each shelf to thedocks and why”, and reveal = “now I will show you numbers that answer those two questions to verify if yourintuition was correct”). Two very different strategies appear: group 20 analyzed the three warehouses in a serialmanner. For instance, after receiving prompt 2, they jointly looked at the first warehouse, then the second one,and finally at the last layout. Group 13, in comparison, continually compared the three warehouses. We do nothave empirical evidence that one strategy is superior to the other, but common sense and the PFL frameworkwould suggest that multiple comparisons between contrasting cases would increase students’ chances to noticeimportant features between those layouts. One indication that this strategy might be more beneficial to learning isthat group13 achieved higher learning gains on the test (indeed, they achieve the highest learning gains in ourexperiment). We plan to quantify this behavior in future work to see whether it positively correlates with learningacross all groups who participated in this study.Coming back to groups 13 and 20, there is one last piece of information worth mentioning. We knowfrom Table 1 that group 20 talked slightly more than group 13 (540 vs 409 sec.). On the bottom of Fig. 3, we showparticipants’ speech data. Observe that group 20 has more imbalanced participation (participant 39 talked for 117sec. vs 423 sec. for participant 40) compared to group 13 (in which participant 25 talked for 170 sec. vs 239 sec.for participant 26); thus, one group 20 student talked 78% of the time whereas one group 13 student talked 58%of the time. Might these differences be symptomatic of collaborative learning issues?To illuminate those results comparing groups 13 and 20, we qualitatively examined the videos of theexperiment. It should be noted that those differences (strategy used, learning gains and speech distribution) arealready a striking contrast to the supposedly superior (black and white) cross-recurrence graph of group20.ICLS 2016 Proceedings525© ISLSFigure 3: Cross recurrence graphs of two high-performing groups (13 and 20). The top figures show standardcross-recurrence graphs. The middle figures show colored graphs (red pixel = JVA on the 1st warehouse; green= JVA on the 2nd one; blue = JVA on the 3rd one). The bottom figures show speech data for each dyad.Qualitative analysisWe further compared our two groups by analyzing videos of their interactions. We generated videos by combiningthe scene cameras of the students (top left section of Fig 4) and remapping their gazes onto a ground truth duringtask 2 (bottom left section of Fig. 4). We also added an animation of the cross-recurrence graph on the right sideof Fig. 4, showing the graph up to that particular video frame. This video enabled a multi-modal analysis of thestudents’ interactions, in particular by highlighting how they combined gestures and speech to achieve and sustainjoint attention. Groups #13 and #20 were extremely similar in that regard, constantly using pointing gestures toground their verbal contributions. It is reasonable to believe that those deictic gestures played a large role inincreasing their levels of joint visual attention.ICLS 2016 Proceedings526© ISLSFigure 4: A video frame generated for the qualitative analysis. On the top left, we show the perspectives of thestudents with their gazes in blue and green (red dots show moments of JVA). On the bottom left, we remappedtheir gazes onto a ground truth. On the right side, we show the cross-recurrence graph up to that frame.In the table below, we focus on the major differences that we observed for the two groups. Specifically,we focused on one behavior found to be an important predictor of a group’s success: how peers react to a proposal(Barron, 2000). A proposal can be accepted, rejected, challenged, or ignored. One key difference between groups13 and 20 was that one dyad was more likely to uncritically accept proposals, while the other was much morelikely to challenge them (keywords highlighting this difference are marked in bold in Table 2):Table 2: Excerpts of group 13 and 20’s dialogues. (e.g., E=experimenter, 25 = participant 25)Group 1308:24 --> 09:38E: The first question is, in which warehouse wouldyou rather work, and why?25: This one!26: Yeah this one!25: if you look at this one, you have less palettes thanthat one26 Because of the width here...25: Not really!26: yes, it's wider25: but wait, here that's 4 shelves26: Here you can't get it out25: but you can't get from behind26: yes, that's what I'm saying […]25: 1,2,3,4 you're crazy. You have more space here.12:40 --> 13:52E: if you could change something in each warehouse,how would you improve them?26: Hmmm so...25: I would add two shelves, like that, there.Otherwise...26: What if we put those like that... to add moreshelves25: No; you can add two here, that's 18 additionalpalettes26: Otherwise... one here25: No, because then if you take a palette from here,you have to back off like that, even if someone'scoming from that directionICLS 2016 ProceedingsGroup 2000:08 --> 01:30E: The first question is, in which warehouse would youprefer to work, and why?40: I think this one is good (2nd warehouse), becauseyou can use half of the warehouse for loading and theother half for unloading39: yes but you can go faster in this one (1st warehouse)40: yes because in this one you only have space for twopalettes in front of the docks. It's a little tight. I thinkthat I would prefer to work in this one (2nd warehouse).Maybe I would like this one too39: there's enough space between the shelves40: yes, and in this one [ pointing at the 1st warehouse] the shelves are too tight08:12 --> 09:11E: if you could modify those warehouses to minimizethe average distance to each shelf, what would you do?40: What would I do? For instance, in this one (3rdwarehouse), I would move those shelves to the corners39: yes right here40: This one and that one, I put them here, and thosetwo (in the middle), I would put them there39: yeah40: No no, this doesn't make sense. It doesn't changeanything. I was thinking, those two you put them there39: yes40: but then you're too far away from this shelf39: well, it's not too bad.527© ISLSWe found that this difference was a recurring pattern in the videos and transcripts of groups 13 and 20.For group 20, participant 40 was extremely verbose and dominated the interaction by using many pointing gesturesto illustrate his thought process. This was crucial for the group, because it allowed his peer to maintain jointattention on the warehouse layouts. Participant 39, on other hand, was very passive and very rarely contradictedhis partner. His behavior is best described by the “free rider effect” identified by Salomon and Globerson (1989).In summary, the ideas generated by group 20 did not significantly change over the course of this activity becauseproposals were rarely challenged or contradicted.Group 13 provides a strong contrast to this dynamic. In this group, we found that participant 26 tendedto act like participant 40 (i.e., by driving the interaction and assuming a leadership role). This was mostlymanifested by the amount of speech shown on the bottom of Figure 3. But participant 25 was not the kind of freerider that participant 39 was; whenever he did not agree with a proposal, he challenged it until the group reacheda consensus. Those challenges were often initiated by using observations made on other layouts, which explainswhy the group was more likely to have some back and forth on different warehouse layouts (as shown on thecolored cross-recurrence graph in Fig. 3). Thus, a continuous refinement of their proposals seemed to be beneficialfor their learning, while group 20’s reflection stayed on a more superficial level.Detecting imbalances of participation in the eye-tracking dataAnalysis taskConstruction taskThe next question is whether this free-riding behavior pattern is a general one in other groups, and whether it canbe detected from the eye-tracking data. We propose one approach here; other measures might provide the sameresult. We started by identifying, for each moment of joint attention (i.e., each red dot on Fig. 4), which studentinitiated that episode. In our case, we define the initiator as the person whose gaze was most present in this areaduring the previous second. While we recognize that this definition is arbitrary, it is arguably a reasonable firststep in capturing leadership behaviors. We then computed the proportion of the JVA moments that each studentinitiated, and then applied this proportion to the percentage of JVA moments achieved by the group. We took theabsolute value of the difference between the score of each participant in a group to represent the (im)balance of agroup’s “visual leadership”. As an illustration, a group may achieve joint attention during 25% of their timecollaborating together; let us say that one student initiated 5% of those moments of JVA, while the other studentinitiated 20% of those moments. Following the computation above, this group would receive a score of abs(0.05– 0.20) = 0.15. This measure is shown on the x-axis of Figure 5.Figure 5: Negative correlation between students’ learning gains and their visual leadership (i.e., the differencebetween the percentage of moments of JVA initiated by each participant). Left side shows the scatter plot for theanalysis task (r=-0.33, p=0.10) right side shows the scatter plot for the construction task (r=-0.47, p=0.02).Green dots are dyads in the 3D condition, blue dots dyads in the 2D condition.Points on the right side of the graphs (with higher values) represent groups where one person was morelikely to initiate more moments of JVA; points on the left side of the graph (with values closer to zero) representgroups where both students were equally likely to either respond or initiate a moment of JVA. The y-axis showslearning gains computed at the group level. We found a negative correlation between dyads’ learning gains andthe absolute difference of students’ visual leadership during the analysis task: r(23)=-0.33, p=0.10 and theconstruction task: r(23)=-0.47, p=0.02. It suggests that groups who did not equally share the responsibilities ofinitiating and responding to offers of JVA were less likely to learn. This result shows that we could potentiallyrecognize a form of the “free-rider” effect by looking at the eye-tracking data in a collaborative setting.ICLS 2016 Proceedings528© ISLSDiscussionThis paper makes three contributions to the study of collaborative learning groups: first, it analyzes data comingfrom two mobile eye-trackers capturing co-located interactions. Previous work (e.g., Jermann, Mullins, Nuessli,& Dillenbourg, 2011; Richardson & Dale, 2005) studied remote collaborations where participants were indifferent physical spaces and collaborated remotely. This makes our contribution more ecological and opens newdoors for analyzing face-to-face and side-by-side interactions. Second, we showed that augmenting crossrecurrence graphs with spatial and verbal information provides researchers with new insights regarding students’strategies and interactions: color-coding them suggests whether a dyad is working in a serial or parallel mannerwhen they are analyzing contrasting cases. We hypothesize that the latter strategy is more beneficial to learning,as measured by our pre and post-tests. Furthermore, adding speech data to the graphs was crucial in our analyses.It allowed us to visually detect imbalances in the group’s interactions and dig deeper into their discussion. Thisobservation generated the paper’s third contribution: we found that highly coordinated dyads (as measured bydual eye-trackers) were not necessarily the best learning groups. Augmented cross-recurrence graphs revealedimbalances in students’ verbal contributions, which are characteristic of the free-rider effect where one studentdoes all the work while his/her partner stays passive. This difference was also observed by qualitatively analyzingthe videos: we found that the second student was less likely to challenge his peer’s proposals, which preventedthe dyads from refining their understanding of warehouse management. Finally, we extended those results to theentire sample and found a negative relationship between students’ learning gains and their tendency to share theresponsibility of initiating and responding to offers of joint visual attention.Conclusions and implicationsThe implications of this work are that cross-recurrence graphs are highly valuable for distinguishing betweenproductive and unproductive groups. But they should ideally be complemented with spatial and verbal informationto provide a more refined representation of a group’s interactions. Past a certain threshold, high levels of jointvisual attention make higher learning gains possible but they do not guarantee them. There are multiple ways inwhich student dyads can establish and maintain joint visual attention, which is a necessary but not sufficientcondition for socio-cognitive conflicts. This paper makes a first step in detecting the absence of those conflicts invisually coordinated students by identifying imbalances in students’ tendency to initiate or respond to offers ofjoint visual attention.ReferencesBarron, B. (2000). Achieving coordination in collaborative problem-solving groups. The Journal of the LearningSciences, 9(4), 403–436.Bransford, J. D., & Schwartz, D. L. (1999). Rethinking transfer: A simple proposal with multiple implications.Review of research in education, 24, 61-100.Jermann, P., Mullins, D., Nüssli, M.-A., & Dillenbourg, P. (2011). Collaborative gaze footprints: correlates ofinteraction quality. CSCL2011 Conference Proceedings., Volume I - Long Papers, 184–191.Mason, L.,Pluchino, P., & Tornatora, M. C. (2015). Eye-movement modeling of text and picture integration duringreading: Effects on processing and learning. Contemporary Educational Psych., 14, 172-187.Mundy, P., Sigman, M., & Kasari, C. (1990). A longitudinal study of joint attention and language development inautistic children. Journal of Autism and Developmental Disorders, 20(1), 115–128.Richardson, D. C., & Dale, R. (2005). Looking to understand: The coupling between speakers’ and listeners’ eyemovements and its relationship to discourse comprehension. Cog. Science, 29(6), 1045–1060.Salomon, G., & Globerson, T. (1989). When teams do not function the way they ought to. International Journalof Educational Research, 13(1), 89–99.Schneider, B., & Pea, R. (2013). Real-time mutual gaze perception enhances collaborative learning andcollaboration quality. Int. Journal of Computer-Supported Collaborative Learning, 8(4), 375–397.Schneider, B., Sharma, K., Cuendet, S., Zufferey, G., Dillenbourg, P., & Pea, R. (2015). 3D Tangibles FacilitateJoint Visual Attention in Dyads. CSCL2015 Conference Proceedings., Volume I, 158-165.Tomasello, M. (1995). Joint attention as social cognition. In C. Moore & P. J. Dunham (Eds.), Joint attention: Itsorigins and role in development (pp. 103–130). Hillsdale, NJ: Lawrence Erlbaum.AcknowledgmentsWe gratefully acknowledge support from the National Science Foundation for this work from the LIFE Center(NSF #0835854) as well as the Leading House Technologies for Vocation Education, funded by the Swiss StateSecretariat for Education, Research and Innovation.ICLS 2016 Proceedings529© ISLS