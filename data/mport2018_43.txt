Defining and Assessing Risk Analysis: The Key to StrategicIteration in Real-World Problem SolvingSpencer E. Carlson, Leesha V. Maliakal, Daniel G. Rees Lewis, Jamie Gorson, Elizabeth M. Gerber, andMatthew W. Easterdays.c@u.northwestern.edu, lmaliakal@u.northwestern.edu, daniel.rees.lewis@u.northwestern.edu,jgorson@u.northwestern.edu, egerber@northwestern.edu, easterday@northwestern.eduNorthwestern UniversityAbstract: Across domains from science to civics, experts plan to solve real-world problemsiteratively. Despite the importance of strategic iteration, we lack precise understandings ofeffective iterative planning and novice challenges, making it difficult to assess formativelyand therefore to teach. We conducted design-based research to understand iterative planningand design assessment tools in a full-time 6-week program where undergraduate teamsworked on social impact design problems. We found that iterative planning requires a processof risk analysis: detecting risks in the problem space, prioritizing those risks, and setting goalsto reduce them. Novices struggled with each step of risk analysis, so they did not planiterations strategically. We designed assessment tools that surface students’ thinking aboutrisk analysis and support instructors to notice common challenges. We contribute detailedunderstandings of iterative planning and novice challenges as well as tools that can be adaptedto assess real-world problem solving across domains.Keywords: problem solving, design learning, iteration, planning, assessment, risk analysisAcross domains—from science inquiry, to engineering, to civics—learning scientists are committed todesigning learning environments for teaching the highly complex, ill-structured problem solving that constitutesprofessional practice. This includes designing thoughtful assessments that provide instructors with criticalinsights on where students need help, but the challenges of assessing real-world problem solving are daunting.In this study, we developed tools (the Design Canvas, the Iteration Plan, and a planning rubric) for assessingstudents' planning in learning environments for real-world problem solving. We focus on planning because it isthe core of successful problem solving. Planning is unlike other problem solving activities (e.g., gatheringinformation, building and testing solutions, and analyzing data). Planning gives these activities a purpose; it ishow problem solvers decide which activities to do and how to interpret the results. Without thoughtful planning,problem solvers will conduct other activities aimlessly and fail to create successful solutions.Iterative planning to solve real-world problemsMost real-world problems cannot be solved in one try—they require iteration: the practice of testing andrevising ideas to continually improve one’s understanding of the problem and solution (Adams, Turns, &Atman, 2003; Wynn & Eckert, 2017). Scientists iterate on research questions to refine their contributions,engineers iterate on technical solutions to satisfy practical needs, and doctors iterate on diagnoses to prescribemore effective courses of treatment. Iterative planning—the process of strategically planning to iterate—is thuscritical to teach when preparing students for practice in problem solving disciplines.In iterative planning, professionals choose activities that enable them to continually improve both theirunderstanding of the problem and their solution (Adams et al., 2003; Crismond & Adams, 2012). For example, ascientist might iteratively improve their argument in a research paper by planning to collect, analyze, and writeup additional data. This approach to planning contrasts with teaching students to choose activities (like datacollection, analysis, and writing) based on a predetermined project schedule. Unfortunately, novices struggle toiterate strategically (Ahmed, Wallace, & Blessing, 2003; Crismond & Adams, 2012).Need for tools to assess iterative planningIf we want to teach iterative planning, we need formative assessments of iterative planning to help teachersdecide where to focus instruction. To assess iterative planning reliably, assessors need to make consistentdecisions about what information to seek when assessing performance, and how to assess it. Assessment toolscould support consistent decision making when assessing iterative planning.Previous work suggests assessing problem-solving by using performance rubrics and assessingargumentation. For example, it is common to assess the arguments students construct to justify their problemICLS 2018 Proceedings352© ISLSsolving (Cho & Jonassen, 2002; Shin, Jonassen, & McGee, 2003). Highly-ill structured problems like designproblems underlie professional practice in most disciplines (Jonassen, 2010). In these problems, assessing thesolution itself is difficult because the criteria for evaluating design solutions are highly subjective, change withone’s understanding of the problem, and are not fully known until the end of the design process (Jonassen &Hung, 2015). So, if we want to assess this type of problem solving, we cannot simply check whether studentsfound the right solution. Rather, we need to assess how reasonable a proposed solution is, given one’s currentunderstanding of the problem. However, assessing the quality of students’ arguments is an ambiguous taskwithout clearly defined criteria for argument quality, which makes reliable assessment more difficult. Toremedy this, Jonassen (2010) suggests using argumentation rubrics that define criteria for assessing students’problem solving performance.Given that iterative planning is a key process within problem solving, we hypothesize thatargumentation rubrics can also be used to assess performance in iterative planning. Like in solving designproblems, there is no single correct solution in iterative planning. There are poorly-justified plans, and there arebetter-justified plans. If we can create rubrics that define poorly justified versus well justified plans, theserubrics might support reliable assessment of iterative planning performance. However, prior research does notprovide specific guidance about tools and practices for constructing rubrics to assess iterative planning becauseiterative planning is currently an ill-defined task. And so we ask: how do experts reason through iterativeplanning and where do novices struggle? By answering this question, we can construct rubrics that effectivelyassess whether novices exhibit the desired performance.Need for novice and expert models of iterative planningUnfortunately, the literature tells us little about how experts (i.e., professionals) and novices (i.e., students)reason about iterative planning; that is, we lack expert and novice models of iterative planning that are detailedenough to create assessment rubrics. Experts solve design problems by iterating: refining their understanding ofthe problem and solution as new information emerges during problem solving (Adams et al., 2003; Adelson &Soloway, 1985; Atman et al., 2007; Guindon, 1990). However, we know little about how experts reason throughthe iterative planning process. Nor do we know what novices find difficult in that process. For example, a recentliterature review on expert and novice design practices noted the importance of iteration (Crismond & Adams,2012) but did not identify how experts carry out iterative planning. The same review established that novicedesigners undervalue and underutilize iteration, but did not identify why novices struggle to plan iteratively. Weneed to define how expert designers reason through iterative planning—and where in this reasoning processnovices commonly struggle—to create assessments of iterative planning performance.Research questionsThe goal of this project was to develop reliable tools for assessing iterative planning performance, andcorresponding expert and novice models which inform those tools. Specifically, we asked: (a) How do expertdesigners approach iterative planning? (b) What do novice designers typically struggle with when they attemptiterative planning? and (c) How can we assess design students’ performance in iterative planning?In this study we conducted 6 1-week iterations of data collection, analysis, and redesign to create toolsfor assessing student teams’ planning in design projects. We developed expert and novice models of iterativeplanning to design a rubric tool to assess planning. We also developed two tools for externalizing students’plans: the Design Canvas (DC), the Iteration Plan (IP). The DC and the IP are complementary, poster-sized (40in. x 60 in.) templates where students externalize the knowledge and reasoning they use to plan (for images seeour companion study: Rees Lewis et al., 2018). This happens during collaborative planning in a design team,similar to professional design practice (Osterwalder & Pigneur, 2010). The rubric guides assessors to detectstudents’ struggles by reviewing the DC and IP.MethodsContext and participantsWe conducted the research in a 6-week extra-curricular undergraduate summer program at a university designinstitute. In the program, each 4-5 person project team worked with a local community partner organization todesign products and services to address a given real-world challenge for the duration of the program. Challengesincluded: improving airport accessibility for autistic travelers, reducing air-travel-related wheelchair breakages,improving accommodations for people with dementia, increasing first responder support for youth, and reducingteen depression. Teams worked approximately 36 hours a week to create an original solution based on theirICLS 2018 Proceedings353© ISLSunderstanding of the problem. Each week, we prompted students to plan. First, students updated their DC so thatit reflected their current knowledge of the problem space. Then, students constructed and represented a planusing the IP.The study involved 21 undergraduates from 18-22 years old at a large private US university.Participants majored or double-majored in engineering (15), natural sciences (3), social sciences (3), art (3) andjournalism (3), and included 4 first-, 12 second-, and 5 third-year students (57% female).Data collection and iterative analysisWe collected 1 photograph per week of each team’s DC and IP, after the teams had planned. In total, wecaptured images of 30 plans across 5 teams. We also wrote field notes based on observations of team planningsessions. After each weekly planning session, we reviewed photographs and field notes to critique teams’planning. When we made a critique that was not already represented in the rubric tool, we added it to the rubricand wrote an analytic memo explaining why we would make a different planning decision. This process helpedus to articulate the normally tacit knowledge we use to think about planning decisions. In many cases, assessingthe quality of the plan required additional information, such as students’ reasons for choosing a certain goal.When we found we needed additional information (beyond what we saw in the Design Canvas or Iteration Plan)to assess a plan, we revised the tools to prompt students to share that information. We also wrote an analyticmemo explaining why we needed that additional information to judge the planning decision. Like with thememos we wrote to justify critiques, this enabled us to articulate elements of tacit knowledge that we use tothink about planning decisions. For example, while the tools ultimately centered on a critical process of riskanalysis (see Findings), the initial prototypes did not surface students’ thinking about risk. By analyzingstudents’ plans we recognized the centrality of risk in our own thinking, in part because it was impossible toevaluate students’ planning without understanding how they thought about risk in the problem space. This led usto begin articulating our understanding of risk in an analytic memo, and to add additional boxes to the DesignCanvas prompting students to explicitly identify risks.What authorizes us to define expertise? First, we are experienced designers with over 50 collectiveyears of design experience in industry and academic design-based research. Second, in a companion study ofthis learning environment (Rees Lewis et al., 2018), we tested instruction based on our understanding ofiterative planning. Students who received this instruction engaged in key design practices (e.g., iteration,interviewing users, testing ideas with stakeholders) more than students from the previous year (when we did notemphasize iterative planning). This suggests that learning iterative planning (as we define it) helps studentssucceed in design.Summative analysesAt the end of the program, we conducted summative analyses to test the final versions of our models andprototypes. One researcher used the final rubric to assess students’ plans from each week. This allowed us to testthe final novice model against data from each week of the program, and to aggregate our evidence for the finalnovice model. A second researcher used the final rubric to assess 5 of the plans we collected (17% of plans).This allowed us to calculate Cohen’s kappa (.82) between researchers to test reliability of the assessment tools.Figure 1. Task analysis of iterative planning. We found iterative planning requires risk analysis: detecting risksin the problem space, selecting high-priority risks to address, and setting near-term goals to reduce those risks.Novices struggled throughout iterative planning; this paper focuses on challenges with risk analysis.ICLS 2018 Proceedings354© ISLSFindingsWe found that risk analysis is a central reasoning process in strategic iterative planning. In risk analysis,designers review their mental or external representation of the problem space to assess, prioritize, and set nearterm goals to reduce risks in the problem space. We found that novice designers face challenges throughout theprocess of risk analysis. We found that the DC and IP successfully externalized students’ reasoning about riskanalysis such that we could tell where students struggled. Finally, we found we could assess students’ riskanalysis reliably using the assessment rubric.Strategic iterative planning requires Risk AnalysisAs we observed teams and assessed their Design Canvases and Iteration Plans, we reflected upon their planningdecisions. When we identified a decision that we would make differently, we wrote an analytic memo to (a)catalogue our critique of the team’s planning decision and (b) articulate our reasoning about why we wouldmake a different decision. In doing so, we found that our implicit expert model of iterative planning centered onthe process of risk analysis, in which designers detect risks in the problem space, prioritize these risks, and setnear-term goals to reduce high-priority risks (Figure 1). We found that iterative planning also involvesrepresenting the problem space and constructing the full plan (Figure 1), but we focused on risk analysis in thisstudy. We found risk analysis relies on several key concepts that novices must develop, including: a problemspace schema (Table 1); a causal model for detecting risk (Figure 2); knowledge of the process of risk analysis(Table 2: Experts ask themselves; Table 4: What experts do); knowledge of principles justifying the process ofrisk analysis (Tables 2 and 4: Why it matters); and knowledge of common sources of risk (Table 2: Commonsources of risk). The corresponding novice model identifies several aspects of risk analysis that students foundto be particularly challenging (Table 3; Table 4).Table 1: A problem space schema that the experts used to analyze iterative planning in social impact designProblem AspectsDescriptionCommunity Partner A person at a partner organization with expertise in the problem area (e.g., a relevant non-profit). The(CP)CP connects problem solvers with resources (e.g., information, access to users). The CP may alsoimplement designers’ solution, if they find it helpful.User Access PlanHow designers plan to access users (see User row) to learn about their needs.Demoing PlanHow designers plan to get regular feedback from the CP.Desired ImpactThe social impact that designers want to have.UserA persona who will use the proposed solution. Users have needs, which have 3 components: a “job”(a task users must complete), a “pain” (a challenge they face in that task), and a “gain” (the benefitthey will attain if they can complete the task). By satisfying user needs, problem solvers can enticeusers to adopt the proposed solution (and thereby promote the Desired Impact).Root CausesAn analysis of the underlying, fixable causes that explain why the user need, CP need, and desiredimpact are not yet satisfied.Value Proposition A proposed solution and an argument for how the proposed solution will overcome the root causes to(VP)satisfy the user need, CP need, and desired impact.Existing Solutions Designers need to account for existing solutions and argue why theirs is better.ImplementationExplains who will implement the proposed solution and how. Commonly, this involves handing offStrategythe solution to the CP, who integrates it into their existing operations.ImpactEvidence that designers have achieved the desired impact.The central concept in risk analysis is risk—the probability that the design project fails to make impact(i.e., change the status quo in a desired way). Designers’ concept of risk derives from an implicit causal modelof the factors affecting whether a design project makes impact (Figure 2). Teams increase the probability ofmaking impact by working on the causal factors that affect impact (Figure 2). For example, for the project tomake impact, the team must ensure that users adopt the designed solution, which means designing somethingthat users value, which requires understanding user needs (see the problem space schema in Table 1 forexplanations of user, community partner, and other terms). Likewise, a second causal chain focuses onunderstanding the needs of the community partner (CP), who also affects whether the designed solution isadopted. Furthermore, assuming the solution is adopted by users and CPs, the solution also must work asintended to achieve its desired impact. Each causal chain presents a possible risk to the impact of the project, soICLS 2018 Proceedings355© ISLSdesign teams iteratively plan activities that help them understand and address risks of user need, partner need,and solution efficacy.Figure 2. Experts (us) used this implicit causal model to detect risk in the problem during iterative planning.Risk exists when any of the variables in the causal model represent unmet conditions (e.g., designersdon’t know the root cause of the CP’s need, or the user doesn’t value the design solution) or unknownconditions (e.g, designers are unsure if they are right about the root cause, or they don’t know if the user valuestheir solution). Downstream variables are closer to making impact than upstream variables, so downstreamvariables have a more direct causal effect on making impact and thus matter more for risk. For example, solverscan only achieve a relatively small decrease in risk by working on the Know User Need variable, because evenif they understand the user need perfectly, there are many downstream variables that can interfere with makingimpact.Table 2: Expert model of risk detection in social impact designIdentify and estimate the level of risks associated with the Community Partner (CP).Experts ask themselves:Have we specified the CP’s need well enough to begin thinkingabout its root cause (or are there significant remaining risks)?Why it matters (connection to Figure 2: causal model of risk):Clearly articulating the CP’s need makes it much easier to thinkabout the fixable root cause(s) of that need, which makes it mucheasier to think of design solutions that will address the CP’s need.Precisely defining the CP’s need helps to narrow down themassive pool of potential design solutions, so solvers can avoidwasting time working on totally off-base solutions.Common sources of risk*:• We haven’t made contact with a real person at apartner organization• We can’t specify the partner’s need (either as aconcrete “job,” “pain,” and “gain,” or a clearlymeasurable social impact goal)• Our ideas about the partner’s need aren’t reasonable(they conflict with data and/or common knowledge)• When we cite data, we don’t specify both itscontent and its source* i.e., conditions that may threaten impact by interfering with the variables that lead to impact (shown in Figure 2)This explains why designers cannot sufficiently reduce risk by spending all their time understandingthe problem (by interviewing the partner and users, and defining desired impact): an understanding of theproblem does not guarantee a working solution. For this reason, designers try to move as quickly as possible tobuilding and testing potential solutions to see whether users and CPs actually value and adopt the solutions, andwhether the solutions actually achieve measurable impact. In other words, designers plan by weighing the risksinherent in different potential plans; they must judge whether they can achieve impact most quickly by buildingand testing a solution, or whether spending time understanding the problem will save precious time by avoiding“building the wrong thing.” These questions rarely have a clear right answer, if ever. However, designers use theprocess of risk analysis to discriminate between more and less reasonable answers, based on their knowledgeand experience. This involves detecting and prioritizing risks. In our study, expert designers (i.e., the authors)detected risks by analyzing aspects of the problem space, using knowledge of the process of risk analysis, theprinciples justifying it, and common sources of risk (Table 2). The full expert model of this knowledge does notfit within the page limit, but Table 2 provides an excerpt. The experts then prioritized risks and set goals usingadditional knowledge of the process and principles of risk analysis (Table 4).Novice challenges in risk analysisWe found that students struggled throughout the process of risk analysis. Students faced challenges in riskdetection (Table 3), risk prioritization, and goal setting (Table 4). In some cases, students skipped steps in theprocess of risk analysis (e.g., not attempting to detect risks). In others, students’ reasoning did not align acrossICLS 2018 Proceedings356© ISLSthe steps of risk analysis (e.g., setting a goal that did not address high-priority risks). In still other cases, studentsstruggled to complete the steps of risk analysis in a way that would be useful (e.g., setting a goal that is toovague to guide the rest of planning). Students also struggled to identify all of the risks we noticed in theirproblems, and they often struggled to make reasonable estimates of risk level.Table 3: Novice model of risk detectionNovice Challenges in Risk Detection (frequency observed)Don’tattempt todetect risksDon’t both identifyrisks and estimatethe level of risk.“Risks” identifiedare not trulyrelevant.Did not identifycertain salientrisks.Estimated level ofrisk isunreasonable.Community Partner (CP)2022272929User Access Plan1217232425Demoing Plan1424212927Desired Impact1821232825User1017192521Root Causes1424252823Value Proposition (VP)1220182626Existing Solutions815242726Implementation Strategy1122222628Impact2426263028Problem AspectsTable 4: Expert and novice models of risk prioritization and goal settingPlanning Step Expert ModelNovice ChallengesRiskPrioritizationWhat experts do: Experts review the risks they identified,prioritize those risks, and select a set of high-priority risks theycan address in 1 iteration (1-2 weeks). Often, experts look foreconomies of scope using a concept called slicing, whichinvolves creating a single plan to address multiple risks acrossthe problem. Further understanding of slicing is crucial, butbeyond the scope of this paper.Why it matters: Selecting the greatest risks from across theproblem space each iteration allows experts to shift theirefforts in response to new knowledge. This minimizes theirchances of failing to solve the problem.Do not attempt to select highpriority risks10Spontaneously generate risks(rather than selecting risksidentified by analyzing theproblem space)20Do not prioritize selected risks(among the high-priority risks,which are the most important toaddress?)25What experts do: Experts construct a goal that will guide the restof their planning. This goal takes the form of a conclusivelyanswerable question, or a falsifiable hypothesis or designargument.Why it matters: Having the right goal is crucial because goalsinform planning. Useful goals involve resolving questions andhypotheses about important unknowns in the problem space.Experts use such goals to craft plans that will reduce thebiggest risks threatening their success.Goal is not framed as a question,hypothesis, or design argument23Goal is too vague toconclusively reach it (e.g.,hypothesis is not falsifiable,question is not answerable)25Setting a goal that will notreduce the selected risk(s)15Goal SettingCountExternalizing students’ risk analysis using problem space templatesRecall that we designed 2 tools to surface teams’ reasoning about risk analysis: the Design Canvas (DC) andIteration Plan (IP). These tools are necessary because when an instructor or coach advises a team on theiriterative planning, they typically ask a series of questions to surface the team’s reasoning. Likewise, to assessthe reasoning behind students’ plans, we need a way to surface that reasoning. In risk analysis, this includesdesign teams’ knowledge of the problem space, such as their knowledge about existing solutions to the problem(Table 1). It also includes how design teams analyze this knowledge to identify risks in the problem space,select high-priority risks to address, and then construct near-term goals that will reduce those risks.ICLS 2018 Proceedings357© ISLSStudents planned each week by filling out the DC and IP. The final version of the DC works byexternalizing what students know about the problem, and what risks they have identified. The DC is a postersized template with boxes for each of the key schema components we identified as central to how the expertdesigners assessed risk in the problem space (Table 1). Students worked in their design teams to fill in the DCby placing sticky notes in each box to represent their knowledge about different aspects of the problem space.Each box in the DC also contained a section for risks, where students placed sticky notes identifying risks in thataspect of the problem space. In a subsection of the risks section, students could rate the risk level as low,medium, or high.The final version of the IP works by externalizing what students plan to do next, and why they think itis important. The IP is a poster-sized template with boxes for each key component of students’ plans. The IPincluded boxes for Selected Risks, Goals, Design Method, Metric, Criteria, and Tasks. For assessment, wefocused only on Selected Risks and Goals because these determine whether students have begun crafting a planthat links back to major risks in the problem space. In other words, this is the heart of iterative planning. To beeffective, students also need to construct coherent plans; they need to choose the right methods, set relevantmetrics and criteria, and accurately plan specific tasks. We have evidence that students struggled with thesesteps, but here we focus only on risk analysis; a discussion of how experts and novices construct coherent plansis outside the scope of this paper.Assessing students’ risk analysis by using a rubric on problem space templatesOnce students have externalized their reasoning using the DC and IP, we can assess it using a rubric thatsensitizes assessors to the concepts that experts use in iterative planning, and then directs assessors to check forspecific, common novice challenges. The rubric has 3 sections. The first section asks assessors to assess the riskin the problem space, and supports this by providing assessors with heuristics for risk detection taken from ourexpert model (Table 1). This is necessary before assessors can judge how well students have assessed risk in theproblem space. The second section asks assessors to check how well students have assessed risk. Assessorscheck for each of the challenges listed in our novice model of risk detection (Table 3). The third section asksassessors to check how well students have selected risks and set goals. Assessors check for each of the novicechallenges listed in our novice model of risk prioritization and goal-setting.The assessment tools were reliable when applied by different experts with similar disciplinaryknowledge. We achieved a Cohen’s kappa = .82 between two researchers testing the final rubric on 5 plans(17% of plans). For each criterion, we coded the plan “yes,” “no,” or “can’t grade.” Inter-rater reliabilityindicates that the rubric provides sufficient guidance for experts to make similar judgements about quality ofstudents’ plans. Our primary goal was developing expert and novice models and assessment tools. Therefore, weanalyzed all available data as we collected it to iteratively improve the expert and novice models and theassessment tools. We then calculated inter-rater reliability by re-analyzing a subset of that data. This is alimitation that we will address by testing the tools on fresh data in future work. Nonetheless, we argue that wecan have reasonable confidence that the tools are reliable because 2 researchers coded the 5 plans independently,we had not discussed specific coding decisions when we initially analyzed the data, and this was the first timeboth of us applied the final version of the rubric to data.Discussion and conclusionThis paper contributes: (a) a model of how experts in social impact design used risk analysis to inform iterativeplanning; (b) a model of novice challenges in risk analysis in social impact design; and (c) tools for surfacingand assessing novices’ risk analysis in social impact design.Our expert and novice models identify risk analysis: an important reasoning process that drivesiterative planning, and therefore strategic iteration. Our findings build on previous research which showed thatexperts use iteration to solve design problems, but did little to define strategic iterative planning, or novicechallenges in iterative planning specifically enough to guide assessment (Adams, Turns, & Atman, 2003;Adelson & Soloway, 1985; Atman et al., 2007; Crismond & Adams, 2012; Guindon, 1990). We found thatstrategic iterative planning requires risk analysis, in which designers identify risks in the problem space,prioritize those risks, and construct goals to reduce them. Likewise, by showing that novices struggle with eachstep in risk analysis, we provide a plausible explanation for why novices tend to under-utilize iteration (cf.Crismond & Adams, 2012). Also recall that our normative model of iterative planning seems to definepractically useful skills (Rees Lewis et al., 2018; see Data Collection and Iterative Analysis section).We have also demonstrated that it is possible, despite the ambiguity and subjectivity of designproblems, to assess risk analysis reliably using tools for externalizing and evaluating students’ reasoning acrosseach step of risk analysis. We have shown that it is possible not only to assess students’ reasoning about theirICLS 2018 Proceedings358© ISLSfinal solutions (cf. Jonassen, 2010), but also their reasoning about actions taken throughout problem solving—inthis case, reasoning about risk analysis. Additionally, while previous work proposed using argumentationrubrics to assess students’ reasoning (Jonassen, 2010), we add that it helps to create templates (the DC and IP)based on expert schemas to externalize the pieces of students’ reasoning you wish to assess. Future work shouldtest the fidelity with which the DC and IP represent and convey students’ thinking to assessors.Finally, our tools could support assessment of risk analysis in other disciplines (e.g. science or civics).While we designed tools for assessing risk analysis in social impact design problems, risk analysis does notseem specific to social impact design; rather, its function is making highly complex, ill-structured problemsmore manageable. Thus, it seems like a critical reasoning process for real-world problem solving in any domain.In this study, we developed tools for assessing students' planning in learning environments for realworld problem solving. Across domains—from science inquiry, to engineering, to civics—learning scientists arecommitted to designing learning environments for teaching the highly complex, ill-structured problem solvingthat constitutes professional practice. This includes designing thoughtful assessments that provide instructorswith critical insights on where students need help, but the challenges of assessing real-world problem solvingare daunting. Our tools overcome these challenges to enable instructors to understand where students arestruggling in the critical reasoning process of risk analysis. This may make it easier for instructors to helpstudents develop the knowledge and skills to design solutions to real-world problems.ReferencesAdams, R. S., Turns, J., & Atman, C. J. (2003). Educating effective engineering designers: the role of reflectivepractice. Design Studies, 24(3), 275–294. doi:10.1016/s0142-694x(02)00056-xAdelson, B., & Soloway, E. (1985). The role of domain experience in software design. IEEE Transactions onSoftware Engineering, SE-11(11), 1351–1360. doi:10.1109/tse.1985.231883Ahmed, S., Wallace, K. M., & Blessing, L. T. (2003). Understanding the differences between how novice andexperienced designers approach design tasks. Research in Engineering Design, 14(1), 1–11.doi:10.1007/s00163-002-0023-zAtman, C. J., Adams, R. S., Cardella, M. E., Turns, J., Mosborg, S., & Saleem, J. (2007). Engineering designprocesses: A comparison of students and expert practitioners. Journal of Engineering Education, 96(4),359–379. doi:10.1002/j.2168-9830.2007.tb00945.xCrismond, D. P., & Adams, R. S. (2012). The informed design teaching and learning matrix. Journal ofEngineering Education, 101(4), 738–797. doi:10.1002/j.2168-9830.2012.tb01127.xCho, K.-L., & Jonassen, D. H. (2002). The effects of argumentation scaffolds on argumentation and problemsolving. Educational Technology Research and Development, 50(3), 5–22. doi:10.1007/bf02505022Easterday, M. W., Rees Lewis, D. G., & Gerber, E. M. (2016). The logic of the theoretical and practicalproducts of design research. Australasian Journal of Educational Technology. doi:10.14742/ajet.2464Guindon, R. (1990). Designing the design process: Exploiting opportunistic thoughts. Human-ComputerInteraction, 5(2), 305–344. doi:10.1207/s15327051hci0502&3_6Jonassen, D. H. (2010). Learning to solve problems: a handbook for designing problem-solving learningenvironments. Routledge. doi:10.4324/9780203847527Jonassen, D. H., & Hung, W. (2015). All problems are not equal: Implications for problem-based learning. In A.E. Walker, H. Leary, C. E. Hmelo-Silver, & P. A. Ertmer (Eds.), Essential readings in problem-basedlearning: Exploring and extending the legacy of Howard S. Barrows (pp. 17-41). West Lafayette, IN:Purdue University Press.Osterwalder, A., & Pigneur, Y. (2010). Business model generation: a handbook for visionaries, game changers,and challengers. John Wiley & Sons.Rees Lewis, D. G., Gorson, G., Maliakal, L. V., Carlson, S. E., Gerber, E. M., Riesbeck, C. K., & Easterday, M.W. (2018). Planning to iterate: Supporting iterative practices for real-world ill-structured problemsolving, presented at the 13th International Conference of the Learning Sciences, London, 2018.Shin, N., Jonassen, D. H., & McGee, S. (2003). Predictors of well-structured and ill-structured problem solvingin an astronomy simulation. Journal of Research in Science Teaching, 40(1), 6–33.doi:10.1002/tea.10058Wynn, D. C., & Eckert, C. M. (2017). Perspectives on iteration in design and development. Research inEngineering Design, 28(2), 153–184. doi:10.1007/s00163-016-0226-3AcknowledgementsWe thank Delta Lab, Haoqi Zhang, and Alex Sher for their feedback and support. This work was funded by U.S.National Science Foundation grants IIS-1530883 and IIS-1320693.ICLS 2018 Proceedings359© ISLS