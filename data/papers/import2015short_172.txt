Comparing the Benefits of a Tangible User Interface andContrasting Cases as a Preparation for Future LearningBertrand Schneider, Stanford University, schneibe@stanford.eduPaulo Blikstein, Stanford University, paulob@stanford.eduAbstract: In this paper we describe an experiment that compared the use of a Tangible UserInterface (TUIs) in a constructivist fashion with a traditional learning activity. We carried outan experiment (N=40) with a 2x2 design: the first factor compared traditional instruction(“Tell & Practice”) with a constructivist activity (using the Preparation for Future learningframework; PFL). The second factor contrasted state-of-the-art PFL learning activity (i.e.,students studying Contrasting Cases) with an interactive tabletop featuring digitally-enhancedmanipulatives. In agreement with prior work, we found that students who followed the PFLactivity achieved significantly higher learning gains compared to their peers who followed atraditional “Tell & Practice” instruction (large effect size). A similar effect was found in favorof the interactive tabletop compared to the Contrasting Cases (small to moderate effect size).We discuss implications for designing constructivist activities using new computer interfaces.IntroductionOver the past decades, educational researchers have been advocating constructivist activities to fostermeaningful and deeper learning in STEM. This movement was an answer to the behaviorist movement that hasbeen prevalent in educational circles for decades (cf. “programmed instruction” Skinner, 1986), which promotedrepetitive exercises to reinforce students’ proficiency at following particular procedures, such as applying aparticular formula or algorithm to solve a mathematical problem. At least among researchers, over the lastdecades, there was widespread acceptance of constructivist theories. Constructivism “surfed” on a wave ofoptimism for many years before educators and researchers realized how difficult it was to design effectivediscovery learning activities. More recently, some scholars directly attacked this theoretical framework, callingit a failure (Kirschner, Sweller & Clark, 2006). The goal of this paper is to present a case of a successfulapplication of constructivist theories and an analysis of the mechanisms that led students to achieve higherlearning gains over traditional instruction. Additionally, we provide evidences that new technologies, such asTangible User Interfaces (TUIs) have the potential to efficiently support constructivist activities.Theoretical frameworkThe general theoretical framework of this paper is the idea that people learn best by using their prior knowledgeto make sense of a new learning material (Piaget, 1928); that is, students actively construct knowledge (asopposed to just receiving and accumulating it). In particular, it has also been shown that building artifacts(digital or physical), and going through a process of debugging mental models by externalizing them usingdifferent media, are especially powerful (Papert, 1980). This is a major contrast with the traditional “Tell &Practice” instruction (T&P) used in most classrooms, where students are first exposed to the “truth” and thenasked to practice their understanding of a particular concept on a series of exercises. Even though thisconstructivist view of the human mind is generally accepted among the scientific community and is seen asbeing beneficial to students’ learning, there are two main limitations when implementing this approach.First, students need to have some pre-existing knowledge that they can use to make sense of newconcepts; this approach falls short if students don’t have any prior experiences in the domain taught or if theydon’t have the opportunity to build some foundations prior to a lecture. It is likely that students in this situationwill resolve themselves to take plenty of notes and memorize as much of the teacher’s lesson as possible withthe hope that they will understand the content later. This scenario favors rote memorization and hinders transfer(Bransford & Schwartz, 1999). One framework that attempts to mitigate this problem is the Preparing for FutureLearning framework (PFL; Schwartz & Bransford, 1998). The idea is to provide students with an open-endedactivity prior to the lecture to allow them to construct some intuition about the concepts taught. Schwartz andBransford argue that Contrasting Cases (CC) are ideal candidates for this task. CC allow students to separatesurface and deep features of a problem and provides them with an opportunity for generating self-explanations(Chi, Bassok, Lewis, Reimann, & Glaser, 1989). This framework motivated the first comparison of our studybetween a PFL-style constructivist activity and a more standard T&P instruction.The second limitation is that efficient discovery learning activities are notoriously difficult to design(De Jong & Van Joolingen, 1998). It takes serious engineering and designing efforts to create an activity thatCSCL 2015 Proceedings134© ISLSengages and motivates learners, target specific learning goals, has affordances for conceptual reflection, workswith both high-achieving students and less proficient ones, and allow for productive failure (Kapur, 2008).Contrasting Cases, for instance, are an especially difficult case. Many believed that computer simulations andprogramming environments would bring a solution to this problem, by providing engaging virtual environmentswhere students could explore rich micro-worlds, and experiment with scientific and mathematic phenomena in ahands-on fashion (Papert, 1980). De Jong and Van Joolingen (1998) conducted a review of the variousempirical studies using computer simulations as discovery-learning tools and found some mixed (but mostlypositive) effects on students’ learning. We build on this prior work and extend this idea to new interfacesappropriate for hands-on learning: Tangible Users Interfaces (TUIs). TUIs are computer systems that replace thetraditional mouse and keyboard with physical objects, detect their states (such as their location) and provide afeedback loop to replace the screen with an augmented reality system (for instance by using a projector anddisplaying additional information directly on the objects). TUIs have specific affordances for constructivistlearning, by supporting students’ exploration of a complex domain (Schneider, Jermann, Zufferey, &Dillenbourg, 2011), students’ engagement and enjoyment (Shaer & al., 2011), small-group collaboration(Schneider & al., 2012) and hands-on activities in co-located settings (Dillenbourg & Evans, 2011).Our goal is to address those two limitations. First, we want to replicate previous results (Schneider,Wallace, Blikstein & Pea, 2013) showing that providing students with an opportunity to build prior knowledgewith a TUI before following a more standard type of instruction is beneficial to learning (compared to atraditional “tell-and-practice” approach, where students are first taught some concepts and then practice theirunderstanding of those concepts on a TUI). Secondly, we want to see how TUIs compare to contrasting cases interms of preparing students for future learning: in other words, is it worth spending time and energy buildinginteractive hands-on activities? Do they provide any benefits compared to traditional “pen and paper” activities?General description of the experimentWe designed the following experiment to investigate those two lines of research: in the control group, half ofthe participants first read an abridged version of a textbook chapter on the human visual system and thencompleted another activity where they had to apply their new knowledge (“T&P” condition). In the treatmentgroup, the other half of the participants first discovered those concepts in a hands-on activity and then learnedabout them in a more traditional way, i.e., by reading an abridged textbook chapter (“Invent” condition). Basedon the PFL framework, our main hypothesis is that the treatment group should achieve higher learning gainscompared to the control group. Additionally, we crossed another factor in our experimental design: the hands-onactivity was either a set of Contrasting Cases (CC) or a Tangible User Interface (TUI). We did not have a stronghypothesis regarding this comparison, but expected the TUI group to slightly outperform the CC group on thefinal learning test; as mentioned above, previous work suggests that TUIs facilitate exploration, increaseengagement and support collaborative problem-solving in small groups. Participants were counter-balancedacross conditions. We designed measures to look at three potential predictors for learning: engagement (using aquestionnaire with validated psychometric properties), curiosity (by having students list all the questions theywould like to see answered after the first activity), and quality of their mental models (by asking students todraw a model summarizing their understanding of the topic taught after the first activity). Our goal was to see ifstudents would differ on those measures between our different conditions.Concerning those measures (engagement, curiosity, quality of their mental models), our hypotheses areas follows: first, the main difference between the “invent” and “T&P” groups should be about the quality ofstudents’ mental model and their curiosity. Since the PFL framework is about helping students construct someprior knowledge, we believe that the PFL activity should help them build a mini-theory of how the human visualsystem works; this difference should be reflected in their drawings after the first activity. In this process,students should be more likely to ask themselves questions and develop their curiosity about the topic taught.This should then help them make sense of a standard instruction (reading a text about the visual system). On theother hand, we expect students in the control group (“T&P”) to focus on memorizing the content of the text andspending their time recalling this information when completing the second activity (i.e., using the TUI orworking on the CC). Secondly, we expect the main difference between the TUI and CC groups to be about theirlevels of engagement. Since TUIs have been shown to promote exploration and hands-on learning (Schneider,Jermann, Zufferey, & Dillenbourg, 2011), we believe that the physicality of the system should invite students tobe less intimidated by the complexity of the domain taught and explore the problem space to a greater extent.Those two hypotheses motivated the use of measures described below (i.e., middle test measuring the quality ofthe students’ mental model and their curiosity, and a questionnaire measuring their engagement). Finally, we didnot have any hypothesis regarding an interaction effect between our two factors (i.e., we don’t have any reasonto believe that CC or the TUI should have a differentiated effect on students in the PFL or T&P conditions.)CSCL 2015 Proceedings135© ISLSMethodsParticipants40 students from a community college participated in this study (13 males, 27 females; mean age = 21.28, SD =4.08). Students signed up for the study as part of a psychology class. The only prerequisite for participating wasto have no prior knowledge on the topic taught (neuroscience and the visual system). Students were randomlyassigned to each experimental condition.MaterialFigure 1: The set of contrasting cases used in the study. Answers are given for diagram 1, 3 and 6. Potentialanswers for the remaining cases are shown on the right column.This study included three different activities (Fig. 3). In the “invent” condition, students first explored thedomain taught with either a TUI or a set of contrasting cases. The TUI is described below (Fig. 2); the CC (Fig.1) included 6 diagrams of the human brain, each one featuring a different lesion. Half of the CC showed thecorrect answer to students (i.e., case 1, 3 and 6). The right column showed potential answers for the remainingcases. After finishing the first activity, students answered the following two questions: 1) “By the end of thisfirst activity, what are the questions that you would like to see answered about the way the human brainprocesses visual information?” and 2) “Please draw a simple model that summarizes your understanding of theway the human brain processes visual information.” During the second activity, students read an abridgedversion of a textbook chapter explaining how visual information is processed in the human brain (available at:http://goo.gl/47RIwv). Finally, they took a post-test that included questions on the terminology used (studentshad to correctly label different brain regions and neural pathways), on the effect of various lesions on the visualfield (given a particular lesion, students had to draw its effect on a person’s visual field), and on more generalscenarios involving human vision (transfer questions). Finally, we asked them to fill the engagementquestionnaire designed by O’Brien, Toms, Kelloway, and Kelley (2008). This questionnaire was developed forresearchers in HCI (Human-Computer Interaction) and measures six dimensions of an activity that relates tousers’ engagement (33 items on a Likert scale): focused attention (9 items; e.g., “I forgot about my immediatesurroundings while doing X”), perceived usability (8 items, e.g., “I felt frustrated / discouraged / annoyed whiledoing X”), aesthetics (5 items, e.g., “X was aesthetically appealing), endurability (5 items, e.g., “I consider myexperience with X a success”), novelty (3 items, e.g., “I continued to do X out of curiosity”) and involvement (3items, e.g., “I was really drawn into X”). In the T&P condition, the order of the learning phases was reversed(students first read the text and then completed the TUI or CC activity).The TUI used in this study is an improved version of a system previously developed in our lab(Schneider, Wallace, Blikstein & Pea, 2013), called BrainExplorer. BrainExplorer (Fig. 2) allows students tophysically manipulate a small-scale replica of a brain while an interactive tabletop displays visual pathwaysbetween brain regions. Users can then cut those pathways to create lesions and observe their effect on the visualfield of a subject. Two eyes (with a webcam) captured the field of view of this brain and shows occlusions onthe corresponding visual field.CSCL 2015 Proceedings136© ISLSFigure 2. The TUI used in this experiment (BrainExplorer). The system on the left shows the tangibles thatstudents can manipulate with the field of view of this brain (bottom right corner of the table). The image on theright shows two students interacting with the system during our study.DesignWe used a 2x2 between-subjects experimental design (Fig. 3). The first factor had two different hands-onconditions: the tangible interface and contrasting cases. The second factor sequenced the two learning activitiesin different ways: either with the hands-on activity first (“invent”  “read”) or second (“T&P”).Pre-test“invent” + TUIMiddle-testactivity 1TUIPost-testactivity 2TextCCText“T&P” + TUITextTUI“T&P” + CCTextCC“invent” + CC~ 5 min~ 15 min~ 10 min~ 15 minQuestionnairesand Debriefing~ 15min~ 5 minFigure 3. The four experimental conditions of our study (factor 1 = "Invent" versus "Tell-and-Practice" (T&P);factor 2 = Tangible User Interface “TUI” versus Contrasting Cases “CC”)ProcedureThe experimenter ran students in groups of two in a private room. Upon their arrival, the experimenterwelcomed them and told them that they would complete two small learning activities in groups. They were alsotold that the topic taught was about neuroscience and the human visual system. After filling a pre-test, studentscompleted two learning activities: In the “invent” condition, they first did the hands-on activity (i.e., TUI or CC)and then read a text about the visual system. In the “T&P” condition, they first read the text and then practicedtheir understanding of the topic on the hands-on activity. A second factor was crossed with those twoconditions: students either worked with the CC or TUI for the hands-on activity. Thus, referring to Figure 3,students in the “invent” + CC condition discovered those concepts with contrasting cases and then read a text.Students in the “invent” + TUI condition followed the same procedure except that they used the TUI instead ofthe CC. Students in the T&P + CC first read a text and then applied the concepts they just learned on a set ofCC. Students in the T&P + TUI followed the same procedure except that they reinforced their understanding ofthe visual system by using the TUI. Each activity was 15 minutes long.Between the two activities, the experimenter gave students two questions to answer individually (10minutes): first, they had to list the questions that they wanted to see answered about the human visual systemafter the first activity (i.e., a measure of curiosity); second, they had to draw a model summarizing theirunderstanding of the concepts taught (see the “material” section for more information)., students individuallycompleted a post-test (15 minutes) and were thanked for their participation.CSCL 2015 Proceedings137© ISLSCodingThe pre-tests and post-tests were coded in a binary fashion (1 point for a correct answer, 0 point for an incorrectanswer). For the middle test, we counted the number of questions students had and applied a simple ratingscheme to their models: 1 = no useful information shown; 2 = some useful information, mostly about theterminology used (no or little conceptual understanding of the effect of lesions on the visual field); 3 =significant signs of understanding of the way visual information is processed by the human brain. Figure 4provides an example for each category. Only one researcher coded the tests and the drawings, because thecoding schemes were simple and straightforward to apply.Figure 4. Three examples of models drawn by our participants. The model on the top left received 1 point (= noor little useful information); the one on the top right received 2 points (= some useful information, mostly aboutthe terminology); and the one on the bottom received 3 points (= clear signs of conceptual understanding).ResultsSince our samples are not independent (i.e., members of a dyad influenced each other) and since the intraclasscorrelation for the learning test is significant (p < 0.001; Kenny, Kashy & Cook, 2006), it is advised to conductanalyses at the dyad level. Since this reduced our statistical power, we will also report results where p < 0.1 witha moderate effect size. We also checked students’ Grade Point Average (GPA), since populations in communitycolleges are known to be heterogeneous. Because there was an interaction effect: F(1,16) = 4.46, p = 0.049 (Fig.5, left side) between the two levels of our factors, we used GPA as a covariate for the following analyses.Learning gainsThe results supported our two main hypotheses. Since participants did not score any points on the pre-test, weonly considered their results on the post-test. Scores are shown on Figure 5 (right side). An ANCOVA revealedthat students in the “Invent” condition outperformed students in the “T&P” condition: F(1,16) = 15.45, p < .001,Cohen's d = 1.41 (for the “invent” group, M = 12.95, SD = 3.15; for the “T&P” group, M = 7.85, SD = 4.06).Additionally, students using the TUI outperformed students in the CC group: F(1,16) = 5.32, p = .036, Cohen'sCSCL 2015 Proceedings138© ISLSd = 0.48 (CC: M = 9.35, SD = 3.90; TUI: M = 11.45, SD = 4.74). All distributions were checked for normalityand homogeneity of variance.         Figure 5. Left: distributions of students’ GPA. Right: Results on the learning test (Standard Errors shown). CCstands for Contrasting Cases, TUI for Tangible User Interface and T&P for “Tell-and-Practice”.Curiosity and mental modelsAdditionally, we looked at the effect of our two factors on the results of the middle-test (i.e., the number ofquestions students would like to see answered about this topic – a crude measure of curiosity – and the qualityof the model they drew). For the first factor, we found that students in the “Invent” group created significantlyhigher quality models compared to the students in the “T&P” group: F(1,16) = 7.38, p = .016, Cohen’s d = 1.32(for the “invent” group, M = 2.00, SD = 0.56; for the “T&P” group, M = 1.35, SD = 0.41). There was nosignificant difference in terms of the number of questions they asked themselves: F(1,16) = 2.75, p = .118,Cohens’d = 0.81. For the second factor (TUI vs CC), both comparisons were not significant (F < 1). We thencorrelated those two measures with our main dependent variable. Both measures were significantly associatedwith higher learning gains: r(20) = .40, p = .043 for the number of questions students asked themselves andr(20) = .55, p = .007 for the quality of students’ model.EngagementFinally, we looked at the engagement questionnaire administered at the end of the study (see Table 1). Wefound that participants in the “Invent” condition were more engaged (aggregate measure of all the items) thanthe students in the “T&P” group: F(1,16) = 6.1, p = .025, Cohen’s d = 1.05. Results were significant on the“endurability” sub-dimension (p < .05) and marginally significant on the “Focus” (p = .096), “involvement (p =.068), novelty (p = .066) and aesthetics (p = .071) sub-dimensions. Similarly, students in the “TUI” group weremarginally more engaged compared to the “CC” group: F(1,16) = 3.4, p = .064, Cohen’s d = 0.80. They foundthe TUI to be more usable (p < .05) and rated the endurability and aesthetics’ dimensions marginally higher (p =.108 and p = .114, respectively). Across all participants, being engaged was significantly correlated with higherlearning gains (p = .015); more specifically, involvement and endurability were the only sub-dimensionssignificantly correlated with students’ learning (p = .011 and p = .03, respectively).Table 1: Engagement scores between our experimental groups.Endurability AestheticsUsabilityEngagementF = 2.9F = 2.8F = 5.4F = 3.4p = .108p = .114p = .033*p = .064d = 0.45d = 0.73d = 0.85d = 0.80InventF = 3.1F = 3.8F = 3.9F = 4.7F = 3.7F<1F = 6.1vs T&Pp = .096 p = .068p = .066 p = .045*p = .071p = .025*d = 0.45 d = 0.71d = 0.72 d = 0.77d = 0.70d = 1.05Correlationr = .33r = .56r = .33r = .49r = .36r = .21r = .53with learning p = .152 p = .011*p = .154 p = .030*p = .122p = .380p = .015*Notes: Row 2 and 3: MANCOVA between the two levels of our two factors on the dimensions of theengagement questionnaire. Row 4 reports correlations with students’ learning gains. Degrees of freedom areindicated as above: r(20) for the correlations, and F(1,16) for the F-tests; d is Cohen’s d.TUI vs CCFocusF<1CSCL 2015 ProceedingsInvolvementF<1NoveltyF<1139© ISLSLinear regressionWe ran a linear regression to find how much variance of the learning gains our main predictors could explain(i.e., number of questions on the middle-test, complexity of students’ mental model, endurability, involvement).We found that the quality of students’ mental model was the strongest predictor (β = .43), followed by theendurability variable (β = .30), students’ involvement (β = .22) and curiosity (β = .12). Altogether, these fourvariables explained more than half of the variance of students’ learning gains: R2 = .58, F(4,18) = 4.78, p =0.012 (Table 2).Table 2: Linear regression with students’ learning gains as the dependent variable (R2 = .58)Variable NameEndurability (“students’ perception of success”)InvolvementCuriosity (Number of Questions Asked)Quality of Students’ Mental Modelβ.304.224.119.434t-testt(15) = 1.35, p = .199t(15) = 0.94, p = .361t(15) = 0.60, p = .555t(15) = 2.34, p = .035*DiscussionThe main goal of this paper was to show that technologically enhanced hands-on activities (i.e., TUIs) have thepotential to increase students’ learning in traditional constructivist activities. We also wanted to replicateprevious results showing the benefits of creating prior knowledge before receiving formal instruction. Ourresults suggest that PFL activities have a large effect on students’ learning. When prompted to explore a domainby themselves before reading an abridged textbook chapter, students developed more refined mental models,became more curious, more engaged and perceived themselves as being more successful (compared to thestudents in the T&P group); additionally, those differences were associated with higher learning gains. The factthat students created better models based solely on their analyses of the contrasting cases or by interacting withthe TUI is promising: it shows that providing students in the control group (T&P) with an already complete setof diagrams prevented them from creating their own model; worse, they didn’t even internalize the ideal modelfrom the textbook in a proper way. Additionally, our results suggest that using a Tangible Interface as apreparatory activity has a positive effect on students’ learning compared to studying CC. Participants learnedmore when using the interactive system and felt more engaged compared to the CC. Marginally significanteffects suggest that students using the TUI felt more successful with the task (“endurability”), which wascorrelated with higher learning gains. They also found the activity to be more aesthetically appealing and theTUI to be more usable, but those measures were not associated with higher learning gains.However, those measures do not provide us with the full picture; it is likely that the TUI had abeneficial effect on students’ learning beyond their level of engagement. We suggest a few hypotheses to betested in future work. First, BrainExplorer, and similar TUI-based exploratory systems, provide students with“on-demand” or “just-in-time” information about the visual system. From our qualitative observations, we sawstudents develop different strategies when using the system. Some were more confortable using a “bottom-up”approach (i.e., they started by analyzing the pathways from the eyes to the LGN, and then from the LGN to thevisual cortex); some others followed the same approach, but in reverse; finally some students started by makingas many lesions as they could, and then focused on more specific regions. In future work, we plan to comparethe variety of strategies used in both conditions (TUI vs. CC) to test this hypothesis. Secondly, it is possible thatusing a TUI had an indirect effect on students’ collaboration, which in turn had a positive effect on learning. Forinstance, it is conceivable that students found it easier to explore the problem space together (Schneider,Jermann, Zufferey & Dillenbourg, 2011), share information and build hypotheses (Shaer & al., 2011) with theTUI. We plan to code students’ quality of collaboration and correlate those measures with learning gains infuture work. Finally, It is possible that the results above were caused by a novelty effect; this is the most naturalexplanation for the significant effect found between TUI and CC, since most students had likely not interactedwith a tangible interface in the past. However, the statistical analyses performed on the “novelty” dimension ofthe engagement questionnaire did not support this explanation. Future work will look more closely at thispossible confounding variable.It is worth mentioning that we do not take those results as evidence that TUIs are better learningactivities than CC beyond the scope of this experiment. Both activities can take many forms, and their efficiencyis strongly influenced by a variety of design choices. Our findings merely suggest that the TUI introduced in thispaper (BrainExplorer) seemed to promote higher learning gains compared to the CC presented above. Insummary, more analyses are needed both in collecting qualitative segments suggesting explanation for theCSCL 2015 Proceedings140© ISLShigher learning gains found in the TUI (versus CC) condition, and in analyzing the logs of the system todetermine students’ strategies when exploring BrainExplorer.ConclusionThis paper presented a successful application of the “Preparing for Future Learning” framework to education ina complex field of knowledge (neuroscience). Our findings suggest that, under certain circumstances, minimallyguided instruction can be beneficial to learning. Our measures suggest that our intervention dramaticallyinfluenced the quality of students’ mental models, which had a positive effect on their learning gains. Thismeasure, associated with students’ curiosity, involvement and perception of being successful at the discoverytask, predicted more than half of the variance of their scores on the post-test. This shows the positive effect ofusing constructivist-inspired preparatory learning activities for learning scientific concepts. Those results,combined with others (e.g., Schwartz & Bransford, 1998; Schwartz & Martin, 2004), confirm that there is still aconsiderable gap between educational research (that advocates a constructivist view of students’ learning) andregular classroom instruction (that still prevalently use a “T&P” framework). The contribution of this paper is topropose a step toward closing this gap. We suggest combining the affordances the new technologies (e.g., TUIs)with existing educational frameworks (e.g., PFL) to provide students with compelling, carefully-crafted handson learning experiences that prepare them for future learning.ReferencesBransford, J. D., & Schwartz, D. L. (1999). Rethinking transfer: A simple proposal with multiple implications.Review of research in education, 61-100.Chi, M. T., Bassok, M., Lewis, M. W., Reimann, P., & Glaser, R. (1989). Self‐explanations: How studentsstudy and use examples in learning to solve problems. Cognitive science, 13(2), 145-182.Dillenbourg, P., & Evans, M. (2011). Interactive tabletops in education. International Journal of ComputerSupported Collaborative Learning, 6(4), 491-514.De Jong, T., & Van Joolingen, W. R. (1998). Scientific discovery learning with computer simulations ofconceptual domains. Review of educational research, 68(2), 179-201.Kapur, M. (2008). Productive failure. Cognition and Instruction, 26(3), 379-424.Kenny, D. A., Kashy, D. A., & Cook, W. L. (2006). Dyadic data analysis. Guilford Press.Kirschner, P. A., Sweller, J., & Clark, R. E. (2006). Why minimal guidance during instruction does not work:An analysis of the failure of constructivist, discovery, problem-based, experiential, and inquiry-basedteaching. Educational psychologist, 41(2), 75-86.O’Brien, H. L., Toms, E. G., Kelloway, E. K., & Kelley, E. (2008). Developing and evaluating a reliablemeasure of user engagement. Proceedings of the American Society for Information Science andTechnology, 45(1), 1–10.Papert, S. (1980). Mindstorms: Children, computers, and powerful ideas. Basic Books, Inc..Piaget, J. (1928) The language and thought of the child. New York: Harcourt.Schneider, B., Jermann, P., Zufferey, G., & Dillenbourg, P. (2011). Benefits of a tangible interface forcollaborative learning and interaction. Learning Technologies, IEEE Transactions on, 4(3), 222-232.Schneider, B., Strait, M., Muller, L., Elfenbein, S., Shaer, O., & Shen, C. (2012). Phylo-Genie: engagingstudents in collaborative 'tree-thinking' through tabletop techniques. In Proceedings of the SIGCHIConference on Human Factors in Computing Systems (pp. 3071-3080). ACM.Schneider B., Wallace J., Pea, R. & Blikstein P. (2013). Preparing for Future Learning with a Tangible UserInterface: the Case of Neuroscience. IEEE Transactions on Learning Technologies, 6(2), 117-129.Schwartz, D. L. & Bransford, J. D. (1998). A time for telling. Cognition & Instruction, 16, 475-522.Schwartz, D. L., & Martin, T. (2004). Inventing to prepare for future learning: The hidden efficiency ofencouraging original student production in statistics instruction. C&I, 22(2), 129-184.Shaer, O., Strait, M., Valdes, C., Feng, T., Lintz, M., & Wang, H. (2011). Enhancing genomic learning throughtabletop interaction. In Proceedings of the SIGCHI Conference on Human Factors in ComputingSystems (pp. 2817-2826). ACM.Skinner, B. F. (1986). Programmed Instruction Revisited. Phi Delta Kappan, 68(2), 103-10.AcknowledgmentsThe authors gratefully acknowledge grant support from the National Science Foundation (NSF) for this workfrom the Bifocal Modeling CAREER Award (NSF #1055130), as well as the Stanford’s Lemann Center forEducational Entrepreneurship and Innovation in Brazil.CSCL 2015 Proceedings141© ISLS