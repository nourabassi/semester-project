How Learning Outcomes are Measured in Digital LearningEnvironments in Higher EducationElke Kümmel, Leibniz-Institut für Wissensmedien, e.kuemmel@iwm-tuebingen.deGabriele Irle, Leibniz-Institut für Wissensmedien, g.irle@iwm-tuebingen.deJohannes Moskaliuk, Leibniz-Institut für Wissensmedien, j.moskaliuk@iwm-tuebingen.deJoachim Kimmerle, Leibniz-Institut für Wissensmedien, j.kimmerle@iwm-tuebingen.deUlrike Cress, Leibniz-Institut für Wissensmedien, u.cress@iwm-tuebingen.deAbstract: We investigated how learning outcomes are typically measured in empirical studiesof digital learning environments in higher education. A database search of articles publishedin peer-reviewed journals between January 2000 and May 2017 resulted in n = 356 articleswhose abstracts we screened for different types of dependent variables. We identified sevencategories of learning outcomes: Self-reports, observable behavior, learning skills, elaborationdepth, personal initiative, digital activity, and social interaction. We discuss opportunities forfuture research on the basis of these categories.Keywords: learning outcomes, digital learning environments, higher education, database searchDigital learning environmentsEven though several factors have been identified as being relevant for successful learning in digital learningenvironments (e.g., Tham & Werner, 2005), it remains largely unclear how digital learning environmentsimprove learning success (Al Zahrani & Laxman, 2016). At least in part, this lack of clarity may be due to theheterogeneous definitions of learning with digital media and the great variety of different measures of learningsuccess. Based on this assumption, we aimed to conduct a detailed review of prototypical approaches that areused for operationalizing learning outcomes in existing research on digital learning environments. Moore,Dickson-Deane, and Galyen (2011) analyzed existing literature to identify how current research defines digitallearning environments. Although the authors encountered a lack of consistency in the terminology, they foundfour core characteristics of digital learning environments: (1) the provision of learning materials independent oftime and space, (2) the broad access to learning materials, and (3) the support of educational opportunities (4)even for non-traditional learners. These characteristics are also highly relevant in higher education, resulting inincreasing importance of digital learning environments for higher education contexts (e.g., Bientzle, Griewatz,Kimmerle, Küppers, Cress, & Lammerding-Koeppel, 2015). Currently, it is evident that the approaches tomeasuring learning outcomes in digital learning environments are quite manifold, making it very difficult torecognize potential success factors of digital learning. We therefore set out to identify how previous empiricalresearch studies have measured learning outcomes in digital learning environments in higher education.MethodsThe aim of our study was to describe how previous research has measured learning outcomes in the context ofdigital media in higher education. We followed the procedure proposed by Cooper (2016) and identified fourrelevant thematic threads for our search in peer-reviewed journals: (1) digital learning environment, (2)instructional design, (3) higher education, and (4) performance criteria. The first thread of thought ensured thatwe would maintain a neutral perspective by finding as many studies as possible which dealt withoperationalization of digital learning environments independently of theoretical traditions. The second thematicthread focused on the instructional perspective, since we were interested in cognitive processes and properties ofdigital learning environments that might support learning and instruction. The two remaining threads restrictedour search to higher education and learning outcomes in that context. Learning outcomes of students are oftenreferred to as academic performance, so that the fourth thread aimed to identify performance criteria. Wesearched the database Web of Science and limited our results to English language articles of empirical studiespublished in peer-reviewed journals with a publication date from January 2000 to May 2017. We further limitedthe results to the top three Web of Science research areas (Education & Educational Research, ComputerScience, and Psychology) and the 25 most frequently represented journals. This procedure resulted in n = 1492records. We ranked these journals with respect to their impact factor and included only journals with an impactfactor equal to or larger than 0.8. The journal that was the most frequently represented was Computers &Education. Our next step was to conduct an abstract screen for the resulting n = 758 articles. We excludedqualitative studies that did not have precise operationalizations of learning outcomes as dependent variables.ICLS 2018 Proceedings1551© ISLSSimilarly, we excluded samples without students and studies without teacher instructions. We included datafrom the abstract screening of the remaining articles (n = 356). The features which were relevant to describinglearning outcomes were the particular dependent variables of the respective studies.ResultsThese data indicated seven categories with respect to digital learning environments. (1) Self-reports refer toaccounts by learners of their own attitude, satisfaction, or motivation. To evaluate learning outcomes, it isimportant to know how individual learners assess their learning outcomes based on their experiences andperceptions. (2) Observable behavior is the learners’ goal-orientated behavior, observed with the goal ofevaluating learning outcomes in a more activity-oriented way, such as the evaluation of learners’ intention tolearn, choice of lectures, and persistence. (3) Learning skills refer to metacognition (e.g., time management,reflection, or self-regulation), writing, reading or listening skills, as well as awareness of group processes,workspaces or persons, and even usage of technologies, software or tools. This list of skills is not exhaustive butrepresents variables of skills in higher education. (4) Elaboration depth refers to the amount of mental effortinvested, understanding, and comprehension, and cognitive load. This category represents cognitive informationprocessing. (5) Personal initiative as a core property of interacting with digital media represents thecommitment necessary for learning and learners’ impact on social interaction (e.g., participation, attendance,access, and amount of contributions to a discussion). (6) Digital activity represents active and adapted use ofdigital tools (e.g., searching and sourcing). (7) Social interaction refers to the influence of the involved learningcommunity on learners’ outcomes, for example by providing feedback, and through co-operation orcollaboration in group specific tasks (Jeong, Cress, Moskaliuk, & Kimmerle, 2017).DiscussionPrevious research has shown that a broad variety of variables are involved in learning with digital media inhigher education. The results of our analysis indicate that the definition of learning outcomes in existingresearch is correspondingly multifaceted and versatile. Even within the limited context of students in highereducation, the terminology for learning outcomes in empirical studies is multifarious and diverse. The datagathered for this research synthesis are preliminary results from screening abstracts. Some abstracts did notprovide sufficient information about sample, design, variables, or effects. It would therefore be important tocarry out a full-text screening of the empirical studies. This, in turn, could provide detailed descriptions ofcontext, designs of the studies, methods, and statistical analyses to provide a reliable evaluation of learningoutcomes with digital media in higher education. We have made a first step by describing measurements oflearning outcomes, but no overall conclusion concerning the relationship of these learning outcomes withparticular features of digital learning environments could be made and would definitely be required for futureresearch. The process of identifying independent variables that are associated with these learning outcomeswould also be a future significant step. Gathering and structuring these data would empower practitioners andscientists to be able to rely on the huge amount of results already in existence.ReferencesAl Zahrani, H., & Laxman, K. (2016). A critical meta-analysis of mobile learning research in higher education.Journal of Technology Studies, 42, 2-17. doi:10.21061/jots.v41i2.a.1Bientzle, M., Griewatz, J., Kimmerle, J., Küppers, J., Cress, U., & Lammerding-Koeppel, M. (2015). Impact ofscientific versus emotional wording of patient questions on doctor-patient communication in an internetforum: A randomized controlled experiment with medical students. Journal of Medical InternetResearch, 17, e268. doi:10.2196/jmir.4597Cooper, H. (2016). Research synthesis and meta-analysis: A step-by-step approach. Thousand Oaks, California:Sage Publications.Jeong, H., Cress, U., Moskaliuk, J., & Kimmerle, J. (2017). Joint interactions in large online knowledgecommunities: The A3C framework. International Journal of Computer-Supported CollaborativeLearning, 12, 133-151. doi:10.1007/s11412-017-9256-8Moore, J. L., Dickson-Deane, C., & Galyen, K. (2011). E-learning, online learning, and distance learningenvironments: Are they the same? The Internet and Higher Education, 14, 129-135. doi:10.1016/j.iheduc.2010.10.001Tham, C. M., & Werner, J. M. (2005). Designing and evaluating e-learning in higher education: A review andrecommendations. Journal of Leadership & Organizational Studies, 11, 15-25.doi:10.1177/107179190501100203ICLS 2018 Proceedings1552© ISLS