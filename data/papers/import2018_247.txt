Exploring Teacher Learning through STEM Teachers’ Explorationof Data Using a Domain Specific Coding LanguageDetra Price-Dennis, Teachers College Columbia University, price-dennis@tc.eduCharles Lang, Teachers College Columbia University, charles.lang@tc.columbia.eduAbstract: In the following paper we outline a method that endeavors to understand howteachers can systematically utilize data in classroom teaching. Informed by Cultural HistoricalActivity Theory and teacher education, we interview teachers about how they think about dataand use it in their everyday workflow. From these interviews we develop semantic models ofactions that teachers take in the classroom to learn about their students. We then convert thosesemantic models into computer code that can be utilized to both aid teachers’ exploration ofdata and study the teachers’ usage patterns.BackgroundWhen teachers engage meaningfully with the data produced by multiple sources (test scores, onlineprogramming, classroom observations) they can better respond to the educational needs of students andadvocate for them within educational systems. However, these efforts have been met with a mixed responsefrom teachers themselves. This challenge is a matter of balancing both (1) the capacity of teachers to processand utilize growing amounts of data in a meaningful way for instructional purposes and (2) the capacity ofsoftware developers and researchers to design teacher-centered, usable, and useful analytic tools. However,while teachers are increasingly collecting data from a variety of assessments and being asked to use this data toinform their instructional decisions, they have very little input about the type of data they collect nor thefrequency in which data is collected. This issue is compounded by the tremendous growth in the amount andvariety of data collected about students through the advent of online and mobile computing and use oftechnology in the classroom (Mandinach & Gummer, 2013). We are at the very early stages of utilizing thesenew forms of data for educational purposes (Ferguson, 2012) and are faced with the daunting task of bothdetermining the utility of new data sources, disseminating that information throughout education systems andensuring that teachers have the skills to interpret and use it to meaningfully impact instruction.As the sociotechnical landscape continues to evolve, opportunities for developing innovative STEMcurricula that denote multiple data collection points, informed by the needs of classroom teachers could createspace for evidence-based decision making that better supports daily instruction. To begin this process,classroom teachers need a systematic and intuitive way to collect, analyze, and disaggregate assessment data toinform real-time decision-making as they teach. This requires not only new skills, but flexible systems in whichteachers can creatively incorporate data into their practice. This is particularly true within STEM education thatis proved to be the vanguard of these technological changes. STEM is inundated by new technology-basedpedagogical aides such as online tutoring platforms for math and science (Kulik & Fletcher, 2016), blendedlearning applications (Tempelaar, Rienties, & Giesbers, 2015), Virtual Reality for whole body experiences(Potkonjak et al., 2016), and Augmented Reality laboratories (Chang, Chung, & Huang, 2016).Evidence based improvement cycles have been encouraged as part of teacher training programs forover a decade (Lewis, 2015). Despite this large scale implementation results have been mixed (Mor, Ferguson,& Wasson, 2015). Similarly, teacher utilization of the data dashboards that accompany technology products ishighly variable (Molenaar & Campen, 2017). In a substantial review of the utilization of data in educationMarsh (Marsh, 2012) outlines four sequential components of the practices adopted by teachers: data,information, knowledge, and action. Marsh is critical of the focus on the professional development of teacherdata skills absent focus on the translation of knowledge into action. However Mandinach and Gumner are waryof data dashboards that automate actions without teachers having gained the understanding or skills required tomake effective use of what is being presented to them. On the one hand, inquiry cycle-style strategies forincorporating data into teachers’ practice are too focused on skills and not on action, on the other handdashboards seem to be too focused on action absent skills. There is no in-between space in which actions andskills can be suitably matched.Theoretical approachFor this study, we draw upon Cultural Historical Activity Theory (CHAT) (Engeström, 2001; Vygotsky & Cole,1978) as a heuristic for analyzing how STEM teachers learn to develop data skills. CHAT scholars posit thatlearning must be viewed within sociocultural, historical and institutional contexts (Wertsch & Rupert, 1993).ICLS 2018 Proceedings1113© ISLSImportantly for this work are three aspects of the framework, 1.) that humans learn through actions, 2.) that theycommunicate those actions via tools and that community is essential to the act of learning. For the purpose ofthis study, those contexts converge in a school-based activity system in which classroom teachers draw upon amyriad of tools, rules, and community interactions to facilitate their learning about data literacy and data tooldevelopment. This theoretical stance attends both the conceptual and practical tools teachers bring to theirteaching, as well as the ideas, theories, and frameworks about teaching and learning.Computer science, and in particular the domain of computer language construction, has been engagedin the problem of communicating technical information about data with non-technical users for the last 50 years(Najd, Lindley, Svenningsson, & Wadler, 2016). In part, data-driven instruction is the most recent wave of aconversation about how technologies, designed and built by experts in those technologies, can communicatewith non-expert users to aid productive use. One particular area, that of domain specific languages, hasdeveloped similar more general models to that of Marsh that look to solve similar issues, broadly, to balanceuser needs with technical constraints (Evans & Szpoton, 2015).These models are language based, Fowler (2010) introduces a framework for thinking about therelationship between code, user actions and computer actions made up of three components:•Target code - the computer program that executes actions•Semantic model - the model that conveys meaning• Script - the user inputIn this model, what Marsh describes as the conversion of information into knowledge is represented as the“semantic model” or sometimes the “domain model”. In a programming language, the semantic model is arepresentation of the constructs that computer code populates. It is an abstraction or framework that links thereal world with the virtual world through code.If we overlay Marsh’s data use model with the Fowler’s computer language model (Fig.1) we canproduce a useful understanding of how a computer language could be used to organize data use by educatorsand the division of complexity between the teacher and the engineer. Within this framework, the semanticmodel is what converts data into information, it is how we organize our understanding of data. The semanticmodel could be informed by teachers, researchers and software developers. It might be as simple as “failure todo homework = unable to complete in-class activity” but could be hugely complex, incorporating theories ofvalidity, psychological or neuroscientific theories, teacher expertise, school contextual factors or culturalfactors. Target code is the machinery that does not need to concern the teacher as long as the script reflects thesemantic model accurately and the target code enacts that model with fidelity. In this way Marsh’s knowledge isrepresented in a script, the way that a teacher instructs a machine to behave in response to data. The scriptshould be an abbreviation of the semantic model, it converts the semantic model into instructions. This could bea menu item or button in a Graphical User Interface (GUI) but could also be a scripting language. The script isthe point of communication between the teacher and the engineer. A script that is both interpretable andaccurately reflects the semantic model is a very valuable tool.Figure 1. Mapping the Marsh data model onto the Fowler computer language framework.The value of a script over and above other implementations such as graphical buttons in a dashboard are manyfold. Scripts require less effort to generate than graphical elements and are easier to alter or retire if they are notuseful or do not reflect the semantic model accurately. Scripts are more extensible, they can be combined toproduce new functionality. They provide more autonomy for users to interact in ways that have not beenexplicitly considered by engineers. Script use can more easily be analyzed as it is already parsed intomeaningful chunks, unlike mouse movements on a dashboard. Scripts can also be more intuitive than otherICLS 2018 Proceedings1114© ISLSforms of graphical interface if they use words that have constrained meanings within a domain such as teaching.But most importantly scripts allow for a common language to exist between technical expert and domain expertso that ideas about functionality can be communicated more effectively (Fowler, 2010).MethodsIn the first phase of the project, we are gaining a deep understanding of the baseline data practices of a cohort of30 elementary STEM teachers by inductively generating semantic models through an iterative process (Glaser& Strauss, 1967). Teachers are interviewed about what they consider data to be and what they use it for. Theseinterviews work through hypothetical data scenarios to unearth and visualize a.) the data teachers currentlyengage with, b.) the data teachers want to engage with but do not have access to, and c.) the questions teachersuse to interrogate this data and d.) make meaning from it. We then engage in repeated readings of the interviewsand focus group transcripts to identity components of semantic models. Next, we will take the semantic modelsand reduce them to abbreviations. In follow up interviews we then test the interpretability of these scripts withthe original teachers and then with a cohort of 100 STEM teachers from different schools. These abbreviationswill then form the basis for the domain specific language.Preliminary resultsCurrently we have isolated three semantic models and preliminary abbreviations to accompany them. They arevisualized in Figure 2 as “summary.complete[X]”, “summary.error[X]” and “funnel[X]”,“summary.complete[X]” queries a database and produces two pieces of information: 1. How manystudents have completed a task X, 2. The names of students who have not completed the task X.“summary.error[X]” queries a database and produces two pieces of information: 1. The percentage correctfor the class and the lowest scoring students. “Funnel[X.Y.Z]” is a function that clusters students intogroups for task Y according to task completion on task X, then again on task Z according to completion on taskY. so as to direct the teachers’ attention to clusters of students who need specific help.Figure 2. “Funnel” model clusteringstudents based on performance on activityX, Y, Z.Future workWe hope to have a collection of 100 semantic models such as these by early 2018. Using the initial scripts, wewill develop a survey style instrument for a larger distribution to test the scripts on real teachers for difficulty ofinterpretation and complexity of analysis and provide feedback about their usability. Scripts that cannot beinterpreted by teachers in the new sample will be discarded or altered based on feedback. The validated scriptswill form the basis of the syntax, grammar and code for the domain specific educational programming languageand will be encoded in the Julia language (julialang.org). We will implement through an InteractiveDevelopment Environment capable of capturing teacher usage patterns of the language to study aspects ofteacher analytic practices.ICLS 2018 Proceedings1115© ISLSReferencesChang, R.-C., Chung, L.-Y., & Huang, Y.-M. (2016). Developing an interactive augmented reality system as acomplement to plant education and comparing its effectiveness with video learning. InteractiveLearning Environments, 24(6), 1245–1264. https://doi.org/10.1080/10494820.2014.982131Engeström, Y. (2001). Expansive Learning at Work: Toward an activity theoretical reconceptualization. Journalof Education and Work, 14(1), 133–156. https://doi.org/10.1080/13639080020028747Evans, E., & Szpoton, R. (2015). Domain-driven design. Helion.Ferguson, R. (2012). Learning analytics: drivers, developments and challenges. International Journal ofTechnology Enhanced Learning, 4(5–6), 304–317. https://doi.org/10.1504/IJTEL.2012.051816Fowler, M. (2010). Domain-Specific Languages (1 edition). Upper Saddle River, NJ: Addison-WesleyProfessional.Glaser, B., & Strauss, A. (1967). Grounded theory: The discovery of grounded theory. London:AldineTransaction.Kulik, J. A., & Fletcher, J. D. (2016). Effectiveness of Intelligent Tutoring Systems: A Meta-Analytic Review.Review of Educational Research, 86(1), 42–78. https://doi.org/10.3102/0034654315581420Lewis, C. (2015). What Is Improvement Science? Do We Need It in Education? Educational Researcher, 44(1),54–61. https://doi.org/10.3102/0013189X15570388Mandinach, E. B., & Gummer, E. S. (2013). A Systemic View of Implementing Data Literacy in EducatorPreparation. Educational Researcher, 42(1), 30–37. https://doi.org/10.3102/0013189X12459803Marsh, J. A. (2012). Interventions promoting educators’ use of data: Research insights and gaps. TeachersCollege Record, 114(11), 1–48.Molenaar, I., & Campen, C. K. (2017). Teacher Dashboards in Practice: Usage and Impact. In Data DrivenApproaches in Digital Education (pp. 125–138). Springer, Cham. https://doi.org/10.1007/978-3-31966610-5_10Mor, Y., Ferguson, R., & Wasson, B. (2015). Editorial: Learning design, teacher inquiry into student learningand learning analytics: A call for action. British Journal of Educational Technology, 46(2), 221–229.https://doi.org/10.1111/bjet.12273Najd, S., Lindley, S., Svenningsson, J., & Wadler, P. (2016). Everything old is new again: quoted domainspecific languages (pp. 25–36). Presented at the Proceedings of the 2016 ACM SIGPLAN Workshopon Partial Evaluation and Program Manipulation, ACM. https://doi.org/10.1145/2847538.2847541Potkonjak, V., Gardner, M., Callaghan, V., Mattila, P., Guetl, C., Petrović, V. M., & Jovanović, K. (2016).Virtual laboratories for education in science, technology, and engineering: A review. Computers &Education, 95(Supplement C), 309–327. https://doi.org/10.1016/j.compedu.2016.02.002Tempelaar, D. T., Rienties, B., & Giesbers, B. (2015). In search for the most informative data for feedbackgeneration: Learning analytics in a data-rich context. Computers in Human Behavior, 47(0), 157–167.https://doi.org/http://dx.doi.org/10.1016/j.chb.2014.05.038Vygotsky, L. S., & Cole, M. (1978). Mind in Society: Development of Higher Psychological Processes. HarvardUniversity Press.Wertsch, J. V., & Rupert, L. J. (1993). The Authority of Cultural Tools in a Sociocultural Approach to MediatedAgency. Cognition and Instruction, 11, 227–239. https://doi.org/10.1080/07370008.1993.9649022ICLS 2018 Proceedings1116© ISLS