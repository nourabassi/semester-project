Maximizing Benefit of Peer-Feedbackto Increase Feedback Uptake in Academic WritingAstrid Wichmann, Alexandra L. Funk, Nikol RummelAstrid.Wichmann@rub.de, Alexandra.Funk@rub.de, Nikol.Rummel@rub.deRuhr University BochumAbstract: Revising text is an essential part of the writing process. Yet, inexperienced writersrevise their texts too superficially. Peer-feedback has become a popular method to provideelaborate and timely feedback to students during writing. Though peer-feedback has shownpromising results, studies also indicate that students have problems benefitting from feedback,resulting in a lack of feedback uptake and little revision. Students might need to be facilitatedto make sense of feedback and reflect more deeply on it. The study investigated the effect ofsense-making support on revision skills and feedback uptake. Altogether, 73 universitystudents were assigned to a condition either with or without sense-making support. Resultsshowed no effect on revision skills, yet results of content-analysis yielded a significant effectconcerning feedback uptake. Students in the condition with sense-making support made lessnew errors and rejected more incorrect feedback. Sense-making support appeared to helpstudents to some extent to maximize benefit from peer-feedback.Keywords: peer assessment, feedback uptake, academic writingIntroductionWriting well is a challenging task for students. One central part of becoming a good writer is to understand theimportance of revision. Revision relates to any changes writers make during writing (Fitzgerald, 1987).Engaging in revision practices can have substantial impact on the quality of a written text as well as on learning.During revision writers re-organize ideas, integrate new ideas with existing ones to produce a coherent line ofargument (Fitzgerald, 1987). Problem detection and problem correction have been identified as crucial subprocesses for revising a text. During problem detection, gaps are identified between the intended text and themeaning of the text produced so far. During problem correction, the writer needs to decide what should bechanged in the text, how to make desired changes and how to instantiate those changes in the text (Flower,Hayes, Carey, Schriver, & Stratman, 1986; Hayes, 2004).Students’ problems to revise textsRevision is generally a difficult process (Scardamalia, Bereiter, & Steinbach, 1984). Especially inexperiencedwriters tend to accommodate too little time for revising a text in its draft state (Allal & Chanquoy, 2004) andrevise text superficially (Hayes, Flower, Schriver, Stratman, & Carey, 1987; Proske, Narciss, & McNamara,2010). The first step, detecting a problem is harder for students than for expert writers. Students have a lessclear goal in mind representing what the text should convey. Furthermore, students have problems to read theirtext from the perspective of the reader considering their audience. In addition, students follow less elaboratewriting criteria than expert writers (Graham, McArthur, & Fitzgerald, 2007). Yet, to identify a problem or anerror, a writer needs to have knowledge about criteria for good writing including knowledge about typicalwriting errors. The second step, correcting a problem is still challenging even if the problem is detected. Fixingerrors is not an automated process for inexperienced writers but a challenge on its own. As students lack thoseskills that are important for revision, they need help to become aware of problems in the text and need to receivesuggestions how to correct the text (Hayes, 2004).Peer-feedback in academic writingPeer-feedback has become a popular method in learning. The following activities belong to the overt activitiestypically included in peer-feedback: First, the assessee creates a product (task performance). Next, it is theassessor’s turn to provide feedback (feedback provision). Subsequently, the assessee needs to make sense andform a coherent picture of feedback (feedback reception). Lastly, the assessee revises his or her own productbased on feedback by the assessors (revision) (Kollar & Fischer, 2010). Receiving and providing feedback frompeers has been shown to be very effective and it has considerable advantages in comparison to feedback from aninstructor (e.g., Topping, 1998). Peer-feedback does not only complement instructors’ assessment (e.g.,CSCL 2015 Proceedings411© ISLSHammer, Ronen, & Kohen-Vacs, 2010; Zariski, 1996), it can be also provided more timely and frequently thanfeedback from an instructor (Falkichov & Goldfinch, 2000). There has been a shift in research on peer-feedbackfrom merely focusing on the reliability and validity aspects towards viewing peer-feedback as a social process.Providing and receiving peer-feedback can be inherently understood as a collaborative activity holding richlearning opportunities for both the assessor and assessee (Falchikov & Goldfinch, 2000).Especially in courses focusing on writing, peer-feedback can help students become better writers and togain understanding about subject-matter (Falchikov, 1986; Roscoe & Chi, 2007). During writing, a studentreceives comments from a peer describing the reader’s perspective, pointing out writing errors, makingsuggestions on how to revise the text. Assessors can help assessees to detect problems, which is a central subprocess for successful revision (Hayes, 2004). Though it is agreed that problem detection is necessary, it is notsufficient for correcting a problem. After a problem is detected, the assessee still needs to decide what to dowith the feedback in order to revise effectively. An assessee has various options: Either the feedback is rejectedbecause the problem pointed out is perceived as too trivial or too difficult to correct, resulting in no revision. Orthe feedback is considered to be relevant. As a consequence the assessee fixes the detected problem therebyrevising the text (Hayes, 2004).Assessees’ problems to leverage from peer-feedbackThere is clear evidence that students have problems to leverage from feedback. Students hesitate to use feedbackor feedback is rejected upfront without considering the information it contains (Boero & Novarese, 2012). Vander Pol and colleagues (Van der Pol, van den Berg, Admiraal, & Simons, 2008) call this problem a failure offeedback uptake. Feedback uptake is the ability to make use of feedback in such a way that it leads to changes inthe text. In other words, feedback uptake relates to revisions that are made based on feedback. For studentwriters, feedback uptake is important, because without considering feedback, students might struggle to detecterrors by themselves. Therefore we believe an important question to ask is: Why do assessees fail to benefitfrom feedback during writing? Research on peer-feedback indicates several problems that might preventfeedback uptake (e.g. Gennip, Segers, & Tillema, 2010; Nelson & Schunn, 2009).One problem concerns the limited knowledge how to handle feedback and the information it contains.Students may not know how to use feedback for the purpose of problem correction because as inexperiencedwriters they do not have a model of how to work through the problems systematically. Expert writers have theability to represent detected writing problems in a means-ends table that helps to correct problems with asystematic procedure which novice writers are lacking. Doing a means-ends analysis, expert writers have abetter understanding what actions need to be taken to correct problems and to successfully revise texts (Hayes,2004; Newell & Simon, 1972).Another problem concerns reflection of received feedback. Assessees do not sufficiently engage inreflection on their own. Yet, reflection has been identified as a crucial aspect for the process of acquiringknowledge and new skills (Zimmerman, 1989). During peer-feedback reflection is especially important forstudents taking the role of the assessee. Reflecting helps assessees to maximize potential benefits fromfeedback. Reflecting consists of several processes including (1) planning, (2) monitoring and (3) evaluation(Schraw, 1998). Those processes are central for reflecting on feedback. Planning includes understanding one’sown knowledge of feedback. This involves that assessee knows how information contained in the feedback(assessors’ intentions) relates to the meaning conveyed in the text (assessees’ intentions). Monitoring includeskeeping track of feedback that is agreed upon and feedback that is rejected. Evaluation includes judging whichfeedback is rejected and which feedback will be used for improving the text.Supporting the assessee to benefit from peer-feedbackWhen integrating peer-feedback in instruction both problems, (1) limited understanding of how to representproblems in a means ends table and (2) lack of reflection on feedback need to be considered. Integrating peerfeedback alone might not be sufficient to ensure feedback uptake. Instructional support might be needed forlearners’ learners to benefit from peer-feedback to succeed in feedback-uptake. Combining peer-feedback withinstructional support might maximize effects of feedback in terms of feedback uptake. Instructional supportshould tackle the mentioned problems: First, assessees should be facilitated to represent problems as part of ameans-ends table. Doing so should help students to represent the relation between detected problems andactions that need to be taken to fix a problem in the text. Second, assessees should be supported to reflect moredeeply on feedback. Similarly to learning protocols (Berthold, Nückles, & Renkl, 2007), assessees should writedown their reflections on previously presented peer-feedback. Support should instruct the assessee to thinkabout whether the feedback was understood, whether there is a gap between the assessor’s intentions and theCSCL 2015 Proceedings412© ISLSassessee’s intentions, whether feedback is considered to be used and how it will be used to improve the text. Inother words, instructional support should encourage the learner to make sense of feedback.There is substantial evidence that taking the role of the assessor by providing feedback and assessingproducts created by peers, leads to learning gains (Topping, 2003). Yet, there is little empirical evidence thattaking the role of the assessee that is to receive feedback leads to learning gains as well (Van der Pol et al.,2008; Cho & MacArthur, 2010; Kluger & DeNisi, 1996). This study focuses on supporting the assessee. In ourstudy, we explore whether instructional support, in form of sense-making support helps assessees to maximizebenefits from feedback. Based on our assumptions, we expected that sense-making support improves assessees’revision skills (Hypothesis 1). Since revision skills were subdivided in problem detection and problemcorrection, we hypothesized that both sub-skills will improve if sense-making support is provided. Furthermore,we expect that sense-making support will facilitate feedback uptake (Hypothesis 2). Since feedback uptake wasconceptualized as changes that assessees make based on received feedback, we assumed that studentsincorporate more feedback-based revisions in the text.Study designParticipantsAltogether, 73 students (12 male, 61 female) from a German university participated in the study. Studentsstudying towards a degree in education served as participants. The mean age was 23.37 (SD=3.37). They wereenrolled in introductory and advanced courses in education. Participation was an obligatory part of their regularclass activities, however no grades were given for the writing activity. Participants were randomly assigned toone of two conditions: Sense-making support condition SMS+ (N= 34) and no-sense-making support conditionSMS- (N= 39). Conditions differed only concerning presence of sense-making support.Learning materialsStudents participated in an online writing activity. The task was to write an essay of 550-650 words on thequestion “How can a mother optimally support the identity formation of her child”. As background information,students received an excerpt of a text from Erikson (1984) on identity formation. The task was accompanied bypeer-feedback.Description and implementation of sense-making support (independent variable)Sense-making support was delivered during feedback-reception in a MS WORD-document. Students in theSMS+ condition were asked to use the document during peer feedback reception. The document included a tablewith seven columns (see Figure 1). Sense-making support aimed at encouraging participants to reflect onunderstanding the feedback involving planning, monitoring and evaluation of feedback. The first column waslisting each feedback comment (column 1). Students were instructed to list each feedback comment using thecopy/paste function. Participants were asked to judge each comment in the list concerning understanding(column 2), agreement (column 3), and impact on text (column 4) and to indicate how the comments will impacttheir text (column 5), organization (column 6) and relevance (column 7).1	  	  Copy	  your	  received	  comments	  into	  this	  column.	  2	  	  I	  understand	  the	  comment.	  3	  	  I	  agree	  with	  the	  comment.	  4	  	  I	  am	  going	  to	  use	  this	  comment	  	  5	  	  I	  will	  improve	  my	  essay	  by	  doing	  the	  following:	  6	  	  Done	  Yes	  No	  Yes	  No	  Yes	  No	  Comment	  1	  ☐	  ☐	  ☐	  ☐	  ☐	  ☐	  	  ☐	  7	  Mark	  the	  three	  most	  important	  comments	  with	  an	  X.	  	  …	  ☐	  ☐	  ☐	  ☐	  ☐	  ☐	  	  ☐	  	  	  Figure 1. Sense-making support tableProcedure: Online-writing activity with peer-feedbackThe online-writing activity was conducted during a period of two weeks (see Figure 2). The learning platformMoodle was customized for the online-writing activity (Moodle, 2013). We followed the structure of the peerfeedback phases identified by Kollar & Fischer (2010) focusing on the role of the assessee.CSCL 2015 Proceedings413© ISLSFigure 2. Online-writing activity with instruments and peer-feedback phasesDuring task performance, students received instruction and background information to write the essay(see Table 1). Students were asked to upload the essay as WORD document. Feedback provision was secludedbecause all participants received feedback from tutors but they did not provide feedback themselves. Duringfeedback reception, participants received peer-feedback. During revision, the essays including feedbackcomments were made available and participants revised their essays. Afterwards, participants uploaded theirrevised documents to Moodle. Participants were guided through each phase on a step by step basis. Each phasebecame active, when the previous one was completed.Table 1: Online-writing activity including times for peer-feedback phasesPeer Assessment PhasesTime on task (min)Task Performance120Feedback Reception10Revision80As described above, all participants took the role of the assessee. Participants were informed that thefeedback was given by a peer. However, in order to control for variance of feedback quantity and quality,feedback was given by trained tutors. Peer-feedback included 12 comments for each participant. Each commentreferred to one error in the text. It included a standardized description of the error and a suggestion how torevise it (see Figure 3).Figure 3. Examples of highlighted commentsEach comment identified one out of 5 writing errors related to Sequence/Logic of Argument,Transition Words, Nested Sentences, Direct/Clear Reference and Filler Words. The content of a comment wasstructured such as: “This sentence is hard to read. Rephrase it in order to make it more readable”. Each studentreceived two comments per writing error (10 comments all together). Additionally, two incorrect commentswere included, because peer-feedback is prone to be erroneous. Feedback was given in the WORD documentsusing the commenting function of MS Word. For each writing error we highlighted the relevant portion of theCSCL 2015 Proceedings414© ISLStext. For filler words we highlighted the word itself, for missing transition words we highlighted the last wordand the first word of the adjacent sentences. For the remaining criteria we highlighted the whole sentence.Feedback was provided by trained tutors following a rigorous procedure. First, tutors read an essay inorder to get an impression of the intended statement and logical structure of the essay. Next, the essays were reread and commented on focusing on one writing error at a time. Tutors read each essay at least six times. Eacherror was commented with a standardized comment for each writing error.Measures and instrumentsControl measures and treatment checkWe controlled for uneven distribution in the conditions taking into account demographical information,experience with and interest in using computers. In order to see whether sense-making support was used asintended, we analyzed participants’ attendance to the sense-making support table.Revision skill (pre-posttest)Revision skills were assessed using counterbalanced pre- and posttest versions. The pre- and posttests assessedtwo distinct skills related to academic writing: problem detection and problem correction. Problem detectionwas assessed with an erroneous text and participants were asked to highlight and label errors in the text. Themaximum score was 20 points. For problem correction, participants had to correct errors in text sections. Errorswere related to the writing errors described earlier. The highest score that could be achieved was 22 points.Feedback uptakeFeedback uptake was assessed by measuring feedback-based changes in the text and correctness of changesafter feedback was received. Below, we refer to feedback-based changes, that is, only changes that were madebased on received feedback.Table 2: Description and reliability of feedback uptake variablesFeedback uptake variablesDescriptionAgreement %KappaSuccessful changeAny change that resultedimprovement of the text.an87,3%.66New error changeAny change that erroneously created a newmistake instead of correcting one.91,3%.47Incorrect comment changeAny change that was made based on anincorrect feedback comment.90,1%.82inAll dependent variables were measured at the individual level. For all analyses, coders were unawareof the treatment conditions. 25 % of the texts were coded by a second coder. Coders’ percentage of agreementwas between 87% and 91% (see Table 2).ResultsReported results can vary with respect to number of participants, because not all 73 participants finished allrelevant stages.Control measures and treatment checkWe controlled for uneven distribution in the conditions taking into account demographical information,experience with and interest in using computers. Participants in both conditions showed no substantialdifferences regarding prior experience with computers (F (1, 71) = .02, p = .88) and interest in computers (F (1,71) = .46, p = .50). In order to check whether sense-making support was used as intended, we analyzedparticipants’ attendance to the sense-making support table. We were interested, whether the participants (N=34)attended to each of the columns particularly columns three, five and six.Out of 10 feedback comments, participants noted to understand 85.9%, agree on 61.8% and use 70.3%feedback comments (see Table 3). For every comment we have also captured whether the particular commentwas used or not, therefore we were able to relate the comments that were used to the comments that werereported to be used. We found that participants actually used 93.6% of the comments they indicated to use. OutCSCL 2015 Proceedings415© ISLSof the 2 additional erroneous comments, participants noted to understand 82.5%, agree on 41% and use 47%feedback comments However, they only used 75.5% of the comments they indicated to use. Based on theseresults, it appears that participants have used sense-making support for the purpose of reflection.Table 3: Means and standard deviations for participants’ attendance to sense-making support tableFeedbackcomments (10)Additional erroneousfeedback comments (2)M(SD)%M(SD)%Column 3: “I understand the comment”8.59(1.84)85.91.65(.60)82.5Column 5: “I agree with the comment”6.18(8.57)61.8.82(.80)41.0Column 6: “I am going to use the comment”7.03(5.85)70.3.94(.78)47.0Actual usage of feedback comments for revision8.0(1.65)80.0.85(.70)42.5Effect of sense-making support on revision skillsThe low number of participants resulted from a combination of corrupted files and participants that did not takepart in the last phase of the online activity in which the posttest was conducted. An ANOVA showed nosignificant differences between both conditions regarding revision skills F (1, 48) = .1.81, p = .18. Descriptively,problem detection scores increased from pre to post in the SMS+ condition, while in the SMS- condition,problems detection scores decreased (see Table 4). Problem correction did not change from pre- to post test.Table 4. Means and standard deviations for revision skillSMS+ (N=20)PreSMS- (N=31)PostPrePostM(SD)M(SD)M(SD)M(SD)Problem detection (PD)(max. 20 points)7,76(4,023)8,80(4,029)9,54(3,144)8,84(3,284)Problem correction (PC)(max. 22 points)16,73(3,118)16,19(3,270)16,72(2,976)16,58(2,355)Revision skills (PC+PD)25,27(5,423)24,88(6,192)26,72(4,531)25,54(4,207)Effect of sense-making support on feedback uptakeA one-way multivariate analysis of variance (MANOVA) was conducted to determine the effects of sensemaking support on feedback uptake with the variables successful change (SC), new error change (NC) andincorrect comment change (IC). Results showed a significant effect, Wilks λ = .85, F (1, 71) = 3.99, p < .01, η2= .15. Separate ANOVAs for the feedback uptake variables were then conducted to the correspondingMANOVA. Results showed that the average score of successful change, F (1, 71) = 1.81, p < .18, η2 = .03 wasnot different between conditions. Yet, results showed that the feedback uptake variables, new error change, F(1,71) = 6.58, p < .01, η2 = .09 and erroneous comment change, F (1, 71) = 4.14, p < .05, η2 = .06 were higher forstudents in the SMS+ condition than for students in the SMS- condition (see Table 5).The changes above relate to feedback-based changes. We also analyzed the amount of changes thatwere unrelated to feedback. Apart from feedback-based changes, students engaged in very little revision.Therefore results on revision apart from feedback-based changes are not reported.CSCL 2015 Proceedings416© ISLSTable 5. Means and standard deviations for feedback uptakeSMS + (N=34)SMS- (N=39)Revision typesM(SD)Percentageof uptakeM(SD)Percentageof uptakeSuccessful change (SC)5.06(1.67)50.65.59(1.7)55.9New error change (NC)1.03(1.09)10.31.72(1.19)17.2Incorrect comment change (IC).85(.70)42.51.21(.77)60.5ConclusionsIn this study, we explored instructional support that aimed at helping the assessee in the context of peerfeedback. Particularly, we looked at the impact of sense-making support to increase revision skills and feedbackuptake during writing. Statistically, sense-making support did not affect revision skills in terms of problemdetection and problem correction (hypothesis 1). Results showed increased scores for problem detection in theSMS+ condition and an improvement from pre to posttest. Yet, one can only speculate whether this increasewas due to our variation or whether it was by chance because the differences were not significant. It is possiblethat because of the low sample size, we were unable to detect hypothesized effects. Future studies should aim atlarger number of participants per condition. In both conditions (SMS+ and SMS-), we can see that studentsengaged very little in problem detection. Yet, it seemed easier for student to do problem correction. One reasonfor low scores on problem detection can be explained by the peer-feedback that was given. Peer-feedback hasthe purpose of providing the writer with information on problems in the text thereby helping with problemdetection. Peer-feedback might inhibit students to do problem-detection themselves, because there is no need forassessees to detect problems. Future studies should look at how we can support students in problem detectionwhen integrating peer-feedback in instruction.Sense-making support appeared to affect feedback uptake only to some extent (hypothesis 2). Studentsin the SMS+ condition showed only on two out of three feedback uptake variables significant better scores.Students receiving sense-making support made less new errors. From the treatment check analysis, we saw thatover 90% of the feedback comments that students indicated to use, were actually used in the text to makesuccessful changes. We can conclude that students indeed used the sense-making support table as a way to do ameans-ends analysis (Hayes, 2004). Students in the SMS+ condition seemed to relate the errors that werepointed out in the feedback to the actions that need to be taken to improve the text. We can infer that workingwith sense-making support and using the provided sense-making support table to systematize how to use thefeedback might have helped to avoid making erroneous changes in the text. Furthermore, students used lessincorrect feedback comments for text changes. These results indicate that sense-making support helped studentsto reflect on given feedback and to think more deeply which feedback to use for text changes (Schraw, 1998).This picture seems to be confirmed by the treatment-check results. Though we could see that students did useerroneous comments, they did so much less frequently than using the correct feedback. Working with sensemaking support might have helped to become more aware of which comments were correct and which oneswere not. Students in the SMS+ condition made as many successful changes as students in the SMS- condition.One reason why sense-making support was not effective regarding successful changes might be the extra workload that students in the SMS+ condition needed to deal with. It is possible that spending time with sensemaking support might have taken away time that students in the SMS- condition could fully spend for makingfeedback-based changes in the text. Future studies should make sure not to overburden the assessee to leavesufficient time to make changes and to do revision.ReferencesAllal, L. & Chanquoy, L. (2004). Revision revisited introduction. In L. Allal, L. Chanquoy & P. Largy (Ed.)Revision and Cognitive Instructional Processes: Studies in Writing. (pp.1-7). Vol. 13. Norwell: KluwerAcademic Publishers.Berthold, K., Nückles, M., & Renkl, A. (2007). Do learning protocols support learning strategies and outcomes?The role of cognitive and metacognitive prompts. Learning and Instruction, 17, 564–577.Boero, R., & Novarese, M. (2012). Feedback and Learning. Encyclopedia of the Sciences of Learning, 12821285.CSCL 2015 Proceedings417© ISLSCho, K., & MacArthur, C. (2010). Student revision with peer and expert reviewing. Learning and Instruction,20(4), 328-338.Falchikov, N. (1986). Product comparisons and process benefits of collaborative peer group and selfassessment. Assessment and Evaluation in Higher Education, 11 (2), 146–166.Falchikov, N., & Goldfinch, J. (2000). Student peer assessment in higher education: A meta-analysis comparingpeer and teacher marks. Review of Educational Research, 70(3), 287-322.Fitzgerald, J. (1987). Research on revision in writing. Review of Educational Research, 57(4), 481–506.Flower, L., Hayes, J. R., Carey, L., Schriver, K., & Stratman, J. (1986). Detection, diagnosis, and the strategiesof revision. College Composition and Communication, 37(1), 16-55.Graham, S. & Harris, K. R. (2007). Best Practices in Teaching Planning. In Graham, S., MacArthur, C.A.,Fitzgerald, J.(Eds.) Best Practices in Writing Instruction. (pp. 119-140). New York: Guilford Press.Hammer, R., Ronen, M., & Kohen-Vacs, D. (2010). Stressed yet motivated: Web-based peer assessedcompetition as an instructional approach in higher education. In Gomez, K., Lyons, L., & Radinsky, J.(Eds.), Learning in the Disciplines: Proceedings of the 9th International Conference of the LearningSciences, Vol. 1, (pp.65-72), Chicago, IL.Hayes, J. R. (2004). What triggers revision? In L. Allal, L. Chanquoy, & P. Largy (Eds.), Revision: Cognitiveand instructional processes (pp. 9–20). Dordrecht, The Netherlands: Kluwer.Hayes, J. R., Flower, L., Schriver, K. A., Stratman, J., & Carey, L. (1987). Cognitive processes in revision. In S.Rosenberg (Ed.), Advances in applied psycholinguistics. Reading, writing, and language processing,Vol. II. Cambridge: Cambridge University Press.Kluger, A. N., & DeNisi, A. (1996). The effects of feedback interventions on performance: a historical review, ameta-analysis, and a preliminary feedback intervention theory. Psychological Bulletin, 119, 254-284.Kollar, I. & Fischer, F. (2010). Peer assessment as collaborative learning: A cognitive perspective. Learning andInstruction, 20(4), 344-348.Moodle Pty. Ltd. (2013). Moodle (Version 2.3.3) [Learning environment software]. Retrieved fromhttps://moodle.org/.Nelson, M.M. & Schunn, C.D. (2009). The nature of feedback: how different types of peer feedback affectwriting performance. Instructional Science, 37, 375-401.Newell, A. & Simon, H. (1972). Human Problem Solving. Englewood Cliffs, NJ: Prentice-HallProske, A., Narciss, S., & McNamara, D. (2010). Computer-based scaffolding to facilitate students developmentof expertise in academic writing. Journal of Research in Reading 33(1), 1-17.Roscoe, R. D. & Chi, M.T.H. (2007). Understanding tutor learning: knowledge-building and knowledge-tellingin peer tutors' explanations and questions. Review of Educational Research, 77,534-574.Scardamalia, M., Bereiter, C., & Steinbach, R. (1984). Teachability of reflective processes in writtencomposition. Cognitive Science, 8, 173–190.Schraw, G. (1998). Promoting general metacognitive awareness. Instructional Science, 26, 113–125.Topping, K. (1998). Peer assessment between students in colleges and universities. Review of EducationalResearch, 68(3), 249–276.Topping, K. J. (2003). Self and peer assessment in school and university: Reliability, validity and utility. In M.S. R. Segers, F. J. R. C. Dochy, & E. C. Cascallar (Eds.), Optimizing new modes of assessment: Insearch of qualities and standards (pp. 55–87). Dordrecht, Netherlands: Kluwer Academic Publishers.Van der Pol, J., van den Berg, B. A. M., Admiraal, W. F., & Simons, P. R. J. (2008). The nature, reception, anduse of online peer feedback in higher education. Computers and Education, 51, 1804-1817.Van Gennip, N, Segers, M. & Tillema, H. (2010). Peer assessment as a collaborative learning activity: The roleof interpersonal variables and conceptions. Learning & Instruction, 20(4), 280-290.Zariski, A. (1996). Student peer assessment in tertiary education: Promise, perils and practice. Proceedings ofthe 5th Annual Teaching Learning Forum, (Perth: Murdoch). In Abbott, J. and Willcoxson, L. (Eds),Teaching and learning within and across disciplines (pp.189-200). Perth: Murdoch.Zimmerman, B.J. (1989). A social cognitive view of self-regulated academic learning. Journal of EducationalPsychology, 81, 329-339.AcknowledgmentsThe research that is discussed in this paper is supported by the German-Israeli Foundation for ScientificResearch and Development (1090-25.4/2010). Special thanks go to our project partners Miky Ronen, MosheLeiba, Dan Kohen-Vacs and Ronen Hammer from Holon Institute of Technology, Israel.CSCL 2015 Proceedings418© ISLS