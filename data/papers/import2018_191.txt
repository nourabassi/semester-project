Characterizing Computational Thinking in High School ScienceHillary Swanson, Golnaz Arastoopour Irgens, Connor Bain, Kevin Hall, Philip Woods, Carson Rogge, andMichael Horn, Uri Wilenskyhillary.swanson@northwestern.edu, golnaz.irgens@northwestern.edu, connorbain2015@u.northwestern.edu,kevin.hall1@northwestern.edu, philipwoods93@gmail.com, carsonrogge2019@u.northwestern.edu, michaelhorn@northwestern.edu, uri@northwestern.eduNorthwestern UniversityAbstract: This study identifies high school students’ computational thinking practices in thecontext of science, technology, engineering, and math (CT-STEM practices) and therelationships between their practices-in-use. More specifically, we explore the CT-STEMpractices that emerged as a result of students’ participation in a two-day biology lesson featuringthe exploration of a computational model on predator-prey dynamics. Digitally recorded datawere taken from seventy-six students across four classes of one teacher. By applying a groundedanalysis to students’ written responses to two different assessment items embedded within thelesson, we found four CT-STEM practices related to identifying a model’s limitations and eightpractices related to exploring the model. Applying a network analysis to responses coded forthese practices, we found networks representing common patterns of practices-in-use. Thiswork identifies the informal CT-STEM practices that students bring to their learning and modelscombinations of practices-in-use.IntroductionIn recent decades, computational thinking (CT) has become critical in a variety of mathematical and scientificfields (Foster, 2006). In turn, STEM education communities have recognized the importance of integratingcomputation into school curricula (Quinn, Schweingruber, & Keller, 2012; Wilensky, Brady & Horn, 2014).However, computation still remains a separate area of study in most K-12 contexts. Because of the separation,students from groups that have been historically underrepresented in computational fields, such as women andracial minorities, are less likely to be exposed to authentic CT practices (Margolis, 2008; Margolis & Fisher,2003). Integrating CT into the context of science not only gives all students access to a more authentic image ofscience, it also increases access to powerful modes of thinking and marketable skills for many careers (Levy &Murnane, 2004). For these reasons, we believe that integrating CT practices into K-12 STEM curricula is criticalfor 21st century education. Our group has worked to create curriculum and assessments to promote CT practicesin the context of STEM content (CT-STEM practices). In the present report, we document and characterize theCT-STEM practices in which students engaged as a result of their participation in one of our computationallyenriched science lessons. We then explore particular combinations of practices the students used synergisticallyin order to address tasks within the lesson.Theoretical frameworkOur perspective on CT is motivated by Wilensky and Papert’s (2010) Restructuration Theory, which demonstratesthat the representational form in which knowledge is embodied significantly influences how it may be understood.Restructuration Theory builds on a long history of psychological and historical research that has argued thatrepresentational forms shape human knowledge and understanding, both at the individual and societal level (e.g.,Goody, 1977; Olson, 1994; diSessa, 2001). In light of this theory, we argue that the representational affordancesof computational tools are changing the way knowledge can be constructed, expressed, and understood acrossdisciplines. However, the field has not yet understood how to measure the affordances of computationalrepresentations for learning or how to characterize practices when engaging with such representations.Our group addressed one aspect of this challenge and characterized the nature of computational thinkingpractices in the STEM disciplines. On the basis of interviews with computational STEM researchers, wedeveloped an operational definition of computational thinking as a set of practices organized in four major strands:Data Practices, Modeling and Simulation Practices, Computational Problem-Solving Practices, and SystemsThinking Practices (Weintrop, et al., 2015). We have used this taxonomy to inform learning objectives,curriculum, and assessments to foster and evaluate students’ development of computational thinking practices inSTEM subjects at the high school level.While this taxonomy has informed our design, it is also important to consider the prior knowledgestudents bring to their learning. diSessa (1993) has argued that students develop more expert knowledge throughICLS 2018 Proceedings871© ISLSthe reorganization and refinement of prior knowledge. He further argued that knowledge is a complex system ofsmaller elements. In the novice knowledge system, elements are loosely interconnected and cued variously forsense-making depending on context. In the expert knowledge system, elements are more reliably connected andcued more consistently in contexts where they are productive. Learning (and the transition from novice to expert)occurs through the reorganization and refinement of the networks of elements in the knowledge system. Thenovice knowledge system is therefore viewed as a resource rich with potentially productive building blocks forthe construction of more expert knowledge networks. Research within the KiP program has documented elementsof naïve knowledge in a variety of forms including (but not limited to) intuitions for why things work the waythey do (diSessa, 1993), naïve epistemologies (Hammer & Elby, 2002), and competencies for representationalpractices (diSessa & Sherin, 2000).Such networks of novice and expert knowledge systems can be visualized and analyzed through networkanalysis tools. In general, network analyses trace the flow of information, uncover prominent patterns in networks,and detect the effects of such patterns. In social network analysis, for example, researchers examine patternsamong people’s interactions, where the nodes of the network represent people and links among the nodes representhow strongly certain people are connected (Freeman, 2006). To measure connections among CT-STEM practices,however, the nodes do not represent people, but rather represent the knowledge and skills of one individual. Thesenodes are elements identified in discourse (e.g., written documents, conversations, or actions), and the linksrepresent the individual’s associations between nodes. These links are analytically determined when elements cooccur in the discourse. Researchers have shown that co-occurrences of concepts in a given segment of discoursedata are good indicators of cognitive connections (Arastoopour, 2016; Lund & Burgess, 1996). In effect, discoursenetworks allow us to analyze the connections among CT-STEM practices.One tool for developing such discourse networks is Epistemic Network Analysis (ENA) (Shaffer et al.,2009; Shaffer, Collier, & Ruis, 2016; Shaffer & Ruis, 2017). ENA measures when and how often students makelinks between domain-relevant elements during their work. It accomplishes this by measuring the co-occurrencesof discourse elements and representing them in weighted network models–meaning when someone repeatedlymakes a link between elements, the weight of the link between those elements is greater. Furthermore, ENAenables researchers to compare networks both visually and through summary statistics that reflect the weightedstructure of connections (Collier, Ruis, & Shaffer, 2016). Thus, researchers can use ENA to not only modeldiscourse networks, but also quantitatively compare the discourse networks of various individuals and groups ofpeople.In this study, we take a KiP lens to students’ activity in the context of our curriculum and chart the spaceof the informal CT-STEM practices in which they engage. We then use ENA to model the relationships betweenstudents’ practices in the context of instructional tasks. The specific research questions we address are: (1) Whatis the character of students’ CT-STEM practices that emerge in the context of one biology lesson? and (2) Howare the practices connected when students use them to accomplish a particular task within the lesson?MethodsWe approached our research questions by analyzing data from the fifth iteration of a design-based research cycle(Collins, Joseph, Bielaczyc, 2004). The implementation spanned the 2016-2017 school year and was tested ineight classrooms across three partner high schools in a large Midwestern city. Over the course of the school year,students ranging from grades 9 – 12, participated in three CT science lessons, each lesson approximately two daysin length. In order to understand the character and connectedness of CT-STEM practices that students enacted inour curricular lessons, we conducted a fine-grained analysis of a smaller sample of student work produced in thecontext of a single high school biology lesson which focused on predator-prey dynamics and ecosystem stability.For this preliminary analysis, we chose to investigate the work of the students of one participating biology teacher.Lesson description“Ecosystem Stability” is a 2-hour biology lesson designed to engage students in CT-STEM practices within theModeling and Simulation strand of our taxonomy. This lesson builds on ecology lessons designed for high schoolbiology classes (Wilensky & Reisman, 2006; Wagh, Cook-Whitt, & Wilensky, 2017). For this lesson, studentsexplored population dynamics in a NetLogo (Wilensky, 1999) simulation of an ecosystem consisting of threeorganisms (grass, sheep, and wolves) (Wilensky, 1997). Students investigated the population-level effects ofparameters for individual organisms (such as initial population and reproduction rate) by running the simulationwith different values for each organism. Through their exploration, the students learned about the complexpopulation dynamics that emerge from the interactions between individual organisms. In this way, students bothlearned about factors affecting the balance of an ecosystem and developed practices related to using and assessingICLS 2018 Proceedings872© ISLScomputational models (e.g., exploring a model by changing parameters; identifying simplifications made by amodel).Participants and data collectionThe lesson was implemented during the fall of 2016 in the five regular biology classes of Ms. Buckthorn, a 9thgrade biology teacher at Greenboro High School. Seventy-six 9th grade students participated in the lesson. Ms.Buckthorn’s students were representative of the students at Greenboro (44% White, 29.4% Black, 18% Hispanic,5.6% Asian, 2.4% Native American, Pacific Islander, or Bi-Racial; 40.5% low-income; 4.2% English Learners;12% IEP students).Data were collected in the form of student responses to assessment items embedded in the lesson. Studentresponses to two particular prompts were coded for this analysis. The first prompt was “Describe 3 limitations ofusing a model like this to make predictions about what could happen in the real world.” This prompt was designedto engage students in CT-STEM practices related to identifying the simplifications made by a model, a practicewithin the Modeling and Simulation strand of the taxonomy. These practices are important to students’epistemological development, as they relate to their understanding of a computational model as a tool that is bothpowerful and limited with regards to the construction of new knowledge. The second prompt followed a challengethat asked students to adjust parameters to stabilize the system (i.e., keep both the wolf and sheep populationsfrom going extinct). This prompt had two parts: “Which specific variable(s) did you change and how did youchange them?” and “Explain why you made these changes. How do you think these changes helped to stabilizethe ecosystem?” It was designed to engage students in CT-STEM practices related to exploring a model bychanging parameters, a practice also organized within the Modeling and Simulation strand of the taxonomy.We addressed our first research question with a qualitative approach to characterize the nature ofstudents’ CT-STEM practices. We addressed our second research question by using a quantitative approach toexplore the relationships between these practices. We began by conducting a grounded analysis of students’written responses to identify and characterize their CT-STEM practices within the Modeling and Simulation strandof the taxonomy under the general practice identifying model limitations or exploring a model by changingparameters (1). We used these practices as the basis of coding schemes which we applied to student responses tothe two questions from the lesson. Two researchers coded a subset of ten student responses from the data as atraining set and calculated their inter-rater reliability using Cohen’s Kappa. If the researchers had a kappa higherthan .60, they split the dataset and coded the remainder of the responses. Cohen’s Kappa statistics are reported inthe findings for each code below.In order to quantify the connections between practices-in-use, we used Epistemic Network Analysis(ENA). In this context, ENA measures when and how often students use CT-STEM practices together whenaddressing a particular question. The network representation allows for an examination of two aspects of studentwork: (1) the density of the networks, which shows how many practices a student is connecting and (2) thethickness of the links, which tells us which practices students are connecting more frequently. Additionally, ENAallows for the comparison of multiple student networks because it fixes each practice in the same Cartesian spacefor all students. Although ENA also provides other features such as an interpretable multi-dimensional projectionspace and offers a variety of confirmatory statistical analyses, in this study, we used only the basic weightednetworks to conduct an exploratory analysis of the various student patterns of CT-STEM practices-in-use.FindingsOur findings focus on students’ CT-STEM practices for identifying model limitations and exploring a model bychanging parameters. We present our findings for research question 1 by characterizing student CT-STEMpractices. We then present our findings for research question 2 by exploring the connections between practicesin-use.Research question 1: Characterizing student CT-STEM practicesThrough a grounded analysis, our team identified four CT-STEM practices relevant to identifying the limitationsof a model in students’ responses to prompt 1: “Describe 3 limitations of using a model like this to makepredictions about what could happen in the real world.” We identified eight CT-STEM practices relevant toexploring a model by changing parameters in students’ responses to prompt 2: “Which specific variable(s) didyou change and how did you change them? Explain why you made these changes. How do you think these changeshelped to stabilize the ecosystem?” We present these CT-STEM practices and characterize and illustrate each withexamples from the data.Identifying model limitationsICLS 2018 Proceedings873© ISLSIdentifying general limitations. Thirty-six students (47%, Cohen’s Kappa = .61) addressed prompt 1 by notinggeneral inaccuracies or missing factors as limitations of the model. For example, one student wrote: “This modelmay not be accurate, and it does not factor in outside variables.” This suggests these students are aware that thewolf-sheep model is an approximation of reality, but they have not engaged in careful thinking to identifyparticular inaccuracies or missing factors.Identifying visual representational limitations. Nine students (11%, Cohen’s Kappa = 1) noted visualinaccuracies as limitations of the model. One student wrote: “It isn't 3-D.” This suggests that these studentsunderstand that the model is not an accurate depiction of reality. The model used presented a 2-D projection ofthe environment which is certainly an approximation of the true reality. However, this is not a “meaningful”limitation compared to other limitations that students mentioned, as in this case, the approximation does notinfluence the interactions between the elements of the model and therefore does not influence the outcome of anygiven simulation trial. In other words, wolf and sheep are confined to movement about the Cartesian plane andthe addition of a third dimension would not influence any possible outcomes of this model.Identifying completeness limitations. Forty-five students (60%, Cohen’s Kappa = .65) offered specificelements or factors that were missing from the model. One student listed three missing or incomplete aspects ofthe model: “1. You only have two animals, 2. You don't have an entire country, 3. You only have one thing asheep can eat.” These students recognize that the wolf-sheep model is an approximation of reality. They havecompared it with the real world and identified factors that are found in the real world but missing from the model.It is probable they believe these factors are somehow important to the model and would change the outcome of asimulation trial. Limitations such as these are important for scientists to identify, because they help them interprettheir results and recognize their limitations.Identifying procedural limitations. Ten students (13%, Cohen’s Kappa = .75) noted differencesbetween the interactions or behaviors encoded in the model and those they expected to find in the real world. Onestudent wrote: “The moving of the animals is random, they run out of energy which isn’t very similar to the realworld, the real world is unpredictable.” Limitations such as this are extremely important for scientists to recognize,as they are related to how successful the model is at approximating reality. Procedural limitations of the modelinfluence the outcome of a simulation run in an important way: if the simulation does not reproduce patterns foundin real-world data, something about the encoded theoretical model is wrong and needs to be revised.Exploring a model by changing parametersVarying a parameter. Sixty-eight students (92%, Cohen’s Kappa = 1) noted the specific parameters theychanged. One student wrote: “We changed every single variable until we found the closest one until the sheepkept spiking so we changed the reproduction rate and they became more balanced.” It is not surprising that somany students engaged in this practice, as they were directly prompted by the lesson to do so. Tinkering with amodel by varying parameters is, however, an activity fundamental to exploring a model.Testing a parameter. Forty-six (62%, Cohen’s Kappa = 1) students noted the range of values (or specificvalues) they tried for different parameters. One student wrote: “We changed both sheep and wolf reproduction,sheep reproduction from 4% to 3%, and wolf reproduction from 4% to 9%. We changed the initial wolf populationfrom 50 to 55. We changed the wolf gain from food from 20 to 25.” This is evidence that they tested specificparameter values. This is a more particular instantiation of varying a parameter that the student executes withperhaps greater intentionality (e.g., they might intend to investigate the relationship between a parameter andsystem behavior by comparing extremes). This is a more systematic approach to exploring a model than a tinkeringapproach.Describing effects qualitatively. Forty-nine students (66%, Cohen’s Kappa = 1) qualitatively describedhow the system responded when they changed particular parameters. One student wrote: “These changes, such asraising the reproduction rate of wolves grew the wolf population and by result lowered the sheep population.” Itis important to attend to outcomes of the simulation when tinkering with or testing parameters, in order to noticerelationships between cause and effect. Simple qualitative characterizations of the relationships within a systemare a foundation for constructing more detailed or mathematical relationships. A simple qualitative almost gestaltunderstanding of a cause-effect relationship can be a powerful tool for reasoning about system dynamics and forconveying the big ideas about the relationships within a system to others (in the scientific world these “others”might be collaborators or members of the scientific community at-large).Describing effects quantitatively. Six students (8%, Cohen’s Kappa = 1) included quantitativeinformation from the simulation when describing how the system responded to their changes to parameters. Onestudent wrote: “I lowered the reproduction rates of both wolves and sheep to 1%. I started with 90 sheep and 50wolves. The sheep had 2 for their gain from food and the wolves had 40.” This suggests that these students wereattending to particular evidence in the data and trying to describe the relationships they saw in a more precise andICLS 2018 Proceedings874© ISLSmathematical way. Note that while this practice is similar to “testing a parameter,” it requires students to attendto model outcomes, not just input parameters.Describing the evolution of a system over time. Ten students (14%, Cohen’s Kappa = 1) describedhow the system progressed over time. One student wrote: “I actually didn't change anything and just clicked go,after watching the graph and the animation for 884 ticks it seemed to be stable, grass goes up sheep begin to goup, sheep go up wolves go up. grass goes down sheep go down and wolves will go down too.” This is an importantpart of exploring a simulation: letting it run and observing how it changes over time. Complex systems such asthe one represented by this ecosystem model, are dynamic systems–they exhibit patterns of change over time.Important changes can only be observed if simulations are run over a long enough period. Describing behavior asit changes over time can lead to recognizing important patterns.Explaining reasoning. Forty-nine students (66%, Cohen’s Kappa = .60) provided explanations for whychanging a particular parameter resulted in a system outcome. One student wrote: “I made these changes becausethe sheep population was growing too large. This caused the wolf to eat more, then reproduce more. Theneventually the sheep would die off, causing the wolfs to die off.” Explanations such as this convey the students’reasoning and suggest that they are not only attending to cause and effect, but that they are going one step furtherand trying to make sense of the relationship between cause and effect – a fundamental activity of science.Strategizing. Thirty-nine students (53%, Cohen’s Kappa = .74) wrote responses that showed evidenceof goal-directed or planned behavior. One student wrote: “I changed the reproduction rate because the wolvesstarted to spike so I figured the wolf reproduction was too high.” This suggests the student was drawing on ahypothesis about the relationship between reproduction rate and population size to make decisions about changingparameters, strategically.Comparing across multiple trials. Fourteen students (19%, Cohen’s Kappa = .62) gave responses thatwere evidence they ran the simulation over multiple trials and compared results across those. One student wrote:“I changed the reproduction rate for each organism and changed the initial amount of each. It was difficult to getthe things exactly right but I got close my closest was 228 ticks.” When exploring a model to learn more aboutthe dynamics of, or test a hypothesis regarding, a complex system, it is important to observe more than onesimulation run. This is because complex systems are inherently random and the results of changing a parametervary over different simulation trials. A pattern of cause-effect relationships will hover around an average tendency,but this average tendency may not be exactly embodied in one (or several) simulation trials. So, if a student onlyruns one trial, they may have a misguided impression of a pattern in system behavior. It is also a good idea to runmultiple trials in order to systematically compare the effects of different parameter values on system behavior.Research question 2: Exploring connections between CT-STEM practicesUsing ENA, our team identified the most frequent individual student networks of CT-STEM practices usingstudents’ responses to prompt 1 and prompt 2. To answer research question 2, we characterize and exemplify eachnetwork of students’ CT-STEM practices with data.Identifying model limitationsThe first question asked students: “Describe 3 limitations of using a model like this to make predictions aboutwhat could happen in the real world.” Thirty-nine students (53%) had discourse networks which consisted of zeroconnections (not pictured). Eleven students (15%) had discourse networks which consisted of one link betweenGeneral Issues and Completeness (Figure 1). We interpreted a link between General Issues and Completeness asa student claiming the model was incomplete and then listing general issues related to a lack of completeness. Forexample, one student responded, “In the real world there are more variables like day to day weather, otherpredators, hunters and so many other things that could affect their habitat.” This student claimed the model wasincomplete (“In the real world there are more variables like day to day weather...”) and concluded with a generalstatement (“...and so many other things that could affect their habitat.”).Figure 1. The second most frequently occurring discourse network for prompt 1 which consists of oneconnection between General Issues (Identifying general limitations) and Completeness (Identifyingcompleteness limitations).ICLS 2018 Proceedings875© ISLSOf the remaining students, two had a network that consisted of links among General Issues, Completeness, andProcedural Limitations (Figure 2). For example, one student provided a variety of limitations of the model:“Nature isn’t a perfect system so it won’t be completely accurate. There is more than one type of predator andmore than one type of prey. Weather isn’t taken into account in this model.” This student identified procedurallimitations (“Nature isn’t a perfect system”), provided a general statement related to such procedural limitations(“so it won’t be completely accurate”), and then claimed that the model was incomplete (“There is more than onetype of predator and more than one type of prey. Weather isn’t taken into account”).Figure 2. The discourse network with the most connections for prompt 1 which consists of connections amongGeneral Issues (Identifying general limitations), Completeness (Identifying completeness limitations), andProcedural Limitations (Identifying procedural limitations).Exploring a model by changing parametersThis question had two parts and asked students: “Which specific variable(s) did you change and how did youchange them?” and “Explain why you made these changes. How do you think these changes helped to stabilizethe ecosystem?” Ten students (13%) had discourse networks which consisted of zero connections (not pictured).Eight students (11%) had discourse networks which consisted of one link between Varying Parameters andTesting Parameters (Figure 3). For these eight students, this link occurred in the first part of the question; thesestudents did not make any links in the second question. We interpreted a link between Varying Parameters andTesting Parameters as being able to vary parameters and then provide specific values for testing. For example,one student responded, “the variables that we changed was the grass regrowth time to 50” which coded for bothVarying Parameters and Testing Parameters. For the second part of the question, the same student responded,“Because the grass is like the most important part to keep the system alive,” which did not contain any cooccurrence of codes and thus, did not appear in the network representation.Figure 3. The most frequently occurring discourse network for Q2 which consists of one connection betweenVarying Parameters (Varying a parameter) and Testing Parameters (Testing a parameter).Out of the remaining students, one had the most connected network which consisted of links amongComparison, Reasoning, Describing Effects Qualitatively, Strategy, Varying Parameters, and Testing Parameters(Figure 4). This student linked between Varying Parameters and Testing Parameters in the first part of thequestion. In the second part of the question, the student again connected Varying Parameters and TestingParameters (hence, the link is thicker in the network representation), but also added Comparison, Reasoning,Describing Effects Qualitatively, and Strategy. The student’s response to the first part was, “I changed the initialnumber of sheep to 150 and the initial number of wolves to 75,” and his response to the second part was “I madethese changes from trial and error. I switched them so that the wolves and sheep wouldn't die out so quickly, sogiving them a greater initial population helped, while the reproduction percentage kept the populations balanced.”The student explained the use of a “trial and error” strategy “so that the wolves and sheep wouldn’t die out soquickly,” which was a qualitative description of the effects and a comparison of the predator and prey populations.The student’s reasoning for those actions were that “a greater initial population helped, while the reproductionpercentage kept the populations balanced.”ICLS 2018 Proceedings876© ISLSFigure 4. The network with the most connections for prompt 2 which consists of connections amongComparison (Comparing across multiple trials), Reasoning (Explaining reasoning), Describing EffectsQualitatively, Strategy (Strategizing), Varying Parameters (Varying a parameter), and Testing Parameters(Testing a parameter).DiscussionIn this study, we examined student responses to two prompts given in one CT science lesson which introducedstudents to predator-prey dynamics through exploration of computational models. Through a grounded analysis,we identified emergent student CT-STEM practices and classified them within the Modeling and Simulationstrand of our theoretical taxonomy. We then analyzed patterns of co-occurrences of these practices-in-use andrepresented these co-occurrences as discourse networks using ENA. Such networks allowed us to shed light oncharacterizing student CT-STEM practices in terms of the relationships among students’ CT-STEM practices-inuse and varying levels of expertise regarding their synergistic use. Our results showed that students brought avariety of informal CT-STEM practices to accomplishing tasks within a computational biology lesson and thatthey drew on multiple practices while working on particular tasks within the predator-prey lesson.We found that students engaged in practices relevant to identifying model limitations, includingidentifying general, representational, completeness and procedural limitations. We found that the majority ofstudents noted general model limitations and missing elements, while very few students noted inaccuraciesregarding visual representations or inconsistencies between the behavior of model elements and that of their realworld counterparts. We found that students engaged in practices relevant to exploring a computational model.These practices included varying parameters by tinkering, testing particular parameters, describing the effects ofchanging parameters in both qualitative and quantitative terms, describing the evolution of a model over time,explaining their reasoning for a system’s behavior in response to changing a parameter, approaching theirexploration of a model strategically, and comparing simulation results across multiple trials. All students variedparameters and most tested specific parameters. The majority of students qualitatively described the relationshipbetween changing a parameter and its effect on the system behavior and explained their reasoning for why thecause-effect relationship made sense. A majority of students also showed evidence of approaching theirexploration of the model strategically. Taken together, these findings suggest that participating students broughtmany informal practices to their learning that can be developed into more sophisticated CT-STEM practices(Smith, diSessa & Roschelle, 1994).Our network analysis revealed a range of complexity in students’ patterns of practices-in-use, fromstudents who drew on very few practices to students who drew on numerous practices to respond to a particularsense-making task. The supporting qualitative analyses of these networks indicated that students with more highlyconnected networks (i.e., students who drew on more practices to accomplish a task) provided more sophisticated,detailed responses to the assessment prompts. This suggests that as students develop more complex andmeaningful patterns of practices-in-use, they may develop more highly and meaningfully connectedrepresentations of understanding (diSessa, 1993). We argue that the networks we presented represent patterns ofpractices-in-use at varying levels of expertise and could be used to model and understand developmentaltrajectories of student CT-STEM practices.EndnotesICLS 2018 Proceedings877© ISLS(1) Although students’ written work may not give us as complete a picture of their engagement in CT-STEM practices as, forexample, video footage, student responses provide evidence of CT-STEM practices, as they often include descriptions of howthe student engaged with computational tools.ReferencesArastoopour, G., Shaffer, D.W., Swiecki, Z., Ruis, A.R., & Chesler, N.C. (2016). Teaching and AssessingEngineering Design Thinking with Virtual Internships and Epistemic Network Analysis. InternationalJournal of Engineering Education, 32(3), 1-10.Collier, W., Ruis, A. R., & Shaffer, D. W. (2016). Local versus global connection making in discourse. InInternational Conference of the Learning Sciences. Singapore.Collins, A., Joseph, D., & Bielaczyc, K. (2004). Design research: Theoretical and methodological issues. TheJournal of the learning sciences, 13(1), 15-42.diSessa, A. A. (1993). Toward an epistemology of physics. Cognition and instruction, 10(2-3), 105-225.diSessa, A. A. (2001). Changing minds: Computers, learning, and literacy. MIT Press.disessa, A. A., & Sherin, B. L. (2000). Meta-representation: An introduction. The Journal of MathematicalBehavior, 19(4), 385-398.Foster, I. (2006) 2020 computing: a two-way street to science’s future. Nature 440(7083):419Goody, J. (1977). The domestication of the savage mind. New York: Cambridge University Press.Hammer, D., & Elby, A. (2002). On the form of a personal epistemology. Personal epistemology: The psychologyof beliefs about knowledge and knowing, 169190.Levy, F. & Murnane, R. (2004). The new division of labor: How computers are creating the new job market.Princeton, NJ: Princeton University Press.Lund, K. & Burgess, C. (1996). Producing high-dimensional semantic spaces from lexical co-occurrence.Behavior , Research Methods, Instruments, & Computers, 28(2), 203-208.Margolis J (2008) Stuck in the shallow end: education, race, and computing. The MIT Press, CambridgeMargolis J, Fisher A (2003) Unlocking the clubhouse: women in computing. The MIT Press, CambridgeOlson, D. R. (1994). The world on paper. New York: Cambridge University Press.Quinn, H., Schweingruber, H., & Keller, T. others. 2012. A framework for K-12 science education: Practices,crosscutting concepts, and core ideas.Shaffer, D. W., & Ruis, A. R. (2017). Epistemic network analysis: A worked example of theory-based learninganalytics. In Handbook of Learning Analytics and Educational Data Mining (in press).Shaffer, D. W., Hatfield, D., Svarovsky, G., Nash, P., Nulty, A., Bagley, E. A., Mislevy, R. J. (2009). EpistemicNetwork Analysis: A prototype for 21st century assessment of learning. The International Journal ofLearning and Media, 1(1), 1–21Smith III, J. P., disessa, A. A., & Roschelle, J. (1994). Misconceptions reconceived: A constructivist analysis ofknowledge in transition. The journal of the learning sciences, 3(2), 115-163.Wagh, A., Cook-Whitt, K. & Wilensky, U. (2017) Bridging Inquiry-based Science & Constructionism:Exploringthe Alignment Between Students Tinkering with Code of Computational Models & Goals ofInquiry. Journal of Research in Science Teaching.Weintrop, D., Beheshti, E., Horn, M., Orton, K., Jona, K., Trouille, L., & Wilensky, U. (2016). Definingcomputational thinking for mathematics and science classrooms. Journal of Science Education andTechnology, 25(1), 127-147.Wilensky, U. (1999). NetLogo. Evanston, IL. Center for Connected Learning and Computer-Based Modeling,Northwestern University. http://ccl.northwestern.edu/netlogo/.Wilensky, U. (1997). NetLogo Wolf Sheep Predation model. Center for Connected Learning and Computer-BasedModeling,NorthwesternUniversity,Evanston,IL.http://ccl.northwestern.edu/netlogo/models/WolfSheepPredation.Wilensky, U., Brady, C. E., & Horn, M. S. (2014). Fostering computational literacy in scienceclassrooms. Communications of the ACM, 57(8), 24-28.Wilensky, U., & Papert, S. (2010). Restructurations: Reformulations of Knowledge Disciplines through newrepresentational forms. In J. Clayson & I. Kalas (Eds.), Proceedings of the Constructionism 2010Conference. Paris, France, Aug 10-14. p. 97.AcknowledgementsThis work was supported by the Spencer Foundation and the National Science Foundation (CNS-1138461, CNS1441041, DRL-1640201).ICLS 2018 Proceedings878© ISLS