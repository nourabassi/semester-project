Representations of Progress in a Learning CommunityCurriculum for Grade 12 BiologyAlisa Acosta, University of Toronto, alisa.acosta@utoronto.caJames D. Slotta, Boston College, slotta@bc.eduAbstract: This paper reports on the design and implementation of several student- and teacherfacing learning analytics representations within a blended learning community curriculum forGrade 12 Biology. Using a custom designed technology environment called CKBiology, theserepresentations captured the real-time progress of the learning community at three levels ofgranularity: Individual students, small groups, and whole class. Our results focus on theperspectives of students and teachers, triangulating data from a student questionnaire, teacherinterview, and CKBiology log files to identify how different representations of progresscontributed to awareness, motivation, and the overall practices of the learning community.Grounded in the theoretical model of Knowledge Community and Inquiry, this work seeks tostrengthen connections between learning analytics research and the learning sciences.IntroductionLearning communities are characterized by a culture of learning wherein all participants are involved in acollective effort of understanding (Bielaczyc & Collins, 1999). As Kling and Courtright (2003) observe,“developing a group into a community is a major accomplishment that requires special processes and practices,and the experience is often both frustrating and satisfying for the participants” (p. 221). One prominent challengein adopting learning community approaches is that of assessment (van Aalst & Chan, 2007). In contrast totraditional forms of instruction, wherein the teacher has sole authority over the assessment of students’ work,learning communities provide students with a greater level of agency, allowing them to “develop ways to assesstheir own progress and work with others to assess the community’s progress” (Bielaczyc & Collins, 1999, p.272). Thus, in a learning community curriculum the activity designs must clearly articulate the learning processes,making them visible and accessible for assessment. Furthermore, because learning communities focus on bothindividual and collective aspects of knowledge production, assessment in these contexts must serve the dualfunction of both measuring and scaffolding learning, producing a “feedforward effect” that serves to catalyze thedevelopment of new knowledge (Scardamalia & Bereiter, 2006; van Aalst & Chan, 2007).This paper reports on the design and implementation of several student- and teacher-facing learninganalytic representations within a learning community curriculum for Grade 12 Biology. Using a custom designedtechnology environment called CKBiology, these representations captured real-time progress of the communityat three levels of granularity: Individual students, small groups, and whole class. Grounded in the theoreticalmodel of Knowledge Community and Inquiry (KCI; Slotta, 2014), this work responds to two of the challengeareas identified by Ferguson (2012) concerning learning analytics: 1) Building strong connections to the learningsciences, and 2) focusing on the perspectives of learners. In this study, we triangulate data from a studentquestionnaire, teacher interview, and CKBiology log files to respond to the following research questions:1. What forms of representation allow students and the teacher to perceive progress (or gaps in progress)within a learning community?2. To what extent do these representations motivate students to contribute to the learning community?3. How are these representations used by the teacher in orchestrating the learning community?The goals of this research are closely aligned with those identified by Buckingham Shum and Crick (2016)concerning learning analytics for formative assessment of 21st century competencies: “to forge new links fromthe body of educational/learning sciences research—which typically clarifies the nature of the phenomena underquestion using representations and language for researchers—to documenting how data, algorithms, code, anduser interfaces come together through coherent design in order to automate such analyses, providing actionableinsight for the educators, students, and other stakeholders who constitute the learning system” (p. 8).Literature reviewKnowledge Community and Inquiry (KCI)ICLS 2018 Proceedings432© ISLSFor many years, theories on collaborative learning tended to focus on how participating in a group would affectan individual’s performance (Stahl, 2015). However, in the late 1980s two programs of research emerged thatsituated groups of learners within a broader community level: Fostering Communities of Learners (FCL; Brown& Campione, 1994) and Knowledge Building (KB; Scardamalia & Bereiter, 2006). FCL and KB differ withrespect to the objectives of the community, the centrality of student-generated ideas, and the level of emphasisplaced on prescribed learning goals. However, both of these research programs advanced the notion that theactivities occurring in school classrooms should mirror those of authentic research communities, incorporatingaspects of collective epistemology and community-level knowledge advancement (Brown, 1994; Scardamalia &Bereiter, 2006). Building upon this body of research, author Jim Slotta developed a pedagogical model calledKnowledge Community and Inquiry (KCI) as a means of integrating the perspectives of KB and FCL and makinglearning community approaches more accessible to researchers and practitioners. As in FCL and KB, students ina KCI classroom work together as a community, building upon each other’s knowledge and nurturing a collectiveepistemology. However, in a departure from KB, an important aspect of KCI is the design of curricular scripts(Fischer, Kollar, Stegmann, & Wecker, 2013) which specify the activity sequences, materials, student groupings,and technology elements that serve to guide the inquiry toward particular learning goals. KCI curriculum designsare guided by five major design principles, each accompanied by a set of epistemological commitments,pedagogical affordances, and technology elements (Slotta, 2014).Student- and teacher-facing learning analyticsLearning analytics (LA) entails the application of data science techniques, such as probability modeling and datavisualization, to educational data in order to generate actionable knowledge to support teaching and learning(Duval, 2011; Siemens, 2012). Because of its origins in online courseware environments, which typicallyembraced knowledge-transmission modes of pedagogy, a large proportion of LA research maintains a focus onassessment at the level of individual learners, emphasizing individual achievement and accountability (Chen &Zhang, 2016; Schwartz & Arena, 2013). A systematic literature review performed by Schwendimann et al. (2016)revealed that the primary audience for most LA dashboards was course instructors (71%) and that the predominantcontext was university settings. Furthermore, only 5% of papers reviewed included an explicit theoretical basisfor its LA designs (Schwendimann et al., 2016). In a subsequent literature review focusing on student-facing LAdashboards, Jivet et al (2017) revealed that only 26 out of 95 dashboards had a) been empirically evaluated, andb) had any theoretical grounding in the learning sciences. Of those that did, 18 of 26 were rooted in cognitivisttheory and promoted competitive, rather than collaborative, learning behaviors (Jivet et al., 2017). While someresearchers have begun to apply LA to more collaborative learning scenarios (e.g. Bachour, Kaplan, &Dillenbourg, 2010; Blikstein & Worsley, 2016; Ferguson & Buckingham Shum, 2012; Shaffer et al., 2009), manyof these studies report on tools and approaches that have been customized for the researchers, often entailingspecialized equipment, complex visual outputs, or data formatting requirements that impede adoption by studentsand teachers (Vatrapu, Teplovs, Fujita, & Bull, 2011).This paper responds to a central challenge in learning analytics research of interpreting and responding toanalytic information within the flow of curricular activities. Wise and Vytasek (2017) define a learning analyticsimplementation design as “the purposeful framing of activity surrounding how analytic tools, data, and reportsare taken up and used as part of an educational endeavor” (p. 151). LA implementation designs address questionssuch as who should have access to particular kinds of LA, why these LA are being consulted, and how the LAcan be fed back into the educational processes taking place (Wise & Vytasek, 2017). Such questions can beincorporated into a curricular script, which specifies how and when to constrain particular interactions, thesequence in which activities take place, and the roles and responsibilities of individuals within the learningcommunity (Fischer et al., 2013). Whereas scripting refers to the structuring of activities before they are run,orchestration refers to the process of executing a curricular script once the activity has already begun(Dillenbourg, 2015). Several researchers (e.g. Rodríguez-Triana, Martínez-Monés, Asensio-Pérez, & Dimitriadis,2015) have recognized that LA can play an important role in supporting students’ and teachers orchestrationaldecision-making throughout the enactment of CSCL scripts. We have developed such a script, including bothstudent- and teacher-facing LA representations of progress, to investigate the research questions above.MethodologyThis study is part of a broader design-based research project, wherein we worked closely with a high schoolbiology teacher to co-design a KCI curriculum and corresponding technology environment called CKBiology. Inthis paper, we report on data collected during the third design iteration of CKBiology, which entailed onecurricular unit of a Grade 12 Biology course, on the topic of Homeostasis, that was implemented in a blendedlearning environment over a 10-week period during the 2016-2017 academic year.ICLS 2018 Proceedings433© ISLSResearch context, participants and samplingThis research was conducted at a university laboratory school in a large urban area. Activities took place withintwo contexts: (1) In a traditional science classroom with a “bring your own device” (BYOD) policy, and (2) in atechnology-enhanced Active Learning Classroom, which was constructed by the school with the explicit aim offostering productive collaborations between students (see Figure 1b). A purposeful sampling approach was usedto select the teacher participant. Selection was based upon the teacher’s prior experience in KCI research as wellas her availability to design and implement a KCI curriculum during the 2016-2017 academic year. The studentswho participated were an incidental sample in that they were those who happened to be assigned to the classes ofour co-design teacher in two sections of a Grade 12 Biology course (n=28).CKBiology activity structureThere were two types of activities in CKBiology: Lessons and review challenge activities. The lesson activitiescomplemented traditional classroom lectures, and were performed by students within their regular scienceclassroom using their own devices. There were eight lesson topics throughout the Homeostasis Unit, which weretaught over multiple days. Each of these lesson topics was visible on students’ CKBiology home screens (seeFigure 1a), with activities enabled sequentially by the teacher as they were taught. Following each lecture, studentslogged on to CKBiology and selected the corresponding lesson activity, where they were assigned three differenttypes of tasks. The first type of task was to define terms or concepts related to that day’s lesson. The list of termsassociated with a given lesson was established in advance by the co-design team based on the learning goals forthe lesson. Concepts to be defined were divvied up evenly among students in the class. Students’ definitions forthese terms were contributed to the community knowledge base in the form of text-based notes with optionalimages (see Figure 2). The second type of task was to identify relationships between terms or concepts in theknowledge base. Within the CKBiology interface, students were presented with two terms separated by a dropdown list of relationship types. In this case, there was actually a “correct relationship” between each pair of terms,established in advance by the co-design team and programmed into the software. If a student chose the correctrelationship, a line would appear connecting the two terms in the knowledge base. The relationship would alsoappear as a sentence within each note involved in the relationship. For example, the sentence “lysozyme is a typeof antimicrobial protein” would appear in both the “lysozyme” note and the “antimicrobial protein” note. Thethird and final task was to peer review or “vet” definitions that had been submitted by other students in thelearning community. Within the CKBiology interface, students were presented with an anonymized definitionfollowed by the prompt: “Is this explanation complete and correct?” If the student responded “yes” to this prompt,that student’s name would be appended to the note along with the statement “This explanation is complete andcorrect.” If the student responded “no” to the prompt, a text box and image uploader would appear beneath theoriginal note, and the student would be asked to add any new ideas and/or corrected information. Any additionalinformation entered by the student would be appended to the original note along with the student’s name.Subsequent vets performed on that note would also include this appended information.Following the lessons, there were two CKBiology review challenge activities, completed by small groupsof students within the Active Learning Classroom, whose purpose was to help students apply their knowledge to“real-world” inquiry problems. In the first review challenge activity, students selected an area of specialization(i.e. immunology, endocrinology, nephrology, and neurology) and worked within their specialist groups to solvea series of problems in order to become ‘certified’ in their chosen specialization. In the second review activity,students formed jigsaw groups (i.e. “medical clinics”), consisting of one representative from each specialization.Playing the role of medical practitioners, students had to bring together their diverse expertise in order to diagnosea virtual patient with ambiguous symptoms. This included ordering the appropriate lab tests, explaining thereasoning behind their diagnosis, and identifying possible treatment options—thereby consolidating theknowledge they had acquired throughout the unit. In both review challenge activities, a series of scaffoldedquestions were presented to students in CKBiology using a shared group display, and responses were entered bydifferent group members using a wireless keyboard.Materials: Representations of progress in CKBiology1. Progress Bars. There were three kinds of progress bars used in CKBiology: Individual progress bars, grouplevel progress bars, and community-level progress bars. Individual progress bars were used for CKBiology lessonactivities and were visible to individual students on their home screen beside each lesson activity (see Figure 1a),and at the top of the students’ screens as they progressed through their CKBiology lesson tasks (i.e. explainingterms, identifying relationships, and vetting other students’ definitions). The number of tasks assigned to eachICLS 2018 Proceedings434© ISLSstudent was calculated by dividing the total number of concepts, relationships, and vets by the total number ofstudents in the class. The knowledge base was considered ‘complete’ when all of the terms had been defined, allof the relationships had been identified, and when each definition had been vetted at least twice. While the numberof tasks assigned to a student varied from lesson to lesson, on average students were assigned five explanations,five relationships, and 30 vets per lesson throughout the Homeostasis unit. If a student achieved 100% progress,they would have the option of going “above and beyond” their assigned work to make additional contributions,earning themselves a gold star (described below) and additional progress points above 100%. These additionalcontributions typically took the form of extra vetting tasks and did not detract from the assigned work of otherstudents. In this sense, no individual student could dominate the knowledge base (e.g., by defining all terms andrelationships), and every student was still held accountable for making their fair share of contributions.Community-level progress bars appeared on students’ home screens immediately to the right of theirindividual progress bar (Figure 1a). These were expressed as a percentage, with 100% being achieved if allstudents completed their minimum number of assigned tasks. Students who chose to go “above and beyond” theirown assigned work by performing additional vetting tasks could increase community-level progress; however aslong as there were students who did not contribute their fair share, community progress could never reach 100%.Group-level progress bars were used for the review challenge activities only, and were displayed on alarge screen at the front of the Active Learning Classroom while students were working (Figure 1b). The grouplevel progress scores represented the proportion of challenge questions that each group had completed. While allthree types of progress bars served to represent the quantity of work that students had completed, other featuresof CKBiology allowed assessment of the quality of students’ work (e.g. vetting, commenting, and review reports).Figure 1. (a) Student home screen showing individual progress bars (purple) and community-level progress bars(blue) for each lesson. (b) Group-level progress bars publically displayed in the Active Learning Classroom.Figure 2. Community knowledge base (left side) and explanation note (right side). Explanations containingincomplete or incorrect information as a result of student vetting are indicated with a yellow dot.2. Gold stars. The gold star representation was used for CKBiology lesson activities. When studentsachieved 100% progress for their work on a given lesson, they received the message: “Thank you for completingyour submission! Would you like to continue contributing your knowledge to the community?” If the studentchose “yes,” a gold star icon would appear beside their individual progress bar (see Figure 1a), and the studentwould earn additional progress points for each additional task they completed. It was up to the student to decidehow much more they wished to contribute—the limiting factor being the availability of notes that had neitherbeen authored nor previously vetted by them. The gold star icon itself was visible only to the individual student,ICLS 2018 Proceedings435© ISLShowever the teacher was able to see students with progress scores above 100% from her teacher dashboard (i.e.no gold star was present there).3. Community knowledge base. For each lesson, students’ contributions to CKBiology were aggregatedinto a shared community knowledge base, which was visible to all members of the learning community. As shownin Figure 2, concepts and terms with completed definitions appeared in blue and those that had not yet beendefined appeared in grey. This feature of the representation enabled all community members to see at a glancewhere gaps existed in the knowledge base. Clicking on a blue term would open the corresponding note, includingthe original definition and author, followed by any vetting, images (if present), relationships, and comments (ifpresent). Terms that appeared in grey were un-clickable, and the students assigned to those terms were not directlyidentified. Within the knowledge base, a yellow dot was used to identify notes that had been deemed ‘incomplete’or ‘incorrect’ as a result of student vetting. This yellow dot served as a cue to the teacher to take a closer look atthese notes and potentially initiate a follow-up discussion to negotiate or improve upon these ideas as a class.4. Teacher dashboard. For the CKBiology lesson activities, the teacher dashboard displayed eachindividual student’s progress score, ordered from highest to lowest (see Figure 3a). The teacher could also viewthe community-level progress bar for each lesson, and could toggle to and from the knowledge base. For thereview challenge activities, the teacher dashboard included the group progress overview—the same as wasdisplayed publicly for the students. From the group progress overview screen, the teacher could click on anygroup’s name and pull up their “review report” (Figure 3b), which displayed the group’s responses in real-timeas they progressed through their review challenge questions. These responses were color-coded to correspond tothe group member (i.e. specialist) who completed the response. The teacher could then use this information todecide when and where to intervene throughout the activity, and to better tailor her support to each group.Figure 3. Teacher dashboard examples. (a) Progress overview screen, showing the individual progress of eachstudent; (b) Review report, showing real-time responses for a group completing a review challenge activity.Sources of dataTo assess how each of the aforementioned representations was used within the learning community, data wastriangulated from the following sources:Student questionnaire. Students were given a questionnaire using Google Forms, which they completedafter their final review challenge activity. The questionnaire consisted of 16 items, which were formatted using afive-point Likert scale ranging from “Strongly Disagree” (1) to “Strongly Agree” (5). Several of the item stemswere drawn from the “awareness” and “impact” dimensions of the Evaluation Framework for Learning Analytics(EFLA v4; Scheffel, 2017). For questions referring to each kind of representation (i.e., progress bars, gold stars,etc.), an image of the representation was included immediately preceding the corresponding items. At the end ofthe questionnaire, students also had the option of submitting open-ended comments. In total, 19 studentscompleted the questionnaire and six students submitted additional comments. Sample questionnaire items forIndividual progress bars: This representation makes me aware of my current level of progress; The fact that mylevel of progress is visible to the teacher motivates me to increase my progress, if necessary. Sample items for theGold Stars: The ability to earn a gold star motivates me to increase my progress if I've already reached 100%.Sample items for Community Progress bars: This representation makes me aware of the level of progress of thewhole class; This representation motivates me to contribute further, if below 100%. Sample items for GroupProgress bars: This representation allows my group to see when other groups are stuck; This representationmotivated me to contribute further, if below 100%. Sample items for the Community Knowledge Base (ConceptMap): This representation accurately captures all of the important terms/concepts for a given lesson; Thisrepresentation makes me aware of any gaps in the knowledge base.Teacher interview. A semi-structured interview with the teacher was conducted following the finalreview activity. The interview was structured around four images, which were discussed in turn: (1) Theindividual student progress overview, (2) the group-level progress overview, (3) review reports, and (4) theICLS 2018 Proceedings436© ISLScommunity knowledge base representation. The initial prompt for each of these images was “Within the contextof a learning community, how useful was this representation to your practice?” with follow-up questions emergingfrom the resulting discussion. The interview was audio-recorded and transcribed, and lasted approximately 30minutes in duration.CKBiology log data. Two types of log data were used for this study: (1) Students’ individual progressscores for each lesson, and (2) students’ gold star earnings. Only data from the Homeostasis Unit was analyzed.Additionally, only the first seven of eight lessons were included in the analysis; the final lesson was excluded dueto an adjustment in the code that artificially boosted students’ progress scores.Results and discussionProgress barsThere were two student questionnaire items included for all progress bar representations: (1) An “awareness” item(i.e. “This representation makes me aware of [my/my group’s/the community’s] level of progress”), and (2) A“motivation” item (i.e. “This representation motivates me to contribute further, if below 100%. The responses toeach of these items are shown in Figure 4 below.Figure 4. Students’ perceptions of the individual, group, and community-level progressbars with respect to their “Awareness” (left side) and “Motivation” (right side).We performed a Friedman test to identify significant differences in students' perceptions of the individual, group,and community-level progress bars with respect to their “awareness” and “motivation” ratings. While theFriedman test did not reveal any significant differences among students’ “awareness” scores, significantdifferences were identified for students’ “motivation” scores, 𝜒𝜒2 (2, N=19) = 8.55, p < .05. In order to identifywhich pairwise comparisons were significant, we performed a post-hoc Conover-Iman test on the “motivation”data, including a Bonferroni correction. Results indicated that there were no significant differences in students’ratings between the individual and group-level progress bars, however the community-level progress bar wasrated significantly lower than both the individual and group-level progress bars (both p<0.001). These resultssuggest that students felt significantly less motivated to make contributions to the learning community when theysaw that the community-level progress bar was below 100% than they did when their individual progress bar ortheir group’s progress bar was below 100%.Data concerning the context and visibility of these representations provides further insight on studentmotivation. For example, 79% of students agreed or strongly agreed with the statement, “The fact that my levelof progress is visible to the teacher motivates me to increase my progress, if necessary.” This suggests that theteacher’s role as an evaluator of students’ work maintained a heavy influence, even within a context of collectivecognitive responsibility (Scardamalia, 2002). The values of a learning community were in direct conflict withstudents’ inclination to focus on competitive, merit-based aspects of schooling, including university applications.Gold starsOnly 42% of students agreed or strongly agreed with the statement, “The ability to earn a gold star motivates meto increase my progress if I've already reached 100%.” The CKBiology log data revealed that an individualstudent’s gold star-earning behavior did not change very much from lesson to lesson: The students who earned agold star in Lesson 1 (Group A, n=12) tended to remain gold-star earners, while students who did not earn a goldICLS 2018 Proceedings437© ISLSstar in Lesson 1 (Group B, n=16) tended to remain non-gold star-earners. A Welch’s two sample t-test wasperformed to compare the difference in the number of gold stars earned by these two groups. Results indicated asignificant difference in the mean number of gold stars earned by Group A (M=4.2) and Group B (M=0.12); t =−6.9509, p < .0001. Thus, if a student did not earn a gold star in Lesson 1, they were unlikely to earn a gold starin any of the subsequent lessons. Using the same two groups, we compared students’ mean progress scores for allseven lessons. A Welch’s two sample t-test revealed a significant difference in the mean progress score for GroupA (M=119.9) and Group B (M=87.3); t = −3.2741, df = 19.268, p < .01, with students in Group A having a 32.6%higher mean progress score than students in Group B.Knowledge base representation (concept map)Sixty-three percent of students agreed or strongly agreed with the statement that the knowledge base accuratelycaptured all of the important terms/concepts for a given lesson. In the “additional comments” field on thequestionnaire, one student wrote: “There were many terms included that were only circumstantially related to theunit.” In her interview, the teacher also commented: “I think we need to trim the number of terms because it's justtoo many. So, I think we should focus more on the basic ones… But that is not something that we would haveknown going in. Like, this is something that I am actually reflecting now that I went through it.” The teacher alsocommented that it would be helpful to have two different kinds of vetting dots—one for when an explanation isincomplete and another for when an explanation is incorrect: “Because many times I went into the yellow dots andthere was no conflict. There was just, like…somebody put half the definition and then the second person put thesecond half of the definition, and then a third person came in and said ‘oh wait a minute, and these are examplesof blablabla,’ which I thought was great… And then you can take it up in different ways.”Teacher dashboardIn her interview, the teacher commented that the lesson progress overview screen was “very useful because itmade very clear what was happening.” In using the progress overview screen as part of her workflow, the teacherwould look for progress scores that she felt were of concern, and would then delve deeper into those students’work. For example, if she saw a student with an exceedingly high progress score (e.g. in comparison to otherstudents, or to past behavior), she would check the knowledge base to make sure that the student’s explanationsweren’t overly superficial or had been flagged as “incomplete” or “incorrect” by other students. Conversely, ifshe noticed that a student who was typically a high achiever had a low progress score, she would follow up withthe student to see what was happening. Regarding the review challenge reports, the teacher commented: “Thatwas really nice. I like the color-coded because it was easy to follow who was doing what. So, I liked them. It wasclear. I'm very visual, I think…colors help me.” The teacher indicated that she would mostly use the review reportsto check the answers of groups who claimed they were finished: “So ok, these people are done, so I'm going to gosee their answers. Then I would go and check ‘ok, so no - this is not great’ ‘mmm, this needs to be looked after.’Then I would go back to them and say, ‘did you consider blahblahblah.’…And that's how I used it.”Implications and next stepsThis study represented our first effort to infuse KCI curriculum and technology environments with learninganalytics. To begin, we chose relatively straightforward functions of progress representation because of theirfamiliarity to students and potential impact on helping the community make decisions in response to thisinformation. The experiences in designing and evaluating these features will guide our future efforts, as we addmore ‘hidden’ layers of learning analytics, such as tracking groups’ interests and sending materials or promptsbased on contextual information. We can also use more nuanced group analytics to determine when a group mightneed input from the teacher, and send the teacher an active notification in real time. This active form of trackingand notification contrasts with the ambient role of progress representations employed in the present study.ReferencesBachour, K., Kaplan, F., & Dillenbourg, P. (2010). An Interactive Table for Supporting Participation Balance inFace-to-Face Collaborative Learning. IEEE Transactions on Learning Technologies, 3(3), 203–213.Bielaczyc, K., & Collins, A. (1999). Learning communities in classrooms: A reconceptualization of educationalpractice. In C. M. Reigeluth (Ed.), Instructional-design theories and models: A new paradigm ofinstructional theory (Vol. 2, pp. 269–292). Mahwah, NJ: Lawrence Erlbaum Associates Inc.Blikstein, P., & Worsley, M. (2016). Multimodal Learning Analytics and Education Data Mining: Usingcomputational technologies to measure complex learning tasks. Journal of Learning Analytics, 3(2),220–238. https://doi.org/10.18608/jla.2016.32.11ICLS 2018 Proceedings438© ISLSBrown, A. L., & Campione, J. C. (1994). Guided discovery in a community of learners. In K. McGilly (Ed.),Classroom Lessons: Integrating Cognitive Theory and Classroom Practice. Cambridge: MIT Press.Buckingham Shum, S., & Crick, R. D. (2016). Learning Analytics for 21st Century Competencies. Journal ofLearning Analytics, 3(2), 6–21.Chen, B., & Zhang, J. (2016). Analytics for Knowledge Creation: Towards Epistemic Agency and Design-ModeThinking. Journal of Learning Analytics, 3(2), 139–163. https://doi.org/10.18608/jla.2016.32.7Dillenbourg, P. (2015). Orchestration Graphs: Modeling Scalable Education. Lausanne: EPFL Press.Duval, E. (2011). Attention Please!: Learning Analytics for Visualization and Recommendation. In Proceedingsof the 1st Intl. Conference on Learning Analytics and Knowledge (pp. 9–17). New York: ACM.Ferguson, R. (2012). Learning analytics: drivers, developments and challenges. International Journal ofTechnology Enhanced Learning, 4(5/6), 304–317.Ferguson, R., & Buckingham Shum, S. (2012). Social learning analytics: five approaches (pp. 23–33). Presentedat the 2nd International Conference on Learning Analytics & Knowledge, Vancouver, Canada.Fischer, F., Kollar, I., Stegmann, K., & Wecker, C. (2013). Toward a Script Theory of Guidance in ComputerSupported Collaborative Learning. Educational Psychologist, 48(1), 56–66.Jivet, I., Scheffel, M., Drachsler, H., & Specht, M. (2017). Awareness is not enough: Pitfalls of learning analyticsdashboards in the educational practice. In E. Lavoué, H. Drachsler, K. Verbert, J. Broisin, & M. PérezSanagustin (Eds.), EC-TEL 2017: Data Driven Approaches in Digital Education (Vol. 10474, pp. 82–96). Tallinn, Estonia: Springer. https://doi.org/10.1007/978-3-319-66610-5_7Kling, R., & Courtright, C. (2003). Group behavior and learning in electronic forums: A sociotechnical approach.The Information Society, 19(3), 221–235.Rodríguez-Triana, M. J., Martínez-Monés, A., Asensio-Pérez, J. I., & Dimitriadis, Y. (2015). Scripting andmonitoring meet each other: Aligning learning analytics and learning design to support teachers inorchestrating CSCL situations. British Journal of Educational Technology, 46(2), 330–343.Scardamalia, M. (2002). Collective cognitive responsibility for the advancement of knowledge. In B. Jones (Ed.),Liberal education in a knowledge society (pp. 67–98). Chicago: Open Court.Scardamalia, M., & Bereiter, C. (2006). Knowledge building: Theory, pedagogy and technology. In R. K. Sawyer(Ed.), Cambridge Handbook of the Learning Sciences (pp. 97–118). Cambridge University Press.Schwartz, D. L., & Arena, D. (2013). Measuring what matters most: Choice-based assessments for the digitalage. Cambridge, MA: MIT Press.Schwendimann, B. A., Rodríguez-Triana, M. J., Vozniuk, A., Prieto, L. P., Boroujeni, M. S., Holzer, A., …Dillenbourg, P. (2016). Understanding Learning at a Glance: An Overview of Learning DashboardStudies. In Proceedings of the 6th International Conference on Learning Analytics & Knowledge (pp.532–533). New York: ACM. https://doi.org/10.1145/2883851.2883930Shaffer, D. W., Hatfield, D., Svarovsky, G. N., Nash, P., Nulty, A., Bagley, E., … Mislevy, R. J. (2009). Epistemicnetwork analysis: A prototype for 21st-century assessment of learning. International Journal of Learning& Media, 1(2), 33–53.Siemens, G. (2012). Learning Analytics: Envisioning a Research Discipline and a Domain of Practice. InProceedings of the 2nd Conference on Learning Analytics and Knowledge (pp. 4–8). New York: ACM.Slotta, J. D. (2014). Knowledge Community and Inquiry. Paper presented at the Network of Associated Programsin the Learning Sciences (NAPLES).Stahl, G. (2015). The group as paradigmatic unit of analysis: The contested relationship of CSCL to the learningsciences. In M. A. Evans, M. J. Jacker, & R. K. Sawyer (Eds.), Reflections on the Learning Sciences.Cambridge, UK: Cambridge University Press.Tissenbaum, M., & Slotta, J. D. (2015). Scripting and Orchestration of Learning Across Contexts: A Role forIntelligent Agents and Data Mining. In L.-H. Wong, M. Milrad, & M. Specht (Eds.), Seamless Learningin the Age of Mobile Connectivity (pp. 223–257). Springer Singapore.van Aalst, J., & Chan, C. K. (2007). Student-directed assessment of knowledge building using electronicportfolios. Journal of the Learning Sciences, 16(2), 175–220.Vatrapu, R., Teplovs, C., Fujita, N., & Bull, S. (2011). Towards Visual Analytics for Teachers’ DynamicDiagnostic Pedagogical Decision-making. In Proceedings of the 1st International Conference onLearning Analytics and Knowledge (pp. 93–98). Banff, AB: ACM.Wise, A. F., & Vytasek, J. (2017). Learning Analytics Implementation Design. In C. Lang, G. Siemens, A. F.Wise, & D. Gašević (Eds.), Handbook of Learning Analytics (pp. 151–160). SoLAR.AcknowledgementsThis work was supported by the Social Sciences and Humanities Research Council of Canada.ICLS 2018 Proceedings439© ISLS