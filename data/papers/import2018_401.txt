Individualized Assessment and Automated Feedbackin Undergraduate Computer Science EducationOmid Mirmotahari, Crina Damsa, and Yngvar Bergomidmi@ifi.uio.no, crina.damsa@iped.uio.no, yngvarb@ifi.uio.noUniversity of OsloAbstract: This study examines modalities of generating and providing automated formativefeedback to Computer Science undergraduate students’ and their experiences with the systemand the received feedback. A software for automated feedback has been developed and used inthis study in order to both support the students learning process, and to test how a set ofelaborated assessment criteria can contribute to the students better understanding of their ownknowledge and learning. The findings show that providing the students with explicit feedbackin addition to regular summative evaluation (grades and point sum) stimulated theirunderstanding and awareness of their knowledge level and exam performance. Besides, ittriggered ideas and reflections related to their future learning steps and approaches, whichfollows the principles of feedforward.IntroductionThis contribution presents case studies of the development and use of a software program and criteria forproviding automatic feedback in a Computer Sciences undergraduate program. Feedback is viewed as apedagogical strategy in teaching-learning environments that has potential to facilitate the students’ learning in ameaningful manner (Jansson, 2006). Greenhow’s (2015) argues for the inclusion of a formative componentwhen developing digital environments for providing feedback. The increase in student population and the everevolving knowledge to be conveyed makes the task of providing feedback and assessment that has a formativevalue becomes quite difficult. Usually, the automatic feedback in a digital assessment system is given by a shortindication of whether the answer is wrong or right. A number of studies examined the effectiveness of providingformative feedback for summative computer-aided assessment, by giving individualized feedback derived fromeach of the five results sections of the assessment was provided to each student (Lewis & Sewell, 2007), or howan automated short-answer marking system can be effectively used to improve teaching and learning atuniversity level (Siddiki et al., 2010). In the latter, the system did not allow features that can provide detailedstatistical analysis of students' performances for both lecturers and students so that each may adjust or modifytheir teaching or learning approach for the course.Our study addresses these issues and aims to provide a better understanding of how a software programdeveloped for providing automatic formative feedback in a Computer Sciences undergraduate course wasimplemented, how the automated feedback contributed to improved learning and how the students experiencedboth the use of the system and receiving feedback in this manner. Whether students engage productively withfeedback, whether it enhances their learning and performance, and whether automated feedback can havemeaningful role in these processes are questions that require empirical examination. For the feedback process tobe productive, learners need to make meaning of the relevant criteria and standards, how their performancecompares against them and what they can do to improve against those standards. This is achievedcollaboratively between students, teachers, tools, course activities. From a socio-material perspective, the toolscan facilitate conveying the feedback, that is, they mediate the process. In this case, the automated feedbacksoftware becomes an entity and a meaning-making resource intertwined in the interaction between students,teachers, and standards for learning and assessment, which has potential to lead to a better understanding of ownprocess and performance.MethodsThe studies presented in this paper were conducted in the context of Computer Sciences program at a largeuniversity in Norway. Dataset from two courses are included in this paper. In each course, lectures, labs weeklyassignments and course compulsory assignment were part of the course design. Feedback was provided on thesubmitted assignments. The software program (Mirmotahari, 2016) was initially developed to facilitate examassessment, but it gradually displayed great potential for being used to give formative feedback. The programwas designed with several stakeholders and users in mind, namely (i) exam evaluators; (ii) students,; and (iii)the teachers. The program can provide both the arguments for the grade as well as an individual formativefeedback. If the evaluator or the teacher chooses to provide students with both arguments for their grade and anICLS 2018 Proceedings1577© ISLSindividual formative feedback, the first part of the feedback will consist of the arguments for each assignmentand grade. The second part will be an individual formative feedback based on feedforward principles. The maincomponent of the assessment program is the generation of the criteria, their weight in measurement and thetextual phrases linked together. A taxonomic model (inspired roughly by Bloom’s taxonomy) was used todevelop a set of criteria that focused on the students’ learning understanding of abstract knowledge and the wayto employ this in solving computing problems. The back-end of the program is constantly monitoring andanalyzing the evaluator’s choice and overruns. The results ofthese analysis lead to individual feedback to each student. Thefeedback consists of three main parts; (i) academic feedbackand discipline-based justification of grade, (ii) personalfeedforward and finally (iii) a profiling for the learningoutcome. The length of the feedback is entirely dependent onthe amount of choices the evaluator has made and theaccumulated sum of the weights of the criteria throughout thewhole assignment. The accumulated sum for each criterion isnormalized to the classes and based on predefined thresholdsgroups the results. All the students’ hand-in assignments werescanned and automatically uploaded into this assessmentprogram. After each iteration, the students were asked toFigure 1. Software program interface.answer an online questionnaire. The average response rate hasbeen 77%. Different questionnaires have been used to collectin answer regarding one or more of these topics: questions about the assignment; perception of the feedbackreceived; evaluation of the technical aspects of the assessment program (computer program, usability, and timeusage); development in relation to the professional domain; their experience of being a peer reviewer; learningoutcome for the students as a participants and a peer-reviewers. The results from the questionnaires were alsodiscussed in the qualitative interviews. Since the questionnaire was anonymous, there was no opportunity toconnect the questionnaires with the interviews. We have conducted qualitative interviews with 15% of theenrolled students.Findings and relevanceThe findings indicate positives experiences and students benefiting from the feedback. In line with argumentsmade by Greenhow (2015) and Siddiki et al. (2010), the findings show that providing the students with explicitfeedback in addition to regular summative evaluation (grades and point sum) stimulated their understanding andawareness of their knowledge level and performance. The program supports providing feedback specificallyaimed at the students' professional skills, triggering focused alternatives for future learnings steps andreflections related to their future learning steps and approaches. The study also provides insights into howautomated feedback generated through the use of criteria can be organized by means of a dedicated softwareprogram. At the level of practice, the method employed provides an innovative available for the teacher and thesensor to provide the students feedback. The developed system involves teacher’s work to define in writing whatgiven values of criteria mean and the different feedback will make it easier to calibrate the evaluators across thesubject. That way, not only the students benefit from this approach, but also the teachers/evaluators gain betterunderstanding of what is required of the students for the various assignments and future learning. Follow-upstudies are recommended in order to examine the quality of feedback not only based on the students perceptions,which have a subjective nature, but also based on quality criteria distilled from specialist literature.ReferencesGreenhow, M (2015). Effective computer-aided assessment of mathematics; principles, practice and results.Teaching Mathematics and its Applications: An International Journal of the IMA, 34(3), 117 - 137.Lewis, D. J. A. &. Sewell, R. D. E (2007). Providing Formative Feedback From a Summative Computer-aidedAssessment. American Journal of Pharmaceutical Education, 71(2), https://doi.org/10.5688/aj710233Mirmotahari, O. & Berg, Y. (2016). Individuell «automagisk» tilbakemelding på skriftlig eksamen. NordicJournal of STEM Education, Vol. 1, No. 1, 287-293.Siddiqi, R., Harrison, C.J., Siddiqi, R., Improving Teaching and Learning through Automated Short-AnswerMarking, IEEE Transactions on Learning Technologies, Vol. 3, No. 3, pp 237-249.ICLS 2018 Proceedings1578© ISLS