The Use of Visual Evidence for Planning and ArgumentationRebecca Cober, University of Toronto, rebecca.cober@utoronto.caAlisa Acosta, University of Toronto, alisa.acosta@utoronto.caMichelle Lui, University of Toronto, michelle.lui@utoronto.caTom Moher, University of Illinois at Chicago, moher@uic.eduAlex Kuhn, University of Michigan, kuhnalex@umich.eduChris Quintana, University of Michigan, quintana@umich.eduJim Slotta, University of Toronto, jim.slotta@utoronto.caAbstract: We report on two learning environments where students used visual evidence(digital photographs) for the scientific practices of planning and argumentation. The first is aknowledge-building environment called Neighborhood Safari, where Grade 5/6 students(n=45) construct investigation plans concerning schoolyard wildlife; the second is animmersive simulation called EvoRoom/Zydeco, where Grade 11 students (n=51) captureobservational evidence to support knowledge claims. We developed coding schemes to assesssupport levels (ranging from 0-4) provided by textual and visual evidence concerning(respectively) students’ (1) investigation plans for observing schoolyard wildlife with cameratraps and (2) knowledge claims about climatic conditions in an immersive rainforestsimulation. Textual evidence was found to provide greater support for the scientific practicesof planning and argumentation than visual evidence. High-level visual evidence madeconnections to investigation plans and arguments, using (1) visual annotations (e.g., arrows),(2) comparison or contrasting images, (3) explanatory captions or (4) compositionaltechniques (e.g., cropping).Keywords: science practices, visual evidence, collaborative inquiry, mobile applicationsIntroductionA central assumption that underlies contemporary science education standards (e.g., NGSS Lead States, 2013) isthat learning should be situated within authentic scientific practice. In this paper, we report on two learningenvironments where students used visual evidence (i.e., non-textual artifacts, such as photographs and drawings)to support two practices that the NGSS has deemed essential for students to learn: (1) planning and carrying outan investigation and (2) engaging in argument from evidence. Our work is situated within the theoreticaltradition of classrooms as knowledge communities, where students engage in methods of knowledgeconstruction that are in line with authentic scientific practice. As students work together as a knowledgecommunity, a collaborative knowledge base is constructed through the sharing of data, ideas, and theorieswithin a rich social environment (Scardamalia & Bereiter, 1999).Inspired by this work, we are investigating our own theoretical model known as KnowledgeCommunity and Inquiry (KCI) (Peters & Slotta, 2013). This approach situates knowledge construction within atechnology-mediated environment, and scaffolds student groups as they contribute new ideas, theories, data, andinformation to a knowledge base, allowing the community to make directed progress towards inquiry goals. TheKCI model guided the inquiry curriculum design to support knowledge building in two case studies. In the firstcase study, we describe an environment where students exchanged digital inquiry notes concerning their plannedinvestigations of wildlife in their schoolyard. In the second case study, we present an immersive simulationenvironment where students captured and exchanged observational evidence to support scientific argumentationabout climatic conditions in a tropical rainforest. The goal of these two case studies is to understand howstudents used visual evidence to support the scientific practices of planning and argumentation. In our analysis,we focus on students’ use of visual evidence—in the form of digital photographs—from the communityknowledge base. Specifically, we ask: Does students’ use of textual evidence differ from their use of visualevidence for plans and arguments? We aim to reveal the kinds of technological and pedagogical supports thatlearners may need to engage in planning and argumentation when using visual evidence.Visual evidenceAs a regular part of scientific practice, scientists use both written material (e.g., text) and visual material (e.g.,graphs, diagrams) to make discoveries (Coopmans et al., 2014) and communicate findings with peers and thepublic (Tufte, 2006). As new forms of data capture become increasingly a part of students’ everyday experienceCSCL 2015 Proceedings548© ISLS(e.g., using a mobile device to take a photograph), such practices are also becoming more common in scienceclassrooms. For example, the Zydeco platform enables students to use mobile devices to contribute textualevidence (e.g., written descriptions) and visual evidence (e.g., photographs and videos) to a shared repository(Quintana, 2012). Students can review their own textual and visual data and that of their peers, and use thesedata to support knowledge claims. These skills align with a third NGSS practice—analyzing and interpretingdata. They are also consistent with an expectation from the Ontario curriculum, which is that studentscommunicate findings using a variety of forms, including oral, written, and visual formats (Ontario Ministry ofEducation, 2007). However, such use of visual evidence may introduce new challenges, and more work isneeded to understand how learners make produce and make sense of visual forms (Ainsworth et al., 2011).Neighborhood Safari: Using visual evidence to support planningA six-week unit engaged middle school students in schoolyard investigations of urban wildlife using motionactivated cameras called “camera traps.” Students set out several camera traps in succession, collecting andusing images from different locations to collaboratively understand patterns of animal behavior. Studentsstrategically positioned their camera traps in the schoolyard four times, for a period of 2-3 days each. One goalwas for students to become proficient at planning investigations (i.e., where to place the camera trap) based onenvironmental cues (e.g., potential food sources) and previous results (e.g., animal sightings).MethodStudents (n=45) from two Grade 5/6 classes from an elementary school in Ontario, Canada participated in thefirst case study. Student groups used a Web-based application for mobile devices called Common Knowledge(Fong et al., 2013) to share investigation plans with peers. Software scaffolds prompted students to describewhere they would place their camera trap, provide a rationale for camera trap placement, and attach justificatoryphotographs (e.g., of an annotated schoolyard map; see Figure 1). These notes were contributed to a sharedknowledge base, and students were able to view the contributions of other groups in real time.Figure 1. Left: Detail of planning map, Middle: Camera trap photo of raccoon, Right: iPad photo of footprintsAnalysis and findingsWe analyzed the planning notes that contained a discernable plan (n=46). First we identified the plan from eachnote, and then we compared textual support (i.e., the written rationale students gave for their plan) and visualsupport (i.e., the justificatory photographs) against each plan.Table 1: Coding scheme for Neighborhood Safari planning notesLevel01234Textual support by students for planNo rationale given for the plan.Text provides an unreasonable expectation for theplan (e.g., We expect to see if the raccoon is female).Some of the text provides a partially reasonableexpectation for the plan, but was too broad—e.g., wewant to see squirrels—or contradictory).Text provides a reasonable expectation for the planand reasoning is implicit (e.g., We expect to seeraccoons by the garbage cans).Text provides evidence to support a reasonableexpectation and reasoning is explicit (e.g., We expectto see raccoons by the garbage cans because we haveseen them there before).CSCL 2015 Proceedings549Visual support by students for planNo justificatory photographs attached to the plan.Photograph was irrelevant to the plan (e.g.,photograph of two students in their classroom)Photograph(s) provide(s) partial description of plan(i.e., information needed to carry out the plan ismissing).Photograph(s) contain descriptive elements relatingto the plan (e.g., map shows proposed location ofcamera trap).Photograph(s) provide(s) justification for plan (e.g.,photo of planning map and camera trap photo ofraccoons near garbage cans).© ISLSThere were 65 photographs attached to these notes, with an average of 1.4 photographs per plan,ranging between 0-5 photographs per plan. Thirty-nine of the photographs were of maps, 15 were from cameratraps, and 11 were taken using the iPad camera. Following the process outlined by Hruschka et al. (2004) fordeveloping a reliable coding scheme, we created codes to assess the level of support provided by (1) textualevidence and (2) visual evidence for the plan. For both textual and visual support, we identified characteristicsfor five levels, from 0 (none) to 4 (highest; see Table 1).The two raters independently coded 20% of the data, with IRR scores reaching perfect agreement fortextual support and substantial agreement for visual support (Kappa=0.61, p=0.005). After independently codingall of the data, the raters reached agreement for all data through discussion. Analysis indicated that the level oftextual support for plans was higher (M=3.5, SD=0.6) than visual support (M=2.67, SD=1.3).EvoRoom/Zydeco: Using visual evidence to support argumentationWe engaged high school biology students in a 75-minute activity (part of a 10-week unit) concerning the longterm effects of climatic events (e.g., earthquake, tsunami, high sunlight, low rainfall) on a Borneo rainforestenvironment (Lui et al., 2014). An immersive simulation displayed four different scenarios (one per classroomwall), each of which visually depicted the effects of one climatic event (e.g., dry plants and soil indicated effectsof low-rainfall). Using their knowledge of how various climatic events would impact biodiversity, studentgroups were tasked with identifying which of the four stations depicted their assigned climatic scenario.MethodStudents (n=51) in two Grade 11 biology classes from a high school in Toronto, Canada participated. Studentsused the Zydeco application (described previously) to: (1) make observations of the environment, (2) add aclaim (e.g., “Station A is the rainforest affected by low rainfall”) and, (3) justify their claim by explaining howthe effects of a particular climatic event were evident. Using the iPad camera, students captured photographs ofthe simulation (either the entire screen or a detail shot – see Fig. 2) to provide evidence for their claims.Figure 2. Left: EvoRoom, Middle: Student collecting evidence with Zydeco using iPad, Right: iPad photo detailAnalysis and findingsWe analyzed the claim notes (n=33) written by student groups, coding separately the written explanations andthe corpus of photographs that student-groups supplied as support for each claim. Captions that accompaniedphotographs were coded as part of the visual support. Ninety-six photographs were attached to these claims,with an average of 2.9 photographs per claim, ranging between 0 and 9 photographs per claim. The first twoauthors followed the process outlined previously to develop a coding scheme to assess levels of supportprovided by textual evidence and visual evidence for claims (see Table 2). For both textual and visual support,we identified characteristics for five levels, from 0 (none) to 4 (highest; see Table 2).Table 2: Coding scheme for EvoRoom/Zydeco claim notesCode01Textual support by students for claimText rationale is not providedText is irrelevant to the claim2Text provides partial support for the claim (i.e.,some statements are contradictory or irrelevant).Text provides descriptive evidence that is congruentwith the claim and reasoning is implicit.Text provides an explanation of the evidence andreasoning is explicit.34CSCL 2015 Proceedings550Visual support by students for claimPhotographic evidence is not providedPhotograph is irrelevant to the claim (e.g.,photograph of two students in their classroom)Photographs provide partial support for claim (i.e.,contradictory or irrelevant photographs are included).Photographs provide support for claim (e.g., anoverview photograph of a simulation panel).Photographs provide convincing evidence to supportthe claim, revealing specific useful details (e.g., closeups of a simulation).© ISLSThe two raters independently coded 20% of the data, with perfect agreement for textual evidence andsubstantial agreement for visual evidence (Kappa=0.7, p=0.001). All of the data was independently recoded, andthe raters reached agreement for all data through discussion. Analysis indicated that the textual support for theclaims was coded higher (M=3.0, SD=1.4) than visual support for claims (M=2.63, SD=1.34).Conclusions and implicationsFor the purposes of analysis, we made a distinction between written evidence and visual evidence, although thenote-sharing tools we used displayed text and photographs side-by-side, making them “colleagues inexplanation” (Tufte, 2006, p. 83). However, it was important for us to tease these two evidence streams apart inorder to understand if the quality of students’ written support (text) differed from visual support (photographs).Our results showed that in both case studies, students achieved higher mean scores for textual evidence than forvisual evidence. This suggests that these students had greater fluency in constructing written explanations thatusing visual evidence to support their reasoning, possibly due to curricular emphases on written literacy.We examined the characteristics of high-level visual evidence in order to extract lessons concerningways that learners can be supported in communicating findings using visual representations within technologymediated environments. In Neighborhood Safari, high-level visual evidence often contained annotations (e.g.,arrows drawn on a map) to provide justification for a camera trap placement plan. In EvoRoom, high-levelvisual evidence often included comparisons or contrasting images to more strongly illustrate claims. Captionsalso served to make connections between visual and written evidence more explicit. Students usedcompositional techniques (e.g., cropping to reveal specific details) to provide their peers with interpretive visualcues (Gilbert, 2008). These findings point to strategies that researchers and designers could use that wouldprovide students with tools to facilitate tighter integration of textual and visual evidence, and expand work onhow verbal and visual evidence are used to justify and dispute claims (e.g., Oestermeier & Hesse, 2000). Forexample, future designs may benefit from the ability to annotate photographs, such as the use of digital drawingtools (e.g., circles, arrows, text boxes) as overlays, drawing the viewer’s attention to important elements withinthe visual representation. This could be particularly beneficial in cases where images are drawn from a sharedknowledge base, for which students may be using a photograph for different reasons.ReferencesAinsworth, S., Prain, V., & Tytler, R. (2011). Drawing to learn in science. Science, 333(6046), 1096-7.Coopmans, C., Vertesi, J., Lynch, M., & Woolgar, S. (Eds.). (2014). Representation in scientific practicerevisited. Cambridge, MA: MIT Press.Fong, C., Cober, R. M., Madeira, C. A., Messina, R., Murray, J., Peebles, B., & Slotta, J. D. (2013). CommonKnowledge: Orchestrating Synchronously Blended F2F Discourse in the Elementary Classroom. CSCL2013 Conference Proceedings (Vol. 2, pp. 26-29). Madison, WI,: ISLS...Gilbert, J. K. (2008). Visualization: An emergent field of practice and enquiry in science education.In Visualization: Theory and practice in science education (pp. 3-24). Springer: Netherlands.Hruschka, D. J., Schwartz, D., John, D. C. S., Picone-Decaro, E., Jenkins, R. A., & Carey, J. W. (2004).Reliability in coding open-ended data: Lessons learned from HIV behavioral research. FieldMethods, 16(3), 307-331.Lui, M., Kuhn, A., Acosta, A., Quintana, C., & Slotta, J. D. (2014). Supporting learners in collecting andexploring data from immersive simulations in collective inquiry. Proceedings of ACM SIGCHIConference on Human Factors in Computing Systems (CHI ’14), (pp. 2103-2112). ACM Press.NGSS Lead States (2013). Next Generation Science Standards: For States, By States. Washington, DC: TheNational Academies Press.Oestermeier, U., & Hesse, F. W. (2000). Verbal and visual causal arguments.Cognition, 75(1), 65-104.Ontario Ministry of Education. (2007). The Ontario Curriculum Grades 1-8: Science and Technology.Retrieved from http://www.edu.gov.on.ca/eng/curriculum/elementary/scientec18currb.pdfPeters, V. L., & Slotta, J. D. (2010). Scaffolding knowledge communities in the classroom: New opportunitiesin the Web 2.0 era. In M. J. Jacobson & P. Reimann (Eds.), Designs for learning environments of thefuture: International perspectives from the learning sciences (pp. 205-232). Secaucus, NJ: Springer.Quintana, C. (2012). Pervasive science: Using mobile devices and the cloud to support scienceeducation. interactions, 19(4), 76-80.Scardamalia, M., & Bereiter, C. (1996). Student communities for the advancement ofknowledge. Communications of the ACM, 39(4), 36-37.Tufte, E. R. (2006). Beautiful evidence (Vol. 1). Cheshire, CT: Graphics Press.CSCL 2015 Proceedings551© ISLSAcknowledgmentsThis material presented here is based on work supported by the U.S. National Science Foundation under grantsIIS-1065275 and DRL-1020027 and Canadian Social Sciences and Humanities Research Council under grant410-2011-0474.CSCL 2015 Proceedings552© ISLS