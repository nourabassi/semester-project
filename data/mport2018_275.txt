Crowdsourcing and Education:Towards a Theory and Praxis of LearnersourcingShayan Doroudi (Chair, Co-Organizer), Carnegie Mellon University, shayand@cs.cmu.eduJoseph Jay Williams (Co-Organizer), National University of Singapore, williams@comp.nus.edu.sgJuho Kim, Korea Advanced Institute of Science and Technology, juhokim@kaist.ac.krThanaporn Patikorn, Worcester Polytechnic Institute, tpatikorn@wpi.eduKorinn S. Ostrow, Worcester Polytechnic Institute, ksostrow@wpi.eduDouglas Selent, Worcester Polytechnic Institute, dselent@wpi.eduNeil T. Heffernan, Worcester Polytechnic Institute, nth@wpi.eduThomas Hills, University of Warwick, t.t.hills@warwick.ac.ukCarolyn P. Rosé (Discussant), Carnegie Mellon University, cprose@cmu.eduAbstract: Due to the scale of online environments, large numbers of learners interact with theexact same resources, such as online math homework problems and videos. It is thereforeessential these are of the highest quality to help learners. Ideally, online educational resourceswould constantly improve based on data and input from each learner, giving a better outcomefor the next. This symposium explores issues around the use of crowdsourcing to harnesslearners’ interactions with resources like online problems and videos in order to improve theseresources for the next learner. We hope to explore the benefits and limitations of thinkingabout learners through the lens of crowdsourcing, to imagine learnersourcing. We will discussfour ways in which researchers have leveraged crowdsourcing to help students learn in avariety of educational contexts, and in doing so we will also discuss ways in whicheducational theory can guide the future of learnersourcing.IntroductionDue to the scale of online environments, large numbers of learners interact with the exact same resources, suchas online math homework problems and videos. It is therefore essential these are of the highest quality to helplearners, but it is difficult to know how best to design these before deploying them ‘in the wild’. Ideally, onlineeducational resources would constantly improve based on data and input from each learner, giving a betteroutcome for the next. This symposium explores issues around the use of crowdsourcing to harness learners’interactions with educational resources in order to improve these resources for the next learner.Crowdsourcing is the outsourcing of jobs or tasks to a large number of (amateur or novice) people,typically through the use of technology, rather than relying on a single expert (Saxton et al., 2009).Crowdsourcing (especially when referred to as “human computation”) can be viewed from the perspective ofusing humans to do something that state of the art artificial intelligence is not capable of doing (Law & vonAhn, 2011). Unfortunately, crowdsourcing does not always have the reputation of being human-centered; forexample, using crowdsourcing to harness the “computational power” of people to complete microtasks, as isoften done on websites such as Amazon Mechanical Turk and Crowdflower, does not evoke the image of atechnology we want to use with our students. But recently, researchers have been exploring many ways in whichcrowdsourcing can be used in more human-centered applications (Bigham et al., 2010; Lasceki et al. 2014;Andolina et al. 2017) as well as finding ways to value the people behind crowdwork (Kittur et al. 2014; Salehiet al. 2015; Gaikwad et al. 2015). Moreover, with the growth of crowdsourcing, researchers and educators arebeginning to realize how students can interact with resources generated and improved by their peers inmeaningful ways for a more enriched learning experience (Weld et al. 2012; Heffernan et al. 2016; Paulin &Haythornthwaite, 2016). In this symposium, we explore various ways to tap into that potential of crowdsourcingto help learners in a variety of settings, from massive open online courses (MOOCs) to online learningplatforms used in K-12 classrooms to instructors using crowdsourcing in their own classrooms. We will look atnot only practical ways that several researchers have used crowdsourcing in these settings, but also thetheoretical insights from the learning sciences and broader educational theory that motivate their approaches.We hope to explore the benefits and limitations of thinking about learners through the lens of crowdsourcing, toimagine learnersourcing (a term originally coined by Juho Kim (2015)).This symposium explores in depth some of the different ways that researchers might understand andapproach various conceptions of learnersourcing. We will discuss the similarities and differences in ways thatfour research groups have used crowdsourcing to impact learners in different settings. Despite the differentICLS 2018 Proceedings1267© ISLSsettings and various approaches to crowdsourcing, one theme that exists across the four presentations in thissymposium is that crowdsourcing can help both the learners involved in generating new content, as well as thelearners who then engage with the crowdsourced content. This vision of how crowdsourcing can impactstudents, is nicely described by Duffy and Cunningham (1996) in a metaphor for their account of constructivistinstruction:In his popular novel The Name of the Rose, Umberto Eco (1983) describes a medieval library,a labyrinth of passages, stairways, and chambers filled with books...learning is illustrated byBrother William, the main character of the novel, feeling and groping his way through thelibrary. As Brother William constructs a path (or pattern of connections) through the library,one of only many possible paths, he is transforming his means of participating in thecommunity of scholars, both those using the library (constructing their own paths) and thosewho have written manuscripts contained therein."Learnersourcing and the Learning SciencesLearning scientists can benefit learnersourcing in a number of ways by (1) bringing theories to bear onpedagogically valuable ways of approaching learnersourcing, (2) studying how teachers and researchers caneffectively use learnersourcing to help learners, (3) studying how learners can effectively engage withlearnersourced resources, and (4) creating systems that can support effective forms of learnersourcing. In doingso, the learning sciences can benefit from better theoretical insights on how students learn and interact in waysmediated by learnersourcing. These insights might transcend beyond learnersourcing into other areas of thelearning sciences, such as computer-supported collaborative learning. Moreover, this symposium will discusshow effectively integrating crowdsourcing and education requires bridging insights from psychology andeducation with systems and algorithms from computer science. Our hope is to show the promise of forming newcollaborations between learning scientists and computer scientists. Such collaborations can have a long-lastingimpact on the future of the learning sciences. Finally, this symposium is particularly relevant this year, ascrowdsourcing is also a topic of interest to the Learning @ Scale and Artificial Intelligence in Educationcommunities, both of which are having co-located conferences with ICLS this year. Our hope is to bringtogether researchers from these fields.Symposium synopsisThe symposium will include four presentations. Juho Kim will present work on learnersourcing for helpinglearners more effectively navigate and use online instructional videos, such as those in MOOCs. Joseph JayWilliams will then report on a system that prompts learners to self-explain, presents those explanations to futurelearners, and then discovers which explanations future learners find most helpful by using machine learning toconduct dynamic experiments. The ASSISTments group will report on their work on deploying a platform usedin many K-12 classrooms that allows students to provide their work as assistance for other students. The groupwill also describe an instance of teachersourcing in their platform, broadening the scope of how crowdsourcingcan impact educational practice. Finally, Thomas Hills discusses how he used crowdsourcing in hisundergraduate level course on the psychology of persuasion in order to allow students to create content thatinfluenced the course by bringing their own interests to the table. As the discussant, Carolyn Rosé will share herinsights on the similarities and differences in the approaches taken by the four presenters, how approaches tolearnersourcing relate to the learning sciences and computer-supported collaborative learning, and what aresome of the limitations of learnersourcing as currently envisioned.A goal of this symposium is to see how theoretical insights from the learning sciences, psychology, andbroader educational theory can guide the application of ideas from crowdsourcing. For example, Joseph JayWilliams’ considers how crowdsourcing explanations from students can be of pedagogical value to thosestudents by building upon the literature on self-explanation. Similarly, Thomas Hills describes how his use ofcrowdsourcing is guided by theories of learner-centered education such as constructivism and related principlescoming from cognitive psychology.Given that the symposium is about crowdsourcing, the session will also include two interactivecrowdsourced components. First, we will ask attendees at the beginning of the session to tell us (through aGoogle form) their first impression of applying crowdsourcing to education (whether that is a thought, idea,question, or concern). We will then ask them the same question at the end of the session. Our goal is to haveattendees reflect on what they think about crowdsourcing and give us a sense of the attendees’ prior backgroundand opinions and how our session might have changed their opinions. Second, we will have a crowdsourcedQ&A session, where attendees can “upvote” and comment on questions, both so that questions that are ofICLS 2018 Proceedings1268© ISLSgreatest interest will get answers by participants, but also so that online discussions can start and continueamong attendees. We will now describe the contributions of each of the four presentations.Learnersourcing: Improving learning with collective learner activityJuho KimMy research has focused on improving the video learning experience online. My primary approach has beenlearnersourcing, in which learners collectively generate novel content and interfaces for future learners whileengaging in a meaningful learning experience themselves. Millions of learners today use educational videosfrom online platforms such as YouTube, Khan Academy, Coursera, or edX. Learnersourcing can improve thecontent and interfaces in a way neither experts, nor computers, nor existing crowdsourcing methods can achieveat scale. My research demonstrates that interfaces powered by learnersourcing can enhance content navigation,create a sense of learning with others, and ultimately improve learning.I draw on several fields to design learnersourcing applications: crowdsourcing to aggregate smallcontributions into meaningful artifacts; social computing to motivate participation and build a sense ofcommunity among learners; content-based video analysis techniques such as computer vision and naturallanguage processing to complement learner input; and the learning sciences to inform the design oflearnersourcing tasks that are pedagogically meaningful. I explore two types of learnersourcing: passivelearnersourcing uses data generated by learners’ natural interaction with the learning platform, and activelearnersourcing prompts learners to provide specific information.Passive Learnersourcing: Natural learner interactions improve video learningIn traditional classrooms, teachers adapt their instruction to students based on their level of engagement andconfusion. While online videos enable access for a wide audience, instructors and learners are disconnected; it isas if instructors are talking to a wall without feedback from learners watching the video. I created a thread ofresearch that leverages natural learning interaction data to better understand and improve video learning,specifically using thousands of learners’ second-by-second video player interaction traces (e.g., clicking theplay button in the video player).Figure 1. An example interaction peak.Figure 2. LectureScape: lecture videoplayer powered by interaction data.Data analysis of 39 million MOOC video clicksExploratory data analyses of four massive open online courses (MOOCs) on the edX platform investigated 39million video events and 6.9 million watching sessions from over 120,000 learners. Analyzing collective invideo interaction traces revealed video interaction patterns, one of which is interaction peaks, a burst of playbutton clicks around a point in a video indicating points of interest and confusion for many learners. Figure 1shows one such interaction peak; notice that the peak occurs just before the video transitions visually fromshowing the instructor walking through code and showing the instructor speaking. I identified student activitypatterns that can explain peaks, including playing from the beginning of new material, returning to missedcontent, and replaying a brief segment (Kim et al., 2014b). These analyses have implications for videoauthoring, editing, and interface design, and provide a richer understanding of video learning on MOOCs.Video interface that evolves with dataLectureScape (Kim et al., 2014a; see Figure 2) is an enhanced video player for educational content online,powered by data on learners’ collective video watching behavior. LectureScape dynamically adapts to thousandsICLS 2018 Proceedings1269© ISLSof learners’ interaction patterns to make it easier to rewatch, skim, search, and review. LectureScape introducesa set of data-driven interaction techniques that augment existing video interface widgets: a 2D video timelinewith an embedded visualization of collective navigation traces; dynamic and non-linear timeline dragging; dataenhanced transcript search and keyword summary; automatic display of relevant still frames next to the video;and a visual summary representing points with high learner activity.Active Learnersourcing: Learner prompts contribute to new learning materialsFigure 3. Crowdy: Learnersourcing workflow for summarizing steps in a how-to video.We asked if learners, both an intrinsically motivated and uncompensated crowd, can generate summaries ofindividual steps at scale. This research question resulted in a learnersourcing workflow that periodically promptslearners who are watching the video to answer one of the pre-populated questions, such as “what was the overallgoal of the video section you just watched?” (Figure 3) (Weir et al. 2015). Learners’ answers help generate,evaluate, and proofread subgoal labels, so that future learners can navigate the video with the solution summary.We deployed Crowdy, a live website with the learnersourcing workflow implemented on a set of introductoryweb programming videos. The 25-day deployment attracted more than 1,200 learners who contributed hundredsof subgoal labels and votes. A majority of learner-generated subgoals were comparable in quality to expertgenerated ones, and learners commented that the system helped them grasp the material. A controlledexperiment with 300 crowd workers on Amazon Mechanical Turk showed that participants’ retention ofknowledge in statistics covered in video was higher with Crowdy than with a baseline video interface, andcomparable to seeing expert-generated subgoals.Generating explanations using crowdsourcing and machine learning fordynamic experimentationJoseph Jay WilliamsTo help students learn from solving online problems and receiving feedback, it can be beneficial to provideexplanations for how to solve these problems after student make their attempts. However, instructors havelimited time and resources to generate quality explanations for all the problems they create, and many MOOCsor online resources only provide correct answers. We developed the Adaptive eXplanation Improvement System(AXIS, Williams et al, 2016) to investigate how to crowdsource explanations from learners by promptinglearners to generate self-explanations (Chi et al, 1989; Williams & Lombrozo, 2010). We then used machinelearning to guide a dynamic experiment that discovered which explanations learners rated as being helpful andanalyzed that data in real-time in order to present the highest rated explanations more frequently to futurelearners.Learners attempted to solve four mathematics word problems (in algebra and probability) and wereprovided the correct answer after entering their own. In addition, they would be assigned one of the explanationsfrom the current AXIS pool (the first few learners did not receive any explanations) and asked to rate howhelpful the explanation was for their learning, on a scale from 0 (not at all helpful) to 10 (extremely helpful).They were also prompted to explain in their own words why they thought the answer was correct, as it wouldhelp them learn. If a learner's explanation was longer than 60 characters and the learner rated their explanationas likely to help others, the explanation would be added to the pool to be presented to future students.Assignment of explanations to learners was initially done with equal probability, but as learnersprovided ratings of explanations, higher rated explanations were presented more frequently than lower ratedexplanations, using weighted randomization. More precisely, we used a statistical machine learning algorithm tocalculate the probability that an explanation was higher rated than all the others in the pool (based on a BetaBinomial model & algorithm commonly used to optimize websites, Chappelle & Li, 2013) and used thoseprobabilities to set the weights for randomization (e.g. transitioning from 50/50 to 60/40 to 80/20). The outputICLS 2018 Proceedings1270© ISLSwas therefore a probability distribution over a constantly increasing pool of explanations (as learners generatednew self-explanations). The probability distribution was updated every time a learner rated an explanation, sothat the probability of future learners receiving an explanation was proportion to the evidence that this was thehighest rated explanation.Benefits of explanations for learningTo evaluate whether the explanations that emerged also led to benefits for learning, we conducted an additionalexperiment. Participants were randomly assigned to receive: No explanation (original problems), learnerexplanations from AXIS explanations, learner explanations that AXIS gave low probability to (got low ratings),and as a gold standard, explanations written by an instructional designer. The second experiment recruited 564new participants.Explanations from the system led to improved learning over the default practice, where learners simplysolved problems and received answers. Participants were significantly more likely to solve future problems afterreceiving AXIS explanations, when compared to practicing problems that did not have explanations. A pairwisecomparison within a mixed-effect model revealed a significant increase in accuracy from the initial problems tothe assessment problems (M = 12% versus just 2.7%, SE = 0.027, p < 0.05). It might seem obvious in hindsightthat providing any explanation will increase learning and success on future problems. However, thelearnersourced explanations that AXIS discarded did not provide any learning benefits beyond normal practiceof math problems (M = 2% vs 3%, p = 0.86) and were significantly less beneficial for learning than explanationsdelivered by the AXIS policy (M = 12% vs. 2%, SE = 0.04, p < 0.029). The AXIS explanations also increasedsuccess in solving novel transfer problems that required going beyond the explicit information in theexplanation (differences of 9-12%, SE = 0.03, 0.04, p < 0.01).Finally, there were no significant differences in learning between learnersourced explanations curatedby AXIS, and the explanations written by the instructional designer themself (all ps > 0.30). Overall, thisillustrates how we can rely on self-explanation to make the learnersourcing experience pedagogicallymeaningful for the learner, as well as using machine learning for dynamic experimentation to identify and thenpresent learnersourced content that is helpful for future learners.Crowdsourcing in ASSISTments: PeerASSIST and TeacherASSISTThanaporn Patikorn, Korinn S. Ostrow, Douglas Selent, and Neil T. HeffernanASSISTments is an online learning platform that is being used by over 600 teachers and 50,000 studentsworldwide (Heffernan & Heffernan, 2014 & Heffernan et al., 2016). The platform is built on the idea that itassists students in learning while providing formative assessments to teachers. As such, ASSISTments valuesthe role of both students and teachers in how the platform is used. Three years ago, we ran a large-scalerandomized controlled trial on ASSISTments in the state of Maine. We discovered that one teacher participatingin the study, Mr. Chris LeSiege, wrote tutoring messages for every problem in his textbook so that he couldbetter assist his students when they were doing their homework. After meeting with Mr. LeSiege and discussinghis vast content creation, we realized the potential of crowdsourcing for ASSISTments, including how it couldallow teachers to create and share content, expanding and improving the ASSISTments system, whilestrengthening the relationship between the system and its users as well as amongst users. As a result of thisrevelation, we have developed two features that utilize crowdsourcing: PeerASSIST and TeacherASSIST.PeerASSISTAs its name suggests, PeerASSIST allows students to help their struggling peers by sharing solutions to theirhomework as worked examples. When a student is struggling, PeerASSIST will automatically select and showthem a correct solution submitted by one of their classmates. The goal is not to have higher-performing studentscompleting homework for lower-performing students, but rather to have moments of struggle turn into momentsof learning. A peer’s worked solution might provide the necessary “Aha!” moments that an automated answer orhint provided by ASSISTments may fail to ignite. As with the AXIS system (see above), PeerASSIST is drivenby multi-armed bandit algorithms that aim to select peer work that will maximize the likelihood that a strugglingstudent will correctly answer the next question in their assignment. Teachers can also designate some of theirstudents as “star students,” whose contributions will be more heavily weighted for prioritization inPeerASSIST’s selection process. Over a seven month period, PeerASSIST distributed worked examples forover 250,000 problem instances (from around 12,000 unique problems) to over 1,000 students.ICLS 2018 Proceedings1271© ISLSTeacherASSISTWhile this symposium primarily focuses on learnersourcing, we are also exploring teachersourcing inASSISTments through TeacherASSIST. This feature will allow teachers like Mr. LeSiege to easily createtutoring feedback messages for not only their own problems, but also for problems sourced from textbooks orwritten by the ASSISTments Team. We are currently allowing beta teachers who are testing the system to sharetutoring messages they have created with other teachers. For example, other teachers in Mr. LeSiege’s school ordistrict who use the same textbook and problems will be able to also access the tutoring messages that hecreated for his students. In future versions of TeacherASSIST, we hope to refine our model into a platform thatnot only allows teachers within the same school or district to create and share with each other, but to broadensharing capacities across the United States, and perhaps the world, allowing teachers to communicate and buildupon each other's content and thereby improve student learning. It is our vision for the future thatTeacherASSIST and PeerASSIST will work in parallel to serve as valuable crowdsourcing tactics to strengthenthe feedback available within ASSISTments while simultaneously working to simplify platform usage forteachers and provide robust, collaborative learning opportunities for students.Crowdsourcing content creation in the classroomThomas HillsThis presentation will describe the Propaganda for Change Project, a case study for how crowdsourcing wassuccessfully implemented in an undergraduate psychology course in 2013 with over 100 students (Hills, 2015).The course aimed to teach students about the psychology of persuasion and influence. Rather than taking anapproach of direct instruction, the course was designed with a more learner-centered educational approach. Dueto the large number of students in the course, traditional approaches to learner-centered education where theinstructor would give individual attention to students would be difficult. Instead the course used crowdsourcingto scale the learner-centered approach in two ways by (a) having students find examples of content in the realworld that were of interest to them and that highlighted various aspects of persuasion and influence and to writea blog post about it, and (b) having students create their own video as their final project that would utilize theprinciples of the course to create a persuasive prosocial message. All of the content created by students wasshared on this blog: http://persuasion-and-influence.blogspot.co.uk/. The crowdsourced content had value in twodistinct ways. First, it gave the instructor talking points in class to illustrate the course topics in ways that weremeaningful to the students. By leveraging examples of advertisements that students found relevant to their ownlives, the instructor could bring up pertinent examples that were shared on the blog in class. Additionally, theinstructor could encounter many more examples (and perhaps better ones) that he himself could have curated.To further help the blog become an essential part of the course, the instructor created questions on the finalexam where students had to describe the forms of persuasion used in images taken from the blog. Second, thecontent generated by students is publicly available on the course blog and so it can be read by (a) studentscurrently in the course, (b) future students who can take inspiration from students who took the coursepreviously, and (c) the general public. Indeed, the blog currently has over 700,000 views, with around 400views per day from people around the world. The quality of student blog posts exceeded expectations. Manystudents were thankful (both in person and in evaluations) that the course dealt with real world content and theyfelt it prepared them for job interviews. Having described the course, we will now turn to discussing the types ofcrowdsourcing the course used, and the theoretical principles from psychology and educational theory that canhelp guide them.Found content and produced contentThere are two types of crowdsourced content used in the course: found content and produced content. Askingstudents to find examples of persuasion in their lives and write a blog about it is an example of found content.The concept of found content is similar to that of found objects in art (e.g., Marcel Duchamp’s Fountain). Byhaving students describe found content, students get experiences with detecting ideas from the course in the realworld as well as finding ways to relate the content back to the principles taught in the course by explaining it (tothemselves and to others). Having students create a persuasive video for positive change is an example ofproduced content. This is the type of crowdsourced content that typically comes to mind and that the otherpresentations in this symposium have focused on.Educational theory, cognitive psychology, and content creationICLS 2018 Proceedings1272© ISLSHaving students find and produce content is supported by several educational theories and principles fromcognitive psychology. Having students contribute content to the course builds on the idea of learner-centerededucation. Many learner-centered theories exist in the educational literature that support the idea ofcrowdsourced content generation. For example, we build on the recently developed idea of the student asproducer (Neary and Winn, 2009), which posits that “undergraduate students working in collaboration withacademics to create work of social importance that is full of academic content and value’’ (Neary and Winn,2009). This approach also builds on a central idea in a variety of constructivist theories that students naturallytry to make sense of their experiences by relying on their prior knowledge and experiences (Resnick, 1987;Raskin 2002), so it is our role as instructors to help students make sense of the world around them and learn toparticipate in a community of learners (Duffy and Cunningham, 1996).Having students generate new content is also supported by principles from cognitive psychology(Dunlosky et al., 2013). For example, psychologists have shown a generation effect whereby students betterrecall and remember information they generated themselves rather than information given by others (Slameckaand Graf, 1978). Moreover, it has been shown that asking questions that relate to one’s prior knowledge andexperiences can help support complex learning beyond asking questions related to course content (King, 1994).Having students explain the relationship of found content to the ideas of the course also builds on the idea ofself-explanation, a well-documented method for improving learning by elaborating on content information (Chiet al. 1989; Rittle-Johnson, 2006).Understanding the relevant educational and psychological theories can help realize how to mosteffectively crowdsource the creation of new content, not just for the sake of creating content for others, but,perhaps more importantly, for enhancing the learning experience of the students engaged in generating newcontent.ReferencesAndolina, S., Schneider, H., Chan, J., Klouche, K., Jacucci, G., & Dow, S. (2017). Crowdboard: AugmentingIn-Person Idea Generation with Real-Time Crowds. In Proceedings of the 2017 ACM SIGCHIConference on Creativity and Cognition (pp. 106-118). ACM.Bigham, J. P., Jayant, C., Ji, H., Little, G., Miller, A., Miller, R. C., ... & Yeh, T. (2010). VizWiz: nearly realtime answers to visual questions. In Proceedings of the 23nd Annual ACM Symposium on UserInterface Software and Technology (pp. 333-342). ACM.Chapelle, O., & Li, L. (2011). An empirical evaluation of thompson sampling. In Advances in NeuralInformation Processing Systems (pp. 2249-2257).Chi, M. T., Bassok, M., Lewis, M. W., Reimann, P., & Glaser, R. (1989). Self‐explanations: How students studyand use examples in learning to solve problems. Cognitive Science, 13(2), 145-182.Duffy, T. & Cunningham, D. (1996). Constructivism: Implications for the design and delivery of instruction.Handbook of Research for Educational Communications and Technology, 51, 170-198.Dunlosky, J., Rawson, K. A., Marsh, E. J., Nathan, M. J., & Willingham, D. T. (2013). Improving students’learning with effective learning techniques promising directions from cognitive and educationalpsychology. Psychological Science in the Public Interest, 14(1), 4–58.Gaikwad, S. N., Morina, D., Nistala, R., Agarwal, M., Cossette, A., Bhanu, R., ... & Mithal, A. (2015). Daemo:A self-governed crowdsourcing marketplace. In Adjunct Proceedings of the 28th Annual ACMSymposium on User Interface Software & Technology (pp. 101-102). ACM.Heffernan, N. T., & Heffernan, C. L. (2014). The ASSISTments ecosystem: building a platform that bringsscientists and teachers together for minimally invasive research on human learning and teaching.International Journal of Artificial Intelligence in Education, 24(4), 470-497.Heffernan, N. T., Ostrow, K. S., Kelly, K., Selent, D., Van Inwegen, E. G., Xiong, X., & Williams, J. J. (2016).The Future of Adaptive Learning: Does the Crowd Hold the Key?. International Journal of ArtificialIntelligence in Education, 26(2), 615-644.Hills, T. T. (2015). Crowdsourcing content creation in the classroom. Journal of Computing in HigherEducation, 27(1), 47-67.Kim, J. (2015). Learnersourcing: Improving Learning with Collective Learner Activity (Doctoral dissertation,Massachusetts Institute of Technology).Kim, J., Guo, P. J., Cai, C. J., Li, S. W. D., Gajos, K. Z., & Miller, R. C. (2014a). Data-driven interactiontechniques for improving navigation of educational videos. In Proceedings of the 27th Annual ACMSymposium on User Interface Software and Technology (pp. 563-572). ACM.ICLS 2018 Proceedings1273© ISLSKim, J., Guo, P. J., Seaton, D. T., Mitros, P., Gajos, K. Z., & Miller, R. C. (2014b). Understanding in-videodropouts and interaction peaks in online lecture videos. In Proceedings of the First ACM Conferenceon Learning @ Scale Conference (pp. 31-40). ACM.Kim, J., Nguyen, P. T., Weir, S., Guo, P. J., Miller, R. C., & Gajos, K. Z. (2014c). Crowdsourcing step-by-stepinformation extraction to enhance existing how-to videos. In Proceedings of the 32nd Annual ACMConference on Human Factors in Computing Systems (pp. 4017-4026). ACM.King, A. (1994). Guiding knowledge construction in the classroom: Effects of teaching children how to questionand how to explain. American Educational Research Journal, 31(2), 338-368.Kittur, A., Nickerson, J. V., Bernstein, M., Gerber, E., Shaw, A., Zimmerman, J., ... & Horton, J. (2013). Thefuture of crowd work. In Proceedings of the 2013 Conference on Computer Supported CooperativeWork (pp. 1301-1318). ACM.Lasecki, W. S., Kushalnagar, R., & Bigham, J. P. (2014). Legion scribe: real-time captioning by non-experts. InProceedings of the 16th International ACM SIGACCESS Conference on Computers & Accessibility(pp. 303-304). ACM.Law, E., & von Ahn, L. (2011). Human Computation. Morgan & Claypool Publishers.Neary, M., & Winn, J. (2009). The student as producer: reinventing the student experience in higher education.The Future of Higher Education: Policy, Pedagogy and the Student Experience (pp. 192-210).Continuum.Paulin, D., & Haythornthwaite, C. (2016). Crowdsourcing the curriculum: Redefining e-learning practicesthrough peer-generated approaches. The Information Society, 32(2), 130-142.Raskin, J. D. (2002). Constructivism in psychology: Personal construct psychology, radical constructivism, andsocial constructionism. In Studies in Meaning: Exploring Constructivist Psychology (pp. 1-25). PaceUniversity Press.Resnick, L. B. (1987). Education and learning to think. National Academy Press.Rittle-Johnson, B. (2006). Promoting transfer: Effects of self‐explanation and direct instruction. ChildDevelopment, 77(1), 1-15.Salehi, N., Irani, L. C., Bernstein, M. S., Alkhatib, A., Ogbe, E., & Milland, K. (2015). We Are Dynamo:Overcoming stalling and friction in collective action for crowd workers. In Proceedings of the 33rdAnnual ACM Conference on Human Factors in Computing Systems (pp. 1621-1630). ACM.Saxton, G. D., Oh, O., & Kishore, R. (2013). Rules of crowdsourcing: Models, issues, and systems of control.Information Systems Management, 30(1), 2-20.Slamecka, N. J., & Graf, P. (1978). The generation effect: Delineation of a phenomenon. Journal ofExperimental Psychology: Human Learning and Memory, 4(6), 592.Weir, S., Kim, J., Gajos, K. Z., & Miller, R. C. (2015). Learnersourcing subgoal labels for how-to videos. InProceedings of the 18th ACM Conference on Computer Supported Cooperative Work & SocialComputing (pp. 405-416). ACM.Weld, D. S., Adar, E., Chilton, L., Hoffmann, R., Horvitz, E., Koch, M., ... & Mausam, M. (2012). Personalizedonline education—a crowdsourcing challenge. In Workshops at the Twenty-Sixth AAAI Conference onArtificial Intelligence (pp. 1-31).Williams, J. J., & Lombrozo, T. (2010). The role of explanation in discovery and generalization: Evidence fromcategory learning. Cognitive Science, 34(5), 776-806.Williams, J. J., Kim, J., Rafferty, A., Maldonado, S., Gajos, K. Z., Lasecki, W. S., & Heffernan, N. (2016).AXIS: Generating explanations at scale with learnersourcing and machine learning. In Proceedings ofthe Third (2016) ACM Conference on Learning @ Scale (pp. 379-388). ACM.ICLS 2018 Proceedings1274© ISLS