The Learning Sciences @ Scale:Current Developments in Open Online LearningJames D. Slotta (co-chair), University of Toronto, jslotta@gmail.comDaniel Hickey (co-chair), University of Indiana, dthickey@indiana.eduCarolyn P. Rosé, Carnegie Mellon University, cprose@cs.cmu.eduPierre Dillenbourg, EPFL, pierre.dillenbourg@epfl.chHedieh Najafi, University of Toronto, hnajafi@gmail.comStian Håklev, University of Toronto, shaklev@gmail.comSuraj Uttamchandani, Indiana University, suttamch@indiana.eduJoshua Quick, Indiana University, jdquick@indiana.eduAbstract: The explosive growth of MOOCs has generated immense interest in online learningin massive courses. However, much of this frenetic activity has emphasized technology andscalability (Reich, 2015), resulting in rudimentary learning experiences (i.e., brief streamingvideos followed by quizzes or online discussion forums). This symposium will showcase anddiscuss four diverse efforts to advance open learning at scale that are directly informed bycontemporary theories of learning and educational research methods. This includes research oninquiry-based community learning, computer-mediated discourse analysis, participatoryapproaches to learning and assessment, and evidence-rich digital-credentials. In each of the fourcases, the desire to extend that program of research to learning at scale resulted in significantadvances in the more general program of research. In this way the symposium explores effortsto foster learning at scale might advance our theories of learning and our principles and methodsfor learning design more generally.Keywords: learning design, online learning, informal learning, learning communitiesIntroductionThe explosive growth of MOOCs, which have not tended to include complex instructional designs (Reich, 2015),has sparked the interest of researchers in the learning sciences. These innovators see the opportunity forinvestigating new modes of learning, including new forms of socially mediated materials and activities and newpedagogical affordances for learning at scale (e.g., through learning analytics and educational data mining). Thissymposium will present a set of papers that describe research of such new opportunities. We recognize the uniqueaspects of MOOCs where participants arrive with altruistic learning goals, situated within diverse contexts – oftenfrom around the world, in all time zones and a wide range of settings – with legitimate interest in engaging withpeers and participating in the learning designs. Many of the elements of MOOCS are quite challenging, such asthe asynchronous, distributed aspects of participation, the high variation amongst learners in terms of theirbackground and available time, and the lowest common denominator of end user technologies. However, somefeatures have captured our attention, and offered new opportunities for research in our design of pedagogicalscripts, collaborative learning environments, and interactive materials. In some cases, this work could potentiallyreturn new theoretical insights and methodological capabilities for the wider learning sciences community. Wewill discuss the implications of our work, the likely trajectories of MOOCS in the coming years, and way that thisnascent community within ISLS can help advanceGrowing pains and a push for greater interactivityThe earliest MOOCS, known as “cMOOCs” were based on a contemporary view of learning called connectivism(Siemens, 2005) and aimed to support robust peer interaction and networked knowledge construction. However,it has been the subsequent form, known as “xMOOCs” that formed the heart of the rapid expansion of courseofferings. Variously attributed to “eXtended” or “eXtension,” the online courses at edX, Udacity, Coursera, andothers, these courses generally feature streaming videos, online readings, problem sets and quizzes, and peerdiscussion forums. Once developed, most courses could be offered to new cohorts of learners for very modestcosts, and sometimes simply left online for any and all to complete at their own pace. This general model hasexpanded to many other platforms and sectors and has become quite pervasive, even in smaller more exclusivecontexts such as corporate training and for learning towards specialized certifications.Amidst the “hype and hyperbole” over MOOCs, several certainties emerged by 2012, which the NewYork Times dubbed “the Year of the MOOC” (Pappano, 2012). In addition to the aforementioned massiveexpansion of opportunities, other certainties were that MOOCs were generating extensive innovation in onlineICLS 2016 Proceedings1114© ISLSlearning more broadly, and that MOOCs were causing many observers to reconsider the design and (particularly)the cost of existing models higher education. Another certainty that emerged around this time is that most MOOCsfeatured much less interaction than is typical of face-to-face courses. Some observers had already commented onthe difficulty of connecting with other learners in the cMOOCs (Mackness, Mak, & Williams, 2010) It turned outthat supporting social interaction in the cMOOCs was proving much harder. An effort to include more interactionand group projects in a Coursera course on online learning was widely cited for going “laughably awry.” (Oremus,2013). One study found that engagement in Coursera discussion forums declined significantly over time amongcompleters, and that instructor involvement actually worsened participation; A consensus emerged that mostMOOC discussion forums suffered from sharp declines in interaction as courses got underway, and that this wasmostly likely due to “information overload” as discussion threads become unnavigable and veered off topic(Brinton, 2013). This relative lack of social interaction was one of the most oft-cited concerns in the “backlash”against MOOCs one year later (Kolowich, 2013).MOOC proponents generally responded that the social experience in typical MOOCs was actually quitesimilar to what many students experience in the large lecture courses that are typical for introductory courses inmany college and universities. Indeed, the peer discussion forums available for many MOOCs were similar inways to the informal study sessions that many students organized themselves into for conventional lecture courses.Furthermore, some discussion forums were moderated by knowledgeable volunteers and even sometimes by paidteaching assistants. Regardless of the reality in 2013, significant effort was already underway among MOOCinnovators and researchers to study social learning and more systematically support more and more productivepeer interaction. Some of this research was more naturalistic research continuing in the tradition of the cMOOCs.A team at Stanford was developing a MOOC platform (now called NovoEd) which is explicitly based on sociallearning theory (Ronaghi, Saberi, & Trumbore, 2015). A major program of research at the Open Universityresulted in the FutureLearn which supports “discussion-in-context” and “community-supported learning” indozens of free open online courses (Parr, 2013). A particularly promising related strand emerged around scaledup efforts to support peer assessment of extended student work (Kulkarni, Socher, Bernstein, & Klemmer, 2014).Structure of the sessionOther promising efforts to study and support social learning at such a scale have drawn on the insights from theLearning Sciences and Computer Supported Collaborative Learning communities. This symposium features foursuch efforts from learning science science research groups, organized as a set of four synthesized paperpresentations with a panel format for the discussion. Each presenter will (a) address the prior theories and researchthat is informing their work, (b) show how those theories are instantiated in new learning features, (c) demonstratethe forms of engagement supported by those features, (d) summarize the evidence showing the impact of thesefeatures for students, (e) articulate design principles for supporting learning at scale, and (f) highlight currentchallenges and near-term goals for continued refinement and research. Following the presentations, each of thefour presenters will pose one question of another presenter, followed by a wider panel commentary on thatquestion. At the end of each panel discussion, there will an opportunity for audience questioning, moderated bythe chair.Supporting reflection and collaboration in a MOOC for in-service teachersHedieh Najafi, James D. Slotta, Stian Håklev, Renato Carvalho, and Rosemary Evans, University of Toronto“Teaching with technology and inquiry” (INQ101x) was a six-week MOOC designed for in-service teachersinterested in learning how to integrate technology and inquiry into their own practice. The course was co-led bya professor and a school principal, and showcased the viewpoints of school administrators and classroom teachers.INQ101x applied Knowledge Community and Inquiry model (KCI; e.g., Slotta, & Najafi, 2012) to alarge-scale context with more than 8000 registered participants. KCI informed our design of a “script” whereparticipants created and applied a collection of annotated resources for teaching with technology and inquiry, anda subset of participants opted to collaboratively design a lesson plan, working in small groups and receivingfeedback from the wider community. Two live events in the last week of the course allowed learners to discusstheir questions with the course instructors and with master teachers who had contributed to INQ101x. To fosterin-depth discussions among learners, we used learners’ professional background to create 10 Special InterestGroups (SIG). Learners chose their SIGs after completing a pre-course survey, a mandatory step to join designgroups and SIG specific discussions. Of all registrants, 2008 learners completed the survey, 357 learners joineddesign groups, and 120 active design groups were formed. More than one thousand annotated resources weresubmitted to the resource collection.ICLS 2016 Proceedings1115© ISLSGiven the high number of learners enrolled in INQ101x, providing personal feedback to learners wasimpractical. Thus, reflection prompts and shared reflective notes were used as a means to promoting deepreflection about course content. Two types of reflection notes were integrated in INQ101x: individual privatereflection notes and public reflection notes submitted to course discussion forums and discussed with peers. Anexample of a public reflection note was: “Lets talk about the pragmatics of student-contributed content andcollective inquiry. When we assign students to create, curate, re-mix and apply ideas and observations, how canwe make sure that every student gains the benefits: creating and contributing resources, engaging in productiveexchanges with peers, and drawing upon the collective resources?”Creating opportunities to foster reflection is integral to teacher education and professional developmentprograms, to help teachers apply new concepts and approaches to their practice, and build new forms ofpractitioner knowledge (Madeira & Slotta, 2012; Pavlovich, 2007; Spalding, & Wilson, 2002). Reflective notes,shared or private, prompted or non-prompted, are used to encourage teacher reflection (Chitpin, 2006). Lee (2010)argues that sharing reflections with peers can help teachers to improve the depth of their reflections. Blomberg etal. (2014) identify three levels of teacher reflection, with increasing levels of sophistication: description,evaluation, and integration.We examine the impact of participation in such reflections on INQ101x learners’ knowledge of teachingwith technology and inquiry. We address the following research questions: (1) How do personal reflection notesevolve over the six weeks of the course? and (2) How does peer feedback received through public reflection notescontribute to progress in teacher understandings over the six weeks of the course? We adapted an existing rubricfor assessing the quality of personal and public reflection notes in INQ101x (Hatton, & Smith, 1995; Moon, 2013)as: non-reflective, descriptive reflection, dialogic reflection, and critical reflection. Reflection notes of scienceand math teachers were included in the data set. We apply our findings to create a set of principles that can guidethe design of effective reflection and discussion prompts for large online courses.Envisioning support of social learning in MOOCsCarolyn P. Rosé, Carnegie Mellon UniversityData from Massive Open Online Courses (MOOCs) offer evidence of the association between types ofconversational interactions and retention (Wen et al., 2014a; Wen et al., 2014b; Wen et al., 2015), team projectquality (Yang et al., 2015), and learning (Wang et al., 2015) in the environment. These insights inform design ofinterventions to support improved outcomes (Howley et al., 2015; Ferschke et al., 2015a; Ferschke et al., 2015b).This work represents a series of investigations related to the broad vision of designing and building out affordancesfor collaborative learning in MOOCs through DANCE[1].If we can leverage the rich potential source of support in the plentiful student population in MOOCs, wemay be able to substantially reduce attrition and meet instructional goals better at the same time. The area ofautomatic collaborative process analysis has focused on discussion processes associated with knowledgeintegration. Frameworks for analysis of group knowledge building are plentiful and include examples such asTransactivity (Berkowitz & Gibbs, 1983; Teasley, 1997; Weinberger & Fischer 2006), Inter-subjective MeaningMaking (Suthers, 2006), and Productive Agency (Schwartz, 1998). These discussion processes are theorized tooccur when students adopt an orientation towards one another in which they are most likely to experiencecognitive conflict and learning (de Lisi & Golbeck, 1999). Automated analysis technology (Rosé et al., 2008)enables triggering support for these types of interactions in an automated way (Adamson et al., 2014).MOOCs are not unique in their pattern of exponential attrition over time. Instead, the same pattern isevident in all forms of online communities. Social support exchanged through discussion forums is known to beassociated with increases in commitment and corresponding reductions in attrition in online communities (Wang,Kraut, & Levine, 2012). Findings from our own MOOC deployment in Fall 2014 provides evidence that theexperience of a synchronous collaborative chat in the midst of MOOC participation reduces attrition at the timepoint of the experience by more than a factor of two (Ferschke et al., 2015b).Our early intervention was designed for short, periodic collaborative exchanges. More recently we havebeen working towards more persistent social interaction throughout a course in the form of team based projects.Our analysis of data from two team based MOOCs suggests that the success of teams in state-of-the-art teambased MOOCs is low (Wen et al., 2015; Yang et al., 2015). While the behavior of team leaders, and to a lesserextent that of other team members, predict team outcomes, the evidence points to the conclusion that the problemstarts even before the teams begin to function in that capacity. In particular, the team formation process itself mustbe improved in order to produce teams that are positioned for success at the start. We propose a deliberation-basedteam formation procedure to improve the selection and initiation process leveraging the same discussion processesassociated with enhanced learning. What that means is that a pretask is assigned to students to do individuallyICLS 2016 Proceedings1116© ISLSand then post to a public discussion forum for feedback from other students in the class. Students are required toselect a small number of students to provide feedback to in this context. An automated process analysis tool isthen used to make an assessment about the number of transactive contributions exchanged between each pair ofstudents in this context. A constraint satisfaction algorithm is then used to assign students to teams in such a waythat the average pairwise observed exchange of transactivity from the discussion forum activity between studentsassigned to the same team is maximized across the student population. Results from pilot investigation in MTurksuggest strong effects both of deliberation pretask with feedback from fellow-students and team selection basedon automatically detected transactivity in during this pretask discussion.Scaling up participatory approaches to learning and assessment in opencoursesDaniel T. Hickey, Suraj Uttamchandani, and Joshua Quick, Indiana UniversityThis paper argues for a gradual iterative response to pressures to scale up learning, so that technology can respondto rather than constrain theoretical advance. This research embraces situated and participatory perspectives onlearning (Greeno et al., 1998) and assessment (e.g., Moss, et al., 2008, Hickey, 2015). A prior program of designbased research resulted in a core set of design principles, local theories, and specific practices for fostering broadlearning outcomes and measuring those outcomes in technology-supported learning environments. By aligninginformal, semi-formal, and formal assessment, these efforts have delivered very high levels of socio-technologicalengagement with disciplinary knowledge (as in Engle & Conant, 2002), while leaving behind dramaticallyenhanced understanding and significantly enhanced achievement (Hickey & Zuiker, 2012)The data for the most recent cycle of research comes from an online graduate course on EducationalAssessment. A conventional version of the course in both Sakai and Google Sites was refined over several yearsin order to overcome the constraints of these platforms on participatory learning (Hickey & Rehak, 2013). Withthe support of a grant from Google, the resulting design principles were further refined in three annual “big openonline courses” (“BOOCs”) using in an extensively customized version of Google Coursebuilder. The first coursestarted with hundreds of learners, including a subset of students taking the course for credit. Each cycle has furtherautomated key features, including personally contextualized registration and participation, assignment tonetworking groups, personalized “wikifolio” open assignments, anchored peer commenting, contextualizedanalytics and feedback, peer endorsement and promotion, and open digital badges for completion, leadership, andadvanced work.This research has led to further refinement of the course design principles, which are now as follows: (1)Use public contexts to give meaning to knowledge tools; (2) publically recognize and reward productive forms ofdisciplinary engagement; (3) assess student generated artifacts through local reflections, (4) help learners selfassess their understanding privately, and (5) measure aggregated achievement discreetly.Analyses from the most recently completed Assessment BOOC revealed levels of persistencecomparable to others MOOCs: 11% of the 179 registrants and 29% of those who completed the first assignmentcompleted the course. But this analysis revealed dramatically higher levels of individual and social engagementthan most MOOCs support. Weekly wikifolios averaged 2,820 words for credential students and 1,377 words foropen completers; credential students averaged 4.2 comments per week and 337 words per comment while openstudents averaged 3.7 comments per week and 302 words per comment. Coding of the comments revealed thataround 90% of the comments were disciplinary (because they referenced the topic of the assignment), while 25%were contextualized (because they referenced a specific practice context). We also obtained satisfactory levels ofachievement on a timed exam consisting of challenging multiple-choice items (averaging 80% for credentialstudents around 78% for open students).This research is significant because it has resulted in streamlined features to support participatorylearning at scale. With modest additional work, these features can be shared broadly as open source modules thatcan be easily integrated into other platforms via Learning Technologies Interoperability (LTI) standards.Orchestration graphs: How to scale up rich pedagogical scenariosPierre Dillenbourg, EPFL, SwitzerlandThe goal of orchestration graphs is to describe how rich learning activities, often designed for small classes, canbe scaled up to thousands of participants, as in MOOCs (Dillenbourg, 2015). A sequence of learning activities ismodeled as a graph with specific properties. The vertices or nodes of the graph are the learning activities. Learnersperform some of these activities individually, some in teams and other ones with the whole class. The graph hasa geometric nature, time being represented horizontally and the social organization (individual, teams, class)ICLS 2016 Proceedings1117© ISLSvertically. These activities can be inspired by heterogeneous learning theories: a graph models the integration ofheterogeneous activities into a coherent pedagogical scenario.The edges of the graph serve to connect activities, representing the two-fold relationship betweenactivities: how they relate to each other from a pedagogical and from an operational viewpoint. Fromthe operational viewpoint, edges are associated with operators that transform the data structures produced duringa learning activity into the data structures needed to run the next activity. From the pedagogical viewpoint, anedge describes why an activity is necessary for the next activity: it can, for instance, be a cognitive pre-requisite,a motivational trick, an advanced organizer or an organizational constraint.The extent to which one activity is necessary for the next one is encompassed in the weight of an edge.The transition between two activities is stored as a matrix: the cell (m,n) of a transition matrix stores theprobability that a learner in cognitive state m will evolve to state n in the next activity. This transition matrix canbe summarized in the form of a parameter that constitutes the edge weight: an edge between two activities has aheavy weight if the learner performance in an activity is very predictive of his success of the connected activity.The graph also constitutes a probabilistic network that allows predicting the future state of a learner. Anorchestration graph describes how the scenario can be modified, stretched, cut, extended.This presentation will begin with a review of the orchestration graph approach, illustrating the applicationof such an approach in several activity designs. Following, the sequence of activities from one recent MOOC willbe presented in terms of orchestration graph, revealing transitions between “orchestrational layers” and suggestingnew opportunities for learning analytics.ReferencesAdamson, D., Dyke, G., Jang, H. J., Rosé, C. P. (2014). Towards an Agile Approach to Adapting DynamicCollaboration Support to Student Needs, International Journal of AI in Education 24(1), 91-121.Berkowitz, M., & Gibbs, J. (1983). Measuring the developmental features of moral discussion. Merrill-PalmerQuarterly, 29, 399-410.Bielaczyc, K. (2006). Designing social infrastructure: The challenge of building computer-supported learningcommunities. The Journal of the Learning Sciences, 15(3), 301-329.Blomberg, G., Sherin, M. G., Renkl, A., Glogger, I., & Seidel, T. (2014). Understanding video as a tool for teachereducation: investigating instructional strategies to promote reflection. Instructional Science, 42, 443-463.Brinton, C. G., Chiang, M., Jain, S., Lam, H. K., Liu, Z., & Wong, F. M. F. (2014). Learning about social learningin MOOCs: From statistical analysis to generative model. Learning Technologies, IEEE Transactionson, 7(4), 346-359.Chitpin, S. (2006). The use of reflective journal keeping in a teacher education program: A Popperian analysis.Reflective Practice 7(1), pp. 73-86.de Lisi, R., & Golbeck, S.L. (1999). Implications of the Piagetian Theory for peer learning. Cognitive perspectiveson peer learning, 3-37.Dillenbourg, P. (2015) Orchestration graphs. EPFL Press.Engle, R. A., & Conant, F. R. (2002). Guiding principles for fostering productive disciplinary engagement:Explaining an emergent argument in a community of learners classroom. Cognition and Instruction,20(4), 399-483.Ferschke, O., Howley, I., Tomar, G., Yang, D., Rosé, C. P. (2015a). Fostering Discussion across CommunicationMedia in Massive Open Online Courses. Proceedings of Computer Supported Collaborative Learning.Ferschke, O., Yang, D., Tomar, G., Rosé, C. P. (2015b). Positive Impact of Collaborative Chat Participation in anedX MOOC. Proceedings of AI in Education.Hatton, N., & Smith, D. (1995). Reflection in teacher education: Towards definition and implementation.Teaching and teacher education, 11(1), 33-49.Howley, I., Tomar, G., Yang, D., Ferschke, O. and Rosé, C. P. (2015) Expectancy Value Theory of Help SeekingApplied to Features in MOOCs, Proceedings of AI in Education.Hickey, D. T. (2015). A situative response to the conundrum of formative assessment. Assessment in Education:Principles, Policy & Practice, 22(2), 202–223.Hickey, D., & Rehak, A. (2013). Wikifolios and participatory assessment for engagement, understanding, andachievement in online courses. Journal of Educational Multimedia and Hypermedia, 22(4), 407–441.Hickey, D. T., & Zuiker, S. J. (2012). Multilevel assessment for discourse, understanding, and achievement.Journal of the Learning Sciences, 21(4), 522–582.Greeno, J. G. (1998). The situativity of knowing, learning, and research. American Psychologist, 53(1), 5.Kolowich, S. (2013, May 1). Faculty backlash grows against online partners. Chronicle of Higher Education.Kulkarni, C. E., Socher, R., Bernstein, M. S., & Klemmer, S. R. (2014, March). Scaling short-answer grading byICLS 2016 Proceedings1118© ISLScombining peer assessment with algorithmic scoring. In Proceedings of the first ACM conference on Learning@scale conference (pp. 99-108). ACM.Lee, O. (2010). Facilitating preservice teachers' reflection through interactive online journal writing. PhysicalEducator, 67(3), 128.Mackness, J., Mak, S., & Wiliams, R. (2010). The ideals and reality of participating in a MOOC. In Proceedingsof the 7th International Conference on Networked Learning 2010, edited by Lone Dircknick-Holmfeld,Vivien Hodgson, Chris Jones, Maarten de Laat, David McConnell, and Thomas Ryberg, 266-275.Lancaster: Lancaster University, 2010.Madeira, C. & Slotta, J.D. (2012). Teacher Paradigm Shifts for 21st Practice Skills: The Role of ScaffoldedReflection Within A Peer Community. Proceedings of the Tenth International Conference of theLearning Sciences. Sydney. Volume 1: 227 – 234. International Society of the Learning Sciences (ISLS).Moon, J. A. (2013). Reflection in learning and professional development: Theory and practice. Routledge.Moss, P. A., Pullin, D. C., Gee, J. P., Haertel, E. H., & Young, L. J. (2008). Assessment, equity, and opportunityto learn. Cambridge, MA: Cambridge University Press.Moon, J. A. (2013). Reflection in learning and professional development: Theory and practice. UK: Routledge.Oremus, W. (2013, February 5). Online class on how to teach online goes laughably awry. Slate MagazinePappano, L. (2012, Nov. 2). Year of the MOOC, New York Times. Retrieved from Retrieved fromhttp://www.nytimes.com/2012/11/04/education/edlife/massive-open-online-courses-are-multiplying-ata-rapid-pace.html?pagewanted=all&_r=0Parr, C. (2013). FutureLearn reveals big plans to deliver MOOCs on the move. Times Higher Education, 23, 9.Pavlovich, K. (2007). The development of reflective practice through student journals. Higher EducationResearch and Development 26(3), 281-295.Reich, J., & others. (2015). Rebooting MOOC research. Science, 347(6217), 34–35.Ronaghi, F., Saberi, A., & Trumbore, A. (2014). NovoEd, a social learning environment. In Paul Kim (eEd.),Massive Open Online Courses: The MOOC Revolution (96-105). New York: Routledge.Rosé, C. P., Wang, Y.C., Cui, Y., Arguello, J., Stegmann, K., Weinberger, A., Fischer, F., (2008). AnalyzingCollaborative Learning Processes Automatically: Exploiting the Advances of Computational Linguisticsin Computer-Supported Collaborative Learning, submitted to the International Journal of ComputerSupported Collaborative Learning 3(3), 237-271.Schwartz, D. (1998). The productive agency that drives collaborative learning. In Dillenbourg, P. (Ed.)Collaborative learning: Cognitive and computational approaches, Emrald Group Publishing.Siemens, G. (2005). Connectivism: A learning theory for the digital age. Instructional Technology and DistanceEducation, 2(1), 3–10.Slotta, J. D., & Najafi, H. (2012). Technology-Enhanced Learning Environments for Science Inquiry. InEncyclopedia of the Sciences of Learning (pp. 3287-3295). Springer US.Spalding, E. & Wilson, A. (2002). Demystifying reflection: A Study of pedagogical strategies that encouragereflective journal writing. Teachers College Record, 104(7), 1393-1421.Suthers, D. (2006). Technology affordances for inter-subjective meaning making: A research agenda for CSCL.International Journal of Computer Supported Collaborative Learning, 1, 315-337.Teasley, S. D. (1997). Talking about reasoning: How important is the peer in peer collaborations? In L. B. Resnick,C. Pontecorvo, & R. Saljo (Eds.), Discourse, tools, and reasoning: Situated cognition andtechnologically supported environments. Heidelberg, Germany: Springer-Verlag.Weinberger A., Fischer F. (2006). A framework to analyze argumentative knowledge construction in computersupported collaborative learning. Computers & Education, 46, 71–95.Wang, Y.-C., Kraut, R. E., & Levine, J. M. (2012). To Stay or Leave? The Relationship of Emotional andInformational Support to Commitment in Online Health Support Groups. In Proceedings of the ACMConference on Computer-Supported Cooperative Work (CSCW'2012). New York: ACM Press.Wang, X.,, Yang, D., Wen, M., Koedinger, K. R., & Rosé, C. P. (2015). How does student’s cognitive behaviorin MOOC Discussion Forums affect Learning. Proceedings of Educational Data Mining.Wen, M., Yang, D., Rosé, D. (2014a). Linguistic Reflections of Student Engagement in Massive Open OnlineCourses. In Proceedings of the International Conference on Weblogs and Social Media.Wen, M., Yang, D., & Rosé, C. P. (2014b). Sentiment Analysis in MOOC Discussion Forums: What does it tellus? Proceedings of Educational Data Mining.Wen, M., Yang, D., and Rosé, C. P. (2015). Virtual Teams in Massive Open Online Courses. In Proceedings ofAI in Education.Yang, D., Wen., M., Rosé, C. P. (2015). Weakly Supervised Role Identification in Teamwork Interactions.Proceedings of the Association for Computational LinguisticsICLS 2016 Proceedings1119© ISLS