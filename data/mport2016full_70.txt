Competency-Based Digital Badges and Credentials:Cautions and Potential Solutions From the FieldDaniel T. Hickey, Indiana University, dthickey@indiana.eduAbstract: This paper explores tensions that repeatedly surface alongside significant advancesin instructional technology. One such recent advance concerns open digital badges that containspecific claims of competency and detailed evidence supporting those claims. Some assume thatbadges should only contain claims and evidence concerning specific measured competencies,while dismissing badges for participating in courses or activities that lack such evidence as“attendance badges.” Others assume awarding badges only for measured competencies bypassesvery important forms of social and inquiry-oriented learning, ignores the limitations ofassessment and measurement, and limits the transformative potential of web-enabled evidencerich credentials. An extended study of the 29 projects in the 2012 Badges for Lifelong Learninginitiative provided a unique opportunity to explore this enduring debate. None of six efforts tocreate competency-based open badge systems resulting in thriving ecosystems..Keywords: digital badges, metadata, competency-based education, constructivismIntroductionEach wave of advances in educational computing brings to the surface an enduring debate between two verydifferent views of learning. Shortly after the graphical computer interface was introduced in 1961, the PLATOsystem was introduced at the University of Illinois. PLATO was primarily used to deliver programmed instructionin the behaviorist mastery-learning model that was widely accepted in era (e.g., Anderson, Kulhavy, & Andre,1971). Six years after PLATO was introduced, Papert and colleagues at Bolt, Beranek and Newman introducedthe LOGO programming language (Niemiec & Walberg, 1989). LOGO was based on the cognitive developmentaltheories that were just coming to light among western educators and psychologists, and was intended to allowlearners to discover programming concepts such as recursion and develop critical thinking skills. In Papert’svision, “…the child programs the computer and, in doing so, both acquires a sense of mastery over a piece of themost modern and powerful technology and establishes an intimate contact with some of the deepest ideas fromscience, from mathematics, and from the art of intellectual model building" (1980, p. 5).In practice, the divide between these approaches to educational computing was not always so clear, andmany innovators resisted being essentialized. For example, Alpert and Bitzer (1970) insisted that PLATO was notlimited to programmed instruction in basic skills but was also fostering inquiry and critical thinking. Nonetheless,moving forward into the personal computer era, publishers, parents, learners, and librarians had the choicebetween drill and practice programs like MathBlaster (Eckert & Davidson, 1987) and more discovery-orientedprograms like the Logical Journey of the Zoombinis (Brøderbund, 1996) which was designed to support logicalreasoning and critical thinking. While this tension simmered as online and hybrid instruction became morecommon (see Koszalka & Ganesan, 2004), it leapt to prominence with the emergence of massive open onlinecourses (MOOCs). The initial “cMOOCs” emphasized knowledge construction and connections in emerging“connectivist” models of learning (Siemens, 2005). But the later “xMOOCs” like edX and Coursera featuredstreaming videos, structured practice problems, and multiple choice tests. The very scalable instructional modeland limited peer interaction in xMOOCs allowed for the dramatic expansion of free online learning opportunities,leading the New York Times to dub 2012 “the year of the MOOC.” However, the static instructional model wasa most frequently cited concerns in the “backlash” that followed (Kolowich, 2013).This tension can be traced back to two very different views of cognition. PLATO, MathBlaster, andxMOOCs all assume disciplinary knowledge can be readily broken down into smaller associations that readily“reassemble” into more sophisticated higher order knowledge; such assumptions support instruction thatefficiently trains and tests these specific associations. While some label such approaches as “behaviorist,” this isusually not appropriate. Many modern information processing models of human cognition also focus on specificcognitive associations as well. Hence, a more appropriate label for these approaches is “associationist.”Conversely, LOGO, Zoombini, and cMOOCs assume knowledge primarily consists of higher order conceptual“schema” that differ from learner to learner and are constructed via inquiry and problem solving. This calls forexploration and discovery that helps learners construct new knowledge. While this perspective is often labeled“cognitivist,” this is also generally inappropriate; the label “constructivist” or Papert’s term “constructionist” moreaccurately capture the focus on having students construct new conceptual knowledge; the term “connectivist”ICLS 2016 Proceedings538© ISLScaptures the additional assumption in the cMOOCs that learners are doing so by digitally connecting with others.While these assumptions are tacit among many educators, the distinction between associationist and constructivistperspectives is widely appreciated in the Learning Sciences and among educational researchers more generally(e.g., Greeno, Collins, & Resnick, 1997) and has been the subject of vigorous debate (e.g., Kirschner, Sweller, &Clark, 2006 vs. Hmelo-Silver, Duncan, & Chinn, 2006; Tobias & Duffy, 2009)Associationism vs. constructivism in new learning technologiesThese tensions are taking on new importance as they surface within three related developments. The first isrenewed interest in competency-based education (CBE). One proponent described CBE as “having a curriculumstructured to demonstrate learning in clearly articulated competencies, is often self-paced, is agnostic as to thesource of learning while maintaining clear and transparent learning standards” (Leuba, 2015, p. 1). Whileeschewing the radically reductionist approach of earlier mastery learning models (e.g., Guskey & Gates, 1985)with a focus on “authentic” and “real-life” demonstrations and projects, modern CBE models still focus onindividualized mastery and demonstration of specific knowledge and skills, as opposed to completing activitiesor courses with other learners. Proponents of CBE put it in opposition with the prevailing credit-hour models,deriding the latter as “’time-served’ rather than learning achieved” (Laitinen, 2012, p. 5). Another departure fromearlier mastery learning models is that many current CBE models also “personalize” learning by giving studentschoices in how they gain and demonstrate their competencies (i.e., “agnostic as to the source of learning”). Withinefforts to individualize learning with digital technologies, CBE has gained significant support from the USDepartment of Education, publishers, and major US Foundations. For example, the Gates Foundation’s ProjectMastery initiative supports K-12 implementation of “proficiency-based pathways” which offer “opportunities forstudents to engage in a learning experience where they can demonstrate mastery of content and skills and earncredit towards a diploma, certificate, or some other meaningful marker” (Gates Foundation, 2014, p. 7). In theUnited States, although just taking hold in K-12 schools, CBE is already being used to structure entire universities(e.g., Southern New Hampshire University) and is entrenched in many professional vocational education sectors(e.g., healthcare and aviation). While concerns over “seat time” are mostly North American issues, a recent surveyfound noteworthy uptake of CBE around the world (Bristow & Patrick, 2014).CBE is not without critics. A report from the Carnegie Foundation (the originators of the “CarnegieUnit”) provided perhaps the most comprehensive recent critique. Reflecting the underlying tensions introducedabove, the report argued that “by focusing on the acquisition of discrete skills,” CBE “may make it more difficultto promote inter-disciplinary teaching, collaborative learning, and other instructional strategies that the latestresearch in learning science encourages—and the deeper, integrative learning that flows from those instructionalstrategies” (Silva, White, & Toch, 2015, p. 27). The report went on to summarize concerns that observers haveraised regarding CBE, including: (a) CBE has the potential to widen achievement gaps, (b) the differentiation ofinstruction for individuals creates new challenges for educators, (c) it is expensive to implement, and (d) itoverlooks the very real limitations of assessment and measurement.A second recent development in learning technology is the advent of open digital badges. Launched bythe MacArthur and Mozilla Foundations in 2011, this new web-enabled vision of credentialing has captured agood deal of innovation and attention (e.g., Carey, 2012). Digital badges can contain specific claims of learning,evidence to support those claims, and hotlinks to more evidence such as completed student work. Open digitalbadges can readily circulate in social networks and are interoperable with other learning management resources.The transparency associated with web-enabled credentials causes issuers, earners, and consumers to deliberatemore about the nature of claims and the validity of the evidence than with conventional static credentials (whosecredibility was derived from the issuing institution and its accreditation). This means that the introduction of opendigital badges in educational ecosystems can be quite disruptive (Casilli & Hickey, 2016)As with CBE, tensions that can be traced back to associationist and constructivist views of learningemerged around digital badges. Some viewed badges as an extension of CBE, insisting that the claims andevidence inserted in badges should contain measurable and (ideally) measured competencies. At the 2011 kickoffof the Badges for Lifelong Learning competition, US Secretary of Education Arne Duncan stated that digitalbadges “can help speed the shift from credentials that simply measure seat time to ones that more accuratelymeasure competency, and we must do everything we can to accelerate that transition” (Duncan, 2011).Conversely, other influential commentators dismissed digital badges as behaviorist extrinsic incentives (e.g.,Jenkins, 2012; Resnick, 2012). Badge proponents responded by pointing out that people could simply avoid usingthem as arbitrary extrinsic rewards for learning. Ravet (2014) raised this issue in the context of using badges tosupport a European effort to promote Key Competencies for Lifelong Learning. He argued against what he called“normative” badges awarded for mastery of specific competencies by articulating the more general worry thatmany constructivist researchers have with highly specific standards: “But there is one more fundamental problemICLS 2016 Proceedings539© ISLSwith standards, not with the standards as such, but with those who think that standards are the alpha and the omegaof everything, letting standards be the proverbial tail wagging the dog” (p. 24). Rather than motivating compliance,and presumably undermining creativity and innovation, Ravet argued for “achievement badges” that are deliveredafter something has been achieved: “Achievements badges, contrary to key competency badges, do not have tobe not normative. Created along the learning pathway, they can be designed with the learners rather than for them.As they can be created post-facto, they do not bear the stigma associated with the use of Open Badges as extrinsicmotivators.” Ravet elaborated that the “beauty of achievement badges is that they capture the context of theachievement in the criteria: where, how, what resources, etc. And the collection of achievement badges creates afabric of interwoven threads of narratives: one’s own story is interconnected to others’ stories throughachievement badges (2014, p. 25).This position directly follows from self-determination theory (particularly, Kohn, 1999). But the largernotion of badge-based pathways around more social forms of learning and recognition represent the underlyinginfluence of contemporary sociocultural theories of learning (i.e., Yowell and Smylie, 1999) that helped shape theMacArthur Foundation’s introduction of open digital badges. In key ways, open digital badges nicely capture thetension between associationist and constructivist approaches, as well as the potential for the newer socioculturalperspective to point out a path forward. As shown below, a two year study of the 29 project funded in theaforementioned Badges for Lifelong Learning initiative was a useful context for exploring this potential.A third relevant development in learning technologies is the ongoing effort to define metadata standardsfor educational information. Metadata is “data about data.” Efforts have been underway for years to come up withcommon standards for labeling and characterizing the vast quantities of information generated by e-learningsystems (e.g., Bohl, Scheuhase, Sengler, & Winand, 2002). This work has taken on new importance with the useof learning management systems (LMSs) in nearly every college and university and many K-12 schools to supportonline, hybrid, and conventional classroom learning. Many in the Learning Sciences community are just beginningto appreciate the scope and implications of current efforts to develop educational information architecturestandards. Proponents argue that these standards are necessary to support the inter-operability needed to allowlearning technologies (including LMSs, applications, analytic tools, content, learner records, and credentials) totake full advantage of the Internet. As anyone who has tried to innovate within a comprehensive learningmanagement system knows, the systems necessarily make assumptions that constrain options.Much of the current information architecture work is being carried out by IMS Global Learning, aconsortium of publishers, technology providers, schools, and policy makers. IMS Global’s new Learning ToolsInteroperability (LTI) standards are already doing for LMSs what external apps do for smart phones. Consider,for example, digital badges. Issuing badges from within earlier LMS’s would have required massive amounts ofprogramming and significant changes to the core program for each LMS. As of fall 2015, there are at least threeLTI-compliant external applications for issuing digital badges (Badgr from the Badge Alliance/Concentric Sky,BadgeSafe from Accreditrust, and BadgeOS from Credly). Each is competing to offer desired functionality andserve different clients. Arguably, the existence of such standards and the ability to readily use and refine externalapplications are crucial for advancing the nascent digital badging community. IMS recently launched a project todevelop a “currency framework” for digital badges that aims to “further the adoption, integration, andtransferability of digital credentials within institutions, schools, and corporations” (IMS Global, 2015) (1).Likewise, similar efforts are underway by foundations and higher education associations to create commonstandards for “connected digital credentials” more generally. For example, one effort promoted by the AmericanCouncil on Education argues that a “common DNA” is a critical ingredient in quality digital credentials. The effortcites research showing that that “all of the types of credentials in use—degrees, certificates, certifications, licenses,badges, etc.—can be described in the same language of competencies: the level of knowledge and specialized,personal, and social skills the credential represents” (Ganzglass & Good, 2015).The tension between associationist and constructivist views of learning do not appear as obvious inefforts to standardize metadata for educational technologies and credentials. Given that education has primarilybeen organized around credits, the current standardization efforts include efforts to broaden technologies andcredentials to support competency-based approaches. Leuba (2015. p. 1) describe a recent pilot effort to overcomethese limitations, attempting to change the fact that “the products used to manage our institutions and the teachingand learning process are all deeply rooted in the credit-hour based, term-based, and course-based educationaldelivery model”. Initially, it seems, these efforts to standardize metadata and credentials will simply include thingslike “social skills” and “critical thinking” that are paramount to constructivist innovators as additionalcompetencies to be categorized and measured and/or observed. As illustrated next, doing so within efforts tointroduce digital badges systems may be more challenging than innovators expect.ICLS 2016 Proceedings540© ISLSRelevant findings from the Design Principles Documentation ProjectThe Design Principles Documentation (DPD) project was tasked with capturing the “practical wisdom” thatemerged across the 29 diverse badges development efforts funded in the 2012 Badges for Lifelong Learninginitiative, with the support of the John C. and Catherine T. MacArthur Foundation’s Digital Media and Learninginitiative and the Gates Foundation’s Project Mastery initiative. In an elaborate competition, over 600 proposalswere reviewed and the 29 winners were granted approximately $200,000 and partnered with one of several badgesystem developer awardees to develop a badge system over a one-year period as the Open Badges Infrastructurestandards and various supporting technologies were being developed by the Mozilla Foundation.The DPD project first analyzed the content of 29 funded proposals to identify four categories of intendedpractices for digital badges: recognizing learning, assessing learning, motivating learning, and studying learning.The project then followed the 29 projects, conducting periodic interviews with project leaders to determine whichof those practices each project was able to enact and what difficulties they ran into. In 2014 (after all of the newfunds had been exhausted), the project carried out follow-up interviews to determine which practices weremaintained, the final status of the badge system relative to the one articulated in the original proposal, and thefinal status of the larger badge-oriented “learning ecosystem” envisioned in the proposal. By clustering the morespecific practices across projects into more general design principles, the DPD project was able to generate awealth of knowledge about which principles were easier to implement and which principles were harder toimplement. In 2015, the DPD project followed up to determine which ecosystems were still "thriving."The DPD project did not set out to study competency-based badge systems. However, seven of the 29awardees (including three of the awardees funded by Gates’ Project Mastery) had proposed to develop badgesystems for individualized self-paced mastery of specific competencies. As such, the DPD project was presentedwith a unique opportunity to examine the success of competency-based badge systems and related badge designprinciples. Before considering how these particular projects fared, it is worth noting that only 16 of the 29 projectssucceeded in establishing the badge system they originally proposed, while 8 of the projects were judged to haveestablished a different badge system than the one they proposed; 5 of the projects did not build any badge system.Sustainable Agriculture and Food Systems (SA&FS) proposed to build its open badges system within alarger effort to develop a new competency-based interdisciplinary major in the College of Agriculture andEnvironmental Sciences at the University of California-Davis. As was widely reported in the educational media,SA&FS proposed to issue badges within a sophisticated custom e-portfolio system with comprehensive scoringrubrics, and was based on “a model of learning, participation, and assessment focused around high-level ‘corecompetencies’ that bridge classroom and real-world experiences, academic investigations and concrete skills” (2).As with many CBE projects, SA&FS carried out extensive research to document the specific competencies thatemployers who might hire their graduates were seeking. However, the competency-based initiative ultimatelyfailed to gain the wider support of the university needed to sustain it and the project ultimately only managed tocreate a few test badges. The staff member who was spearheading the competency-based system left UC Davisand the new major was established as a conventional course-based program (3).The Pathways to Global Competence badge system was proposed by the Asia Society, a New Yorkbased non-profit that was also implementing its curriculum in dozens of US secondary schools with a ProjectMastery grant. Like SA&FS, they attempted to build an open badge system around a sophisticated e-portfoliosystem; however, they proposed to do so in partnership with a commercial e-portfolio provider (ShowEvidence,Inc.). The project proposed to award Global Leadership badges to secondary students once they earned four globalcompetency badges. For example, one of these four badges was for Generating Global Knowledge. Students wereto earn this badge by submitting an e-portfolio that outside experts confirmed as evidence the student could“initiate investigations of the world by framing questions, analyzing and synthesizing relevant evidence, anddrawing reasonable conclusions about globally focused issues.” (Asia Society, 2011). However the additionalresources that were reportedly needed to establish the badges and the new e-portfolio system were not securedand both software development efforts encountered serious challenges, and the badge system was neverdeveloped. However, the competency framework and scoring rubric was implemented and is presumably stillbeing used in most of their partner schools; an evaluation of the Project Mastery awardees found that manyteachers reported having students develop their portfolios using Google Docs (Steele et al., 2014).The LevelUp badge system was proposed by a partnership between the Adams County District 50 SchoolSystem in Colorado, EffectiveSC, and Intific, Inc. Adams 50 was in the midst of a comprehensive effort to reformseveral of its underperforming schools using CBE with the support a Project Mastery grant. EffectiveSC was anon-profit that was developing the open-source LevelUp personalized competency tracking platform. Illustratingthe technology challenges summarized by Leuba (2015), LevelUp was intended to streamline CBE by serving as“middleware” between the district’s existing Educate student information systems and online instructionalresources where students could develop and demonstrate their competencies. Intific is a Texas softwareICLS 2016 Proceedings541© ISLSdevelopment firm that was funded to develop four Space Wolf competency-based “learning progression games”that were to play a central role in the Adams 50 mathematics reforms. The project proposed to use the LevelUpplatform to allow competencies that students developed playing the games to be automatically transferred to theEducate system. However, technology challenges with LevelUp and Educate and intellectual property issues keptthat badges system from progressing beyond an initial pilot and it was suspended. According to participants fromEffectiveSC, another obstacle was that the districts commitment to CBE declined sharply once the Project Masteryfunding had expired and major technology challenges continued to frustrate teachers, students, and parents. Theseobservations were generally confirmed in the external evaluation by Steele et al. (2014) which also reportedstatistically significant declines in math achievement in the participating schools (see also Sturgis, 2014).The Youth Digital Filmmaker Badge System was proposed by the School District of Philadelphia, whopartnered with the Youtopia’s commercial badging/gamification platform and the Philadelphia Youth Network.The badge system was part of a larger Gates Project Mastery initiative that also aimed to enhance the district’sSchoolNet LMS (from Pearson corporation) and its Pathbrite e-portfolio system to better support competencybased learning by allowing teachers to award course credits for competencies demonstrated in non-school projects.The badge system specifically aimed to foster afterschool “extended learning opportunities” around a new youthfilmmaker program designed to “support academic credit attainment, anytime/anywhere learning, and skillsmastery tied to Common Core State Standards for English and Language Arts.” More specifically, the project goalwas “diversifying the ways and locations in which students can demonstrate mastery of critical reading, writing,and communication skills via multiple options to publish and produce films.” To support these goals, the projectproposed to develop detailed scoring rubrics that would allow external agencies to endorse the badges, externalexperts to review storyboards, scripts, and videos for evidence of competencies, and teachers to award studentsformal academic credit for those competencies. However, significant technological challenges were encounteredwith all three new technologies. Additionally, the project failed to secure external endorsements for its badges andsome of the teachers in the badges pilot project were reportedly reluctant to award formal course credit for theyouth filmmaker badges. All three systems were paused after a single pilot.The National Manufacturing Badge System was developed by the non-profit Manufacturing Institute inpartnerships with SkillsUSA, a workforce development agency and with Project Lead the Way (PLTW), a nationalSTEM curriculum-development initiative. The partnership with SkillsUSA aimed to issue badges for secondaryvocational students in automated manufacturing programs who also attained passing scores on standardizedperformance-based assessments developed by SkillsUSA for industry-defined competencies. In this way, theproject proposed to create “efficient competency-based pathways to careers” by integrating standardizedassessments of industry-endorsed competencies into existing course-based educational programs. In this respect,the project has proposed the sort of education reform that many CBE proponents have been calling for. The projectsucceeded in creating a system for offering an Automated Manufacturing Technology badge for vocationalstudents who passed the SkillsUSA assessments. But, the project abandoned its plan to offer “leveled” badges thatrecognized mastery of the more specific competencies (reportedly because they concluded that such a profusionof badges would confuse employers) (4). Instead the criteria for earning the final badge was attaining a passingscore on all of relevant SkillsUSA assessments. Unfortunately, the project was unable to secure formalendorsements from manufacturers who would hire badge earners (reportedly because they wanted to first “seesomebody who had earned the badge who could do the job”). This endorsement was necessary to convinceeducators to incorporate the assessments into their courses; without it, the project stalled at the pilot testing stage.The other Manufacturing Institute badge project was a Computer Integrated Manufacturing badge forstudents at partner schools who completed PLTW’s standardized curriculum and attained a passing score onPLTW’s end of course assessment. However, other than the endorsement of the Manufacturing Institute, the badgedid not contain any additional evidence beyond the formal credential issued by the schools. While the badge isstill being offered by this partnership in 2014, the project leader reported that few, if any, of the potential earnerswere claiming the badge; while the badge was still being offered in 2015, we found no evidence it was thriving.The Young Adult Library Services Association (YALSA) badge system was proposed to recognize masteryof YALSA’s Competencies for Serving Youth in Libraries (5). This included 48 specific competencies in sevenareas, including social media, collection building, and public outreach. YALSA first attempted to create seven“pie badges” that displayed which of the sub-competencies that each earner had demonstrated. However, thisproved technologically challenging and was set aside in lieu of badges that were issued once mastery of all of thecompetencies had been demonstrated. The project encountered significant challenges in establishing its websiteand was forced to scrap its initial system and state anew with a second web development team. They ultimatelycreated a sophisticated system for peer assessment of the artifacts earners were asked to submit in order to earnthe badges. However, in 2015, project leaders reported that few were attempting to earn the badges and concludedthat the amount of work required to earn even a single badge was apparently too great given, given that there wereICLS 2016 Proceedings542© ISLSno specific incentive or opportunity associated with earning it. They reported that their advisors encouraged themto make sure that the badges were "really valuable."The Buzzmath badge system was proposed by ScoLab, a small educational software firm in Montreal.They proposed to issue badges to recognize mastery of specific competencies as learners progressed through adrill and practice game for middle school mathematics (akin to MathBlaster). The firm used the grant to developthe badges as well as the larger Buzzmath platform. The project succeeded in building both the platform and thebadges and aligning both to Common Core math standards. The system has proven to be a commercial successand continues to thrive; an independent evaluation showed that students and teachers believe that playing thegames had a positive impact on math achievement and understanding (Morrison, Ross, & Lusiczka, 2015).However, privacy concerns precluded the use of web-enabled open and a planned peer tutoring system (the useof emails addresses as identifiers violate the stipulations of the US Children’s Online Privacy Protection Act).They were also unable to secure external endorsements of their badges by schools or the organizations theestablished the educational standards their badges were aligned to.Summary and conclusionsIn summary, only none of the seven projects that attempted to build badge systems around self-paced mastery ofhighly specific competencies succeeded in creating a thriving open learning ecosystem--BuzzMath succeeded inbuilding a system that used non-open badges as tokens in a drill and practice program. The reasons the otherprojects struggled were varied. Certainly, the blame cannot be placed entirely (or in some cases even partly) onthe decision to implement a competency-based system. Put differently, the DPD project did not conclude thatthese projects would have been more successful had they attempted to issue “time-based” badges based onparticipation in courses or other education activities. Nonetheless, these projects suggest care and caution isneeded when developing competency-based badge systems. In particular, it seems competency-based systemsshould anticipate the challenges that the DPD project uncovered as well as the tensions in CBE implementationsreported in the separate evaluation of the three Gates’ Project Mastery initiatives (Steele et al., 2015). Thesetensions included equating evidence from anytime/anywhere learning with conventional criteria, determining whocan authorize credit, maintaining a common definition of proficiency, building a sustainable model, technicalbarriers to efficiency, financial barriers to efficiency, logistical barriers to efficiency, and promoting equity. Inkey ways, these conclusions bolster the concerns in the aforementioned Carnegie Foundation, report, while alsohighlighting the challenges that student information systems present for CBE summarized by Leuba (2015).Consider for example, the tensions over equating evidence and authorizing course credit. As mentionedpreviously, one of the three teachers in the Youth Filmmaker pilot was reluctant to accept the badges for coursecredit. The Rand evaluation reported project leaders were surprised by this reluctance given the efforts they hadtaken to align their badges to the Common Core standards. The Rand Report explained that the teacher was notconvinced “that the persuasive writing skills appropriate for script development of a documentary film were theequivalent of what he expected students to achieve in a persuasive essay, especially in terms of issues like essaystructure and sentence structure.” The report went on to state that the teacher “believed that preparing a shortnonfiction film and preparing a persuasive essay tapped different skills, both applicable to the real world, but notinterchangeable” (Steele et al., 2014, p. 42). After examining the curriculum and the relevant standards, the reportconcluded the teacher’s position “seems reasonable.” They also reported concerns such as privacy and validityled all three of the Project Mastery projects to abandon their plans to use external expert arbiters of credit; in thecase of the Youth Filmmaker project, this eliminated the intended expert feedback, a key element of the envisionedlearning ecosystem. The Rand evaluation reported that Asia Society teachers “found it difficult to get districtapproval to turn school-led travel experiences into course credit” (p. 42) and that some resorted to substantialmeasures to bypass their school districts course-based policies and information systems.The tensions over equating and authorizing gets at the heart of the CBE’s “agnosticism” regarding thesource of learning. For many assessment researchers and validity experts, this assumption is untenable. This isbecause an assessment system must take into account for the way individuals prepared for that assessment if theresulting evidence is to provide valid evidence to support claims of competency. This is because two differentindividuals can attain the same score on a given test or produce portfolios or other artifacts that earn the samescore via very different paths. Compare, for example, an individual who has prepared narrowly for a particularassessment by memorizing the specific associations (correct and incorrect) represented by the various items withan individual who has completed a broader course or learning pathway that was not focused on those associations.If both individuals get the same score, the second individual is almost certainly more knowledgeable than the first,because of all of the other new knowledge that the assessment could not capture. Likewise, consider an individualwho creates an e-portfolio following very detailed guidelines and has access to the scoring rubric andICLS 2016 Proceedings543© ISLSindividualized feedback on drafts against the rubric. This individual almost certainly is less competent than anindividual who submits a similarly-scored e-portfolio developed without access to this information and feedback.This concern with “teaching to the test” is what the validity expert Samuel Messick (1994) labeled“construct-irrelevant” easiness. Hickey and Zuicker (2013) argued that this phenomenon has always been moreinsidious and much more complex than many educators and innovators realize. Caution seems called for given(a) the compromise and cheating made possible by new digital devices and social media and (b) the disdain formultiple-choice achievement tests and embrace of performance-based and portfolio assessment among CBE anddigital badge proponents. Take, for example, the seemingly sensible practice of awarding badges and credit for“anytime/anywhere” learning for passing “scenario-based” performance assessments. Once such information iscirculating freely in professional social networks, it becomes substantially easier for vocation educators andpotential test takers to figure out precisely what scenario is used in the assessment and potentially the specificquestions asked. Unless the assessment includes dozens of scenarios and hundreds of questions, it cannot possiblycover the entire range of competencies. Such concerns certainly give credence to the concerns constructivisteducators and proponents of time-based credentials have for strict competency programs. Messick introducedraised this issue of construct-irrelevant easiness in the context of the large-scale K-12 performance and portfolioassessment reforms that came (and largely went) in the 1990s. Concerns over validity and lack of promisedpositive consequences for teaching and learning were major reasons for the rollback of this earlier wave ofassessment reforms. As a kind of assessment reform, it seems that CBE should attend to this issue as well.Arguably, newer sociocultural perspectives that provided much of the impetus for introducing digitalbadges offers a potential path forward. As argued by Hickey (2015), sociocultural approaches to assessment andvalidity tend to focus primarily on communal participation in social practices, and only secondarily on assessmentof individual knowledge and skill. This allows these practices to treat all forms of individual assessment as“secondary” kinds of evidence—special cases of primarily social learning. When coupled with contemporarydesign-based research methods that guide iterative refinement of communal participation, this makes it possibleto treat individual performance on the entire range of assessments and tests as evidence of the success of thoserefinements. An initial examination of two project whose badge systems are particularly thriving (the Support toReporter youth journalism project and the MOUSE youth network manager mentoring program) supports thisbelief. Both issued what might best be described as “role-based” badges. While the badges included claims ofspecific competencies, the evidence of these competencies was primarily the earner’s participation in workshopsand other activities with peers and endorsements by experts and peers. Rather than creating a comprehensive listof competencies and assessments in advance, both projects gradually and iteratively refined networked sociallearning activities for cohorts of participants to maximize the quality of and quantity of disciplinary interactionsamong learners (see O'Byrne, Schenke, Willis, & Hickey, 2015). Additional efforts now underway are exploringthis question and attempting to derive a more comprehensive characterization of how these perspectives guide thedesign of successful open badge systems while reconciling the tensions between different perspectives.Endnotes(1) Disclosure: the author is participating in this activity as an advisory board member.(2) Unless indicated otherwise, all of the quotes from are from the submitted proposals or DPD Project interviews whichcan be accessed at the project website and reported in Author (in preparation).(3) https://www.pltw.org/pltw-engineering-curriculum(4) http://asi.ucdavis.edu/programs/safs(5) http://www.ala.org/yalsa/guidelines/yacompetencies2010ReferencesAlpert, D., & Bitzer, D. L. (1970). Advances in computer-based education. Science, 167(3925), 1582-1590.Anderson, R. C., Kulhavy, R. W., & Andre, T. (1971). Feedback procedures in programmed instruction. Journalof Educational Psychology, 62(2), 148-156.Asia Society (2011). Graduation portfolio system global leadership performance outcomes. New York.Bill and Melinda Gates Foundation (2014). Supporting students: Investing in innovation and college. CollegeReady Work Monographs. Seattle WA.Bristow, S. F., Patrick, S. (2014). An international study in competency education: Postcards from Abroad.Washington, D.C.: International Association for K-12 Online Learning.Bohl, O., Scheuhase, J., Sengler, R., & Winand, U. (2002, December). The sharable content object referencemodel (SCORM)-a critical review. Proceedings from the International Conference on Computers inEducation. 950-951.Brøderbund (1996). Logical Journey of the Zoombinis (Computer software). Novato, CA: Brøderbund.ICLS 2016 Proceedings544© ISLSCarey, K. (2012 November 12). Show me your badge. The New York Times.Casilli, C., & Hickey, D. (2016). Transcending conventional credentialing and assessment paradigms withinformation-rich digital badges. The Information Society, 32(2), 117-129.Duncan, A. (2011, September 1). Opening remarks and DML 2012 competition event transcript. Retrieved from:http://dmlcompetition.net/Competition/4/opening-event-transcript.php.Eckert, R., & Davidson, J. (1987). Math blaster plus [Computer software]. Torrance, CA: Davidson & Associates.Ganzglass, E., & Good, L. (2015). Rethinking credentialing. Washington DC: American Council on Education,Center for Education Attainment & Innovation.Greeno, J. G., Collins, A. M., & Resnick, L. B. (1997). Cognition and learning. In D. Berliner & R. Calfee (Eds.),Handbook of educational psychology (pp. 15-47). New York: Simon & Schuster Macmillan.Guskey, T. R. & Gates, S. L. (1985). A synthesis of research on group-based mastery learning programs.Proceedings from the 69th Annual Meeting of the American Educational Research Association.Hickey, D. T. (2015). A situative response to the conundrum of formative assessment. Assessment in Education:Principles, Policy & Practice, 22(2), 202-223.Hickey, D. T., & Zuiker, S. J. (2012). Multilevel assessment for discourse, understanding, and achievement.Journal of the Learning Sciences, 21(4), 522-582.Hmelo-Silver, C. E., Duncan, R. G., & Chinn, C. A. (2007). Scaffolding and achievement in problem-based andinquiry learning: A response to Kirschner, Sweller, and Clark. Educational Psychologist, 42(2), 99-107.IMS Global Learning Consortium (2015, April 21). IMS Global Announces Initiative to Establish Digital Badgesas Common Currency for K-20 and Corporate Education [Press Release].Jenkins, H. (2012, March 5). How to earn your skeptic badge. [Web log post].Kirschner, P. A., Sweller, J., & Clark, R. E. (2006). Why minimal guidance during instruction does not work: Ananalysis of the failure of constructivist, discovery, problem-based, experiential, and inquiry-basedteaching. Educational psychologist, 41(2), 75-86.Kohn, A. (1999). Punished by rewards: The trouble with gold stars, incentive plans, A's, praise, and other bribes.Boston: Houghton Mifflin Harcourt.Kolowich, S. (2013). Faculty backlash grows against online partnerships. Chronicle of Higher Education.Koszalka, T., & Ganesan, R. (2004). Designing online courses: A taxonomy for strategic use of features availablein course management systems (CMS) in distance education. Distance Education, 25(2), 243-256.Laitinen, A. (2012). Cracking the Credit Hour. Washington DC: The New America Foundation. [Policy Paper].Leuba, M. (2015, October 12). Competency-based education: Technology challenges and opportunities.EDUCAUSE Review, [Online].Messick, S. (1994). The interplay of evidence and consequences in the validation of performance assessments.Educational Researcher, 23(2), 13-23.Morrison, J. R., Ross, S. M., & Lesiczka, J. K. (2015). Report for iZone NYC Department of Education: Resultsof Buzzmath short cycle evaluation. Baltimore MD: Johns Hopkins University Center for Research andReform in Education.Niemiec, R. P., & Walberg, H. J. (1989). From teaching machines to microcomputers: Some milestones in thehistory of computer-based instruction. Journal of Research on Computing in Education, 21(3), 263-276.Ian O'Byrne, W., Schenke, K., Willis, J. E., & Hickey, D. T. (2015). Digital badges: Recognizing, assessing, andmotivating learners in and out of school. Journal of Adolescent & Adult Literacy, 58(6), 451-454.Papert, S. (1980). Mindstorms: Children, computers, and powerful ideas. Basic Books, Inc..Ravet, S. (2014). #Openbadges for key competencies. Learning Futures: Reflections on Learning, Technologies,Identities, and Trust [Web log post].Resnick, M. (2012, February 27). Still a badge skeptic. [Web log post].Siemens, G. (2005). Connectivism: A learning theory for the digital age. International Journal of InstructionalTechnology and Distance Education, 2(1).Silva, E., White, T., & Toch, T. (2015). The Carnegie Unit: A century-old standard in a changing educationallandscape. Carnegie Foundation.Steele, J. L., Lewis, M. W., Santibanez, L., Faxon-Mills, S., Rudnick, M., Stecher, B. M., & Hamilton, L. S.(2014). Competency-Based Education in Three Pilot Programs: Examining implementations andoutcomes. Santa Monica, CA: The Rand Corporation.Sturgis, C. (2014, March 12). A conversation with Adams 50. [Web log post].Tobias, S. & Duffy, T. M. (Eds.). (2009). Constructivist instruction: Success or failure? New York: Routledge.Yowell, C. M., & Smylie, M. A. (1999). Self-regulation in democratic communities. The Elementary SchoolJournal, 469-490.ICLS 2016 Proceedings545© ISLS