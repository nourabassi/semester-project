Research Questions and Research Methods in CSCL ResearchHeisawn Jeong, Hallym University, heis@hallym.ac.krCindy E. Hmelo-Silver, Indiana University, chmelosi@indiana.eduAbstract: While research questions play a critical role in research, their role has seldom beenexamined systematically. In this study, we examined what kinds of research questions areaddressed in CSCL empirical research and what is the relationship between research questionsand methods, in part using outcomes of an earlier analysis. The analysis showed that CSCLresearch has mainly focused on design and implementation of technological and/or curricularinterventions. Research questions influenced research methods to a degree, although therelationships were not strict. The results confirmed, as well as contradicted, some of theintuitive conceptions about the relationship between research questions and methods.Implications for future CSCL research are discussed.Keywords: research questions, research methods, CSCL, relationshipThe scientist is not a person who gives the right answers, she's one who asks the rightquestions (Claude Lévi-Strauss)I suppose it is tempting, if the only tool you have is a hammer, to treat everything as if it werea nail (Abraham Maslow)IntroductionThe progress of a field is often judged based on the validity of empirical findings generated in the field, but thekinds of outcomes we find in our research are inherently tied to the kinds of question we ask. Researchquestions direct our attention to particular aspects of phenomena and/or influence the kinds of methods wechoose to answer the questions. At the same time, research questions are also often determined and/orconstrained by the methods available to us because it is not possible to answer questions without propermethodology. Despite their importance, relatively little attention has been paid to the kinds of researchquestions we ask and how they influence research methods. In this study, we examined research questions inrecent empirical investigations of CSCL and how they are related to research methods used to answer thequestions.Research questionsResearch questions bridge and connect what is known and what is unknown. Research questions organize ourresearch activities. We use research questions to determine the appropriateness of data collection and analysismethods and evaluate the relevance and meaningfulness of results (Onwuegbuzie & Leech, 2006). Byunderstanding what kinds of research questions are being asked, we can understand where the field is headed(Moore, 1993). Research questions are often distinguished from research problems or research goals (Creswell,2007; Johnson & Christensen, 2008; Onwuegbuzie & Leech, 2006). According to Johnson and Christensen(2008), a research problem is an issue or dilemma within the broad topic area that needs to be addressed orinvestigated (e.g., lack of learner motivation in online learning). A research purpose or objective follows fromthe research problem and specifies the intent of the study such as whether it intends to describe variablerelationships, explain the causality of the relationships, or explore a phenomenon (e.g., whether to search forcauses or seek a remedy). A Research question is a statement of specific inquiry that the researcher seeks toaddress (e.g., whether lack of motivation can be reduced in certain conditions). A Hypothesis, unique toquantitative research, is a formal expression of the research question.CSCL emerged as a result of the efforts geared toward understanding how learners learn together withthe help of digital technologies. There is a great diversity in research questions being asked in CSCL. Over theyears, CSCL research has helped us to understand, for example, that collaborative learning is not a recipe, mediaeffectiveness is a myth, a greater resemblance to face-to-face interactions is not necessarily better, andstructuring communication is a subtle compromise (Dillenbourg, Järvelä, & Fischer, 2009). A number ofresearchers, however, recently raised concerns about gaps in CSCL research, arguing that the field as a wholehas emphasized the cognitive dimension of learning too much and neglected the role of affect; lack of researchon institutional contexts of CSCL has also been raised (Arnseth & Ludvigsen, 2006; Dillenbourg et al., 2009;Kirschner & Erkens, 2013). There are a number of different ways to examine the kinds of questions that theCSCL 2015 Proceedings316© ISLSfield as a whole has been asking. We may ask what drives our research questions, that is, whether they aremotivated by theoretical or practical concerns. Some research questions seek proof of concept (e.g., can thiswork?), whereas others aim to test the effect of a design or variable. One may also evaluate research questionsin terms of whether they require quantitative and/or qualitative outcomes to address them and/or whether theyare socially relevant and responsible (Roschelle, Bakia, Patton, & Toyama, 2011). Although these are allmeaningful and useful typologies, in this paper, we examined which aspects of domain knowledge have beenaddressed in CSCL research, that is, whether CSCL research questions are directed toward understandinglearning processes, outcomes and/or interventions.Research questions and research methodsResearch questions influence or even determine the kinds of answer we get (Maslow, 2002; Suthers, Lund,Rosé, Teplovs, & Law, 2013). This is in part because the way we ask questions determines the method we useto answer those questions. Questions often dictate the methodology needed to answer the questions so thatquestions about causal relationships of variables lead to experiments, whereas questions about correlationalrelationships of variable relationships and/or about contextual factors often lead to descriptive studies.Similarly, questions about learning outcomes requires the collection of data that can reveal student performanceon exams or other kinds of measures that can show the interim or end product of learning.The relationships between research questions and methods have been generally conceived in relation tothe research objectives, that is, whether the research aims to achieve quantified or qualitative understanding.According to Onwuegbuzie and Leech (2006), for example, quantitative research questions tend to be veryspecific in nature and deal with descriptive (e.g., what are high-school graduation rates?), comparative (e.g.,what is the different between elementary and middle school students’ math abilities?) or relationship questions(e.g., what is relationship between variable A and B?). Within the quantitative tradition, survey or descriptivedesigns are typically used when describing correlational relationships, but experiments are the method of choicewhen determining causal relationships among variables (Shadish & Campbell, 2002). Qualitative questions, onthe other hand, may be open-ended, evolving, and non-directional. They seek to discover, explore or describeprocesses and experiences. They typically describe, rather than relate variables or groups, and tend to address“what” and “how” questions (Yin, 2009).Although there is a view that certain methods are inherently superior than others (Becker & Geer,1957; What Works Clearinghouse, 2010), appropriate research methods are likely to vary depending on theresearch problems and questions (Onwuegbuzie & Leech, 2006; Trow, 1957). The exact nature of theserelationships, however, has never been systematically examined in CSCL research. In this paper, we areinterested in understanding whether research questions about certain aspects of CSCL learning (e.g., questionsof CSCL outcomes) are studies with certain methods more so than with other methods. For example, doresearch questions about CSCL outcomes tend to be studied more with certain methods over others? Or doresearch questions about collaboration processes require the collection of certain data types and analysismethod? By examining these relationships, we hope to understand more clearly how research questions interactwith research methods and what the resulting relationships mean.Current investigationThis paper aims to address two questions. First, what kinds of research questions have been asked in recentCSCL empirical investigations? Second, what are the relationships between research questions and researchmethods? In order to answer the first question, we used content analysis and categorized research questions interms of whether they address learning outcomes, process, or CSCL interventions. In answering the secondquestion, we relate the analysis of research questions to the results of a prior study. Jeong, Hmelo-Silver, andYu (2014) analyzed the research methods of published empirical investigations of CSCL between 2005 and2009. They coded CSCL research methods along four dimensions—research design, settings, data types, andanalysis—and found that CSCL researchers employ a diverse set of methodology in their research. In order toexplore how research questions guide and influence research methods, we systematically examined therelationship between the question types and dimensions of research methods. In the current study, we focus onstudies between 2005 and 2008 due to the availability of question coding results. Note that this is part of an ongoing project that examines CSCL research practice comprehensively. Analysis of additional years and aspectsof CSCL research are ongoing.CSCL 2015 Proceedings317© ISLSMethodsJournal and paper selectionPapers were selected first by choosing relevant journals and then selecting appropriate papers from them(Hrastinski & Keller, 2007). In this study, we chose representative CSCL journals by surveying 16 CSCLcommunity leaders (e.g., CSCL committee of ICLS and the editorial board members of ijCSCL). Based on theirresponses, we selected the following seven journals: (1) International Journal of Computer SupportedCollaborative Learning (ijCSCL) (2) Journal of the Learning Sciences, (3) Learning and Instruction, (4)Computers and Education, (5) Journal of Computer Assisted Learning, (6) International Journal of ArtificialIntelligence in Education, and (7) Computers in Human Behavior. 1,422 research articles were published in theseven journals during the 2005-2008 periods. We selected 310 empirical CSCL investigations using thefollowing criteria: Learning needed to be collaborative and supported by technological tools, but as long as partsof the learning process involved interaction (e.g., collaborative discussion after individual study), it wasconsidered collaborative (see Jeong et al., 2014 for additional details about the selection criteria and process;Due to space limitations, the full list of these 310 papers are not provided here, but are available upon request).AnalysisWe combined content analysis and qualitative descriptions to analyze research questions and methods of CSCLempirical investigations. Coding categories for the content analysis were developed using a combination ofinductive and deductive approaches: They were initially developed based on a combination of several top-downschemes (e.g., categories drawn from the submission descriptors of the 2005 CSCL conference) and laterrefined inductively in the process of coding. The coding categories used in this study are reported below. 20%of the papers were checked for reliability. Cohen’s Kappa for all coding categories was all above .75.Research questionsResearch questions were categorized into one of the seven categories, some with sub-codes: (1) Conceptual, (2)Methodological, (3) Process, (4) Outcome, (5) Learner characteristics, (6) Interventions, (7) Other (see Table 1).Coding into multiple categories was allowed when the paper addressed more than one research questions.Table 1: Coding categories for CSCL research questionsQuestion codesConceptualMethodologicalProcessOutcomeLearnercharacteristicsInterventionOtherCode descriptionsDeveloping/testing conceptual framework and/or refining/testing theoretical constructs/models.Assessment instrument, analysis methods, or design tools or processes.CSCL processes that can be about (a) collaborative processes (e.g., argumentation), (b) general,often individualistic learning processes (e.g., conceptual change), or (c) other processes relatedto CSCL (e.g., participation frequency, help use).CSCL outcomes that can be about (a) knowledge outcomes either at the individual orshared/collective level (e.g., knowledge building), (b) skills, (c) non-cognitive outcomes (e.g.,perceptions, motivations, attitudes, etc.), or (d) miscellaneous.Individual differences and learner characteristics (e.g., gender differences, individualdifferences in help-seeking).CSCL interventions that can be (a) instructional (e.g., curriculum development, scripting)and/or technological (e.g., mobile technology, representational tools).Questions that did not fit one of the above categories (e.g., parental involvement, communitynetwork structures).Research methodsResearch methods were coded along four dimensions: (1) Research design, (2) Setting, (3) Data, and (4)Analysis. Research designs were coded as (a) Experimental, (b) Descriptive, or (c) Design-based method.Research settings were coded as (a) Laboratory, (b) Classroom or (c) Other settings. Data were coded as (a)Process (e.g., text-messages, video, log data), (b) Outcome (e.g., multiple-choice, open-ended artifacts), and (c)Miscellaneous (e.g., self-report questionnaires, interviews). Analysis methods consisted of three generalcategories: (a) Quantitative (e.g., code-count, inferential statistics), (b) Qualitative (e.g., Conversation Analysis),and (c) Mixed-analysis (see Jeong et al., 2014, for coding details).CSCL 2015 Proceedings318© ISLSFindingsCSCL research questionsAs can be seen in Figure 1, a small portion of CSCL research addressed theoretical or methodological questions(3% and 7% each). Although these studies included data, often in the form of examples, they mainly addressedtheoretical (e.g., how to conceptualize the institutional impact of CSCL) or methodological questions (e.g.,development of rating scheme for interaction quality). Close to half of the methodological papers were aboutcontent analysis addressing issues such as coding scheme development (Meier et al., 2007), unit of analysis(Strijbos & Stahl, 2007), or validity and reliability of content analysis (Beers et al., 2007). The rest of themethodological questions addressed issues such as statistical solutions to multi-level analysis (Cress, 2008),Social Network Analysis (Martine et al., 2006), asynchronous discussion data mining (Dringus & Ellis, 2005),or mixed-method (Schrire, 2006).Figure 1. Research questions in empirical CSCL research.Process questions were addressed in 35% of the studies. Most of them (80%) posed questions aboutcollaborative processes, including collaborative learning and problem solving (e.g., Schwarz & Groot, 2007),linguistic and communication processes such as grounding or chat confusion (e.g., Fuks et al, 2006), or socialprocesses such as group dynamics or development (e.g., Guldberg & Pilkington, 2006). The rest of the processquestions (20%) dealt with miscellaneous processes such as system use or coordination processes in CSCL (e.g.,Erkens et al., 2005), along with generic learning processes such as conceptual change (Parnafes, 2007).Questions about CSCL outcomes were addressed in 36% of the studies. About half of them (52%) examinedknowledge outcomes, but these were mostly outcomes at the individual level. Only a small portion of studies(11%) posed questions about group-level outcomes such as collective knowledge building (e.g., van Aalst &Chan, 2007). Relatively little attention (13%) has been paid to skills such as collaboration skills or criticalthinking skills (Rummel & Spada, 2005), but close to half (46%) examined a variety of non-cognitive outcomessuch as students’ and teachers’ perception of the environment, motivation, or attitudes (e.g., Bergin et al., 2007).The remaining papers (6%) examined miscellaneous outcomes such as ethical behavior and accuracy of peerassessment (e.g., Sitthiworachart & Joy, 2008).The dominant question in CSCL research (65%) was about interventions. Most (76%) focused ontechnological aspects of CSCL interventions such as the effect of a software agent as a learning partner or theuse of ITS to support collaborative learning (e.g., Holmes, 2007). About one-third of intervention studies (31%)examined instructional interventions such as effects of new curricular activities (e.g., Smith & Reiser, 2005) orassessment schemes (e.g., Lee, Chan, & van Aalst, 2006) within the contexts of CSCL. These questions werestudied with a focus on the intervention itself (e.g., situating a specific CSCL application across differentacademic disciplines) as well as in relation to their effects on student learning processes and outcomes (e.g.,effects of CSCL application(s) on post-tests or students’ attitude toward mathematics) or learning process (e.g.,whether a chat tool decreases ‘chat confusion’, comparison of different CSCL applications on the discussionprocess). A small proportion (7%) of studies were interested in understanding learner characteristics. Theseinclude individual differences in help-seeking, communication styles, or gender differences in CSCL (e.g., Choet al., 2007).CSCL research questions and methodsIn this section, we consider how research questions are related to various dimensions of research methods. Tosimply the analysis, we focused on the papers that asked one of the three most frequent CSCL researchquestions, that is, intervention, process, and outcome questions (N=127). The analysis showed that althoughCSCL 2015 Proceedings319© ISLSdescriptive designs were dominant in studying all three types of research question, different questions tended tobe studied with different research designs (Table 2). Questions of CSCL process and outcomes were both likelyto be studied in a descriptive manner, whereas intervention questions relied more on experimental designs.Design-based research was largely used to study CSCL processes and interventions.Table 2: Research questions and designQuestionsProcessOutcomeInterventionTotalDescriptive28 (80%)12 (80%)43 (56%)80.84Experimental3 (9%)3 (20%)22 (29%)28Design-Based4 (11%)0 (0%)12 (16%)16Total351577Research questions are also associated with analysis methods (Table 3). Reliance on quantitativeanalysis was strong for all research questions as reported in Jeong et al. (2014), but the degree varied across thequestions. Although outcome questions exclusively relied on quantitative or mixed methods, process andintervention questions relied more on mixed and qualitative methods. Research questions were also related todata collection, that is, the kinds of the data collected in the study. Process questions were more likely to beassociated with the collection of process data such as text messages or video. Outcome questions relied on datasuch as pre- and post-test questions, but also on self-report and/or questionnaire data. Intervention questionsrelied on a roughly equally frequent distribution of process and outcome data. Unlike other dimensions coded,research settings do not seem to be related to research questions, at least at the level of questions coded in thisstudy. Across all questions, classrooms were the dominant setting, followed by laboratory and other settings,indicating that questions of causal relationships are no longer restricted to studies in laboratory settings.Table 3: Research questions and designQuestionsProcessOutcomeInterventionTotalQuantitative15 (29%)10 (67%)27 (35%)52Qualitative11 (31%)0 (0%)19 (25%)30Mixed9 (26%)5 (33%)31 (40%)45Total351577The relationships reported above suggest that there are some connections between research questionsand methods, we should also bear in mind that the relationships are not entirely deterministic. For example,Mirza, Tartas, Perrer-Clermont, and de Pietro (2007) studied graphical tool use for argumentation. Theirapproach was descriptive and the analysis described an example of learning activity mediated by the softwarewith a focus on knowledge construction and argumentation processes. Although the analysis relied on somecoding and counting (e.g., of meaning-making oriented units), the main part of the analysis were qualitativecharacterizations of two argumentation maps (e.g., whether and when a moral dimension became a focus ofdiscussion) and how each map evolved over turns. On the other hand, Lund, Molinari, Sejourne, and Baker(2007), adopted an experimental approach as they also studied students’ use of argumentation diagrams. Theycompared two conditions, one in which student pairs used the tool as a means for debate and another in whichthey used the tool for representing debate. Graphs generated during the debates were analyzed using anelaborate coding scheme called ADAM (Argumentation diagram Analysis Method), which produced debate anddifference scores. Statistical tests were also carried out to identify main factors for studying argumentativegraphs and to compare the two experimental conditions. These two studies both shared similar, overlappingquestions about the same kinds of graphical argumentation tools. They even collected and analyzed the sametype of data (i.e., argumentation maps). Yet, they adopted different study designs and analysis methods so thatMirza et al. relied on a descriptive case study and described how the tool are used in the process of meaningmaking more or less qualitatively, whereas Lund et al. focused on comparing different instructionsaccompanying the tool and attempted to quantify the impacts. So, while certain question tended to be studieswith certain methods, research questions does not appear to constrain research methods in any inherent way anda diverse methods have been applied to study different aspects of CSCL.CSCL 2015 Proceedings320© ISLSConclusions and implicationsNot surprisingly, our results suggest that questions in CSCL empirical research have mostly been focused onexamining technological and/or instructional interventions of CSCL, often in connection to learning processesand outcomes. These represent important areas of CSCL and it appears that the field is trying to lead the way ininnovating technological and curricular interventions and examining them in connection to learning processesand outcomes. At the same time, the results also raise questions as to whether CSCL research is overly drivenby technological interventions and/or whether adequate attention has been paid to other important aspects ofCSCL, such the role of teachers or other mechanisms for facilitating productive knowledge building. In thissense, our analysis confirmed some of previously identified concerns such as that there were few studies thataddressed institutional contexts of CSCL (Arnseth & Ludvigsen, 2006). Although our coding categories did nothave a separate code for it, we did not find these captured in the other questions category. In addition, althoughlearning in CSCL environments involves changes not just in individual learners, but also in groups orcommunities (Cress, 2008; Stahl, 2013), few studies addressed learning at the collective level. This is likely tochange with newer analytic techniques found in more recent studies, but we need to be mindful of examiningCSCL at multiple levels and/or from multiple perspectives. Although it appears that affective or non-cognitivedimensions of CSCL have received a fair amount of research attention judging from the kinds of questions anddata collected, there have been few attempts to examine them deeply. Moreover, studies that attempt to integratecognitive and social mechanisms of CSCL were rare but this is an area that we anticipate will change as wecontinue our ongoing analysis of more recent research.Our analysis found that research methods did vary depending on the research questions, although thecoupling between the two was loose. Research methods do not automatically follow research question in onepath, and there are different methods for pursuing similar questions. This is good news, especially consideringrecent attempts at crosstalk between different methodological traditions (Suthers et al., 2013) and can serve as abasis for research synthesis. However, the relationship between research questions and method is not arbitrary.Research questions do constrain research methods to some extent. The constraints may be inherent in thequestions and/or method at least in certain cases, but they may also originate from our own biases in the kindsof questions we ask and methods we are familiar with. As the productive multivocality project demonstrated, wealso have much to learn from each other (Suthers et al., 2013). Through a collaborative effort on common datasets, researchers from different disciplines engaged in effectively expanding their repertoire of both questionsand methods. Further research is needed to understand more clearly the nature of these relationships. This willlikely involve understanding the inherent methodological mandates of certain research questions and/or methodsand overcoming our deep-rooted biases and experimenting with new research topics and methods. It will alsoinvolve a further examination of a selection of papers in terms of how they define the problems and justify theselection of the methods. It is unclear yet how the results of the study can be used to guide the choice ofresearch methods, but we hope that our goal opens these issues up for further discussions and help us betterunderstand what we ask and how we seek answers to our question.ReferencesArnseth, H. C., & Ludvigsen, S. (2006). Approaching institutional contexts: systemic versus dialogic research inCSCL. International Journal of Computer-Supported Collaborative Learning, 1(2), 167–185.Becker, H. S., & Geer, B. (1957). Field methods and techniques participant observation and interviewing : AComparison. Human Organization, 16(3), 28–32.Beers, P. J., Boshuizen, H. P. a. A., Kirschner, P. A., & Gijselaers, W. H. (2007). The analysis of negotiation ofcommon ground in CSCL. Learning and Instruction, 17(4), 427–435.Bergin, D. A., Anderson, A. H., Molnar, T., Baumgartner, R., Mitchell, S., Korper, S., … Rottmann, J. (2007).Providing remote accessible field trips (RAFT): an evaluation study. Computers in Human Behavior,23(1), 192–219.Cho, H., Gay, G., Davidson, B., & Ingraffea, A. (2007). Social networks, communication styles, and learningperformance in a CSCL community. Computers & Education, 49(2), 309–329.Cress, U. (2008). The need for considering multilevel analysis in CSCL research—An appeal for the use ofmore advanced statistical methods. International Journal of Computer-Supported Collaborative Learning,3, 69–84.Creswell, J. W. (2007). Qualitative inquiry and research design: Choosing among five approaches. ThousandOaks, CA: Sage Publications.Dillenbourg, P., Jarvela, S., & Fischer, F. (2009). The evolution of research on computer-supportedcollaborative learning. In N. Balacheff (Ed.), Technology-enhance learning. New York: Springer.CSCL 2015 Proceedings321© ISLSDringus, L. P., & Ellis, T. (2005). Using data mining as a strategy for assessing asynchronous discussion forum.Computers and education, 45, 141–160.Erkens, G., Jaspers, J., Prangsma, M., & Kanselaar, G. (2005). Coordination processes in computer supportedcollaborative writing. Computers in Human Behavior, 21(3), 463–486.Fuks, H., Pimentel, M., & Jose, C. (2006). R-U-Typing-2-Me ? Evolving a chat tool to increase understandingin learning activities, 117–142.Guldberg, K., & Pilkington, R. (2006). A community of practice approach to the development of non-traditionallearners through networked learning. Journal of Computer Assisted Learning, 22(3), 159–171.Holmes, J. (2007). Designing agents to support learning by explaining. Computers & Education, 48(4), 523–547.Hrastinski, S., & Keller, C. (2007). An examination of research approaches that underlie research oneducational technology: A review from 2000 to 2004. Journal of Educational Computing Research, 36(2),175–190.Jeong, H., Hmelo-Silver, C. E., & Yu, Y. (2014). An examination of CSCL methodological practices and theinfluence of theoretical frameworks 2005–2009. International Journal of Computer-SupportedCollaborative Learning, 9(3), 305-334.Johnson, B., & Christensen, L. (2008). Educational research: Quantitative, qualitative, and mixed approaches.Thousand Oaks, CA: Sage.Kirschner, P. A., & Erkens, G. (2013). Toward a Framework for CSCL Research. Educational Psychologist,48(1), 1–8.Lund, K., Molinari, G., Sejourne, A., & Baker, M. (2007). How do argumentation diagrams compare whenstudent pairs use them as a means for debate or as a tool for representing debate? International Journal ofComputer-Supported Collaborative Learning, 2, 273-295.Maslow, A. H. (2002). Psychology of science: A reconnaissance. Maurice Bassett Publishing.Meier, A., Spada, H., & Rummel, N. (2007). A rating scheme for assessing the quality of computer-supportedcollaboration processes. International Journal of Computer-Supported Collaborative Learning, 2(1), 63–86.Mirza, N. M., Tartas, V., Perret-Clemont, A., & de Pietro, J-F. (2007). Using graphical tools in a phased activityfor enhancing dialogical skills: An example with Digalo. International Journal of Computer-SupportedCollaborative Learning, 2, 247-272.Onwuegbuzie, A. J., & Leech, N. L. (2006). Linking research questions to mixed methods data analysisprocedures. The Qualitative Report, 11(3), 474–498.Parnafes, O. (2007). What does "fast" mean? Understanding the physical world through computationalrepresentations. The Journal of the Learning Sciences, 16(3), 415-450.Roschelle, J., Bakia, M., Patton, C., & Toyama, Y. (2011). Eight issues for learning scientists about educationand the economy. The Journal of the Learning Sciences, 20(3), 3–49.Rummel, N., & Spada, H. (2005). Learning to collaborate: An instructional approach to promoting collaborativeproblem solving in computer-mediated settings. Journal of the Learning Sciences, 14(2), 201–241.Schrire, S. (2006). Knowledge building in asynchronous discussion groups: Going beyond quantitative analysis.Computers & Education, 46(1), 49–70.Schwarz, B. B., & Groot, R. (2007). Argumentation in a changing world. International Journal of ComputerSupported Collaborative Learning, 2(2-3), 297–313.Shadish, W. R., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causalinference. New York: Houghton Mifflin Company.Sitthiworachart, J., & Joy, M. (2008). Computer support of effective peer assessment in an undergraduateprogramming class. Journal of computer assisted learning, 24(217–231).Smith, B. K., & Reiser, B. J. (2005). Explaining behavior through observational investigation and theoryarticulation. The Journal of the Learning Sciences, 14(3), 315–360.Stahl, G. (2013). Learning across levels. International Journal of Computer Supported Collaborative Learning,1, 1–12.Strijbos, J., & Stahl, G. (2007). Methodological issues in developing a multi-dimensional coding procedure forsmall-group chat communication. Learning and Instruction, 17, 394–404.Suthers, D. D., Lund, K., Rose, C., Teplovs, Chris, & Law, N. (2013). Productive multivocality in the analysisof group interactions. Netherlands: Springer.Trow, M. (1957). Comment on “Participant Observation and Interviewing : A Comparison.” HumanOrganization, 16(3), 33–35.Van Aalst, J., & Chan, C. K. K. (2007). Student-directed assessment of knowledge building using electronicportfolio. The Journal of the Learning Sciences, 16(2), 175–220.CSCL 2015 Proceedings322© ISLSWhat Works Clearinghouse. (2010). Procedures and standards handbook (Version 2.1).Yin, R. K. (2009). Case study research: Design and method. Thousand Oaks, CA: Sage Publications.AcknowledgmentsThis research was funded in part by the National Research Foundation of Korea (Grant No. 20090068919) and also by the US National Science Foundation (Grant No. 1249492). Any opinions, findings, andconclusions or recommendations expressed in this material are those of the author(s) and do not necessarilyreflect the views of the funding agencies.CSCL 2015 Proceedings323© ISLS