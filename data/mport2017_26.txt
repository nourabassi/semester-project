Making the Invisible Visible: A New Method for Capturing StudentDevelopment in MakerspacesRichard Lee Davis, Stanford University, rldavis@stanford.eduBertrand Schneider, Harvard University, bertrand_schneider@gse.harvard.eduPaulo Blikstein, Stanford University, paulob@stanford.eduAbstract: The contribution of this paper is twofold: we introduce a new kind of assessmentdeveloped to capture students’ learning in makerspaces, and we present a new perspective onhow participating in a maker workshop impacts students. As opposed to traditional pen andpaper tests, we designed a series of hands-on task that participants complete before and after amaker workshop. In this paper, we contrast high-school students’ performance with experts(graduate students in mechanical engineering) and found evidence that the students’ behaviorbecame more similar to experts’ after participating in a maker workshop. For the scope of thispaper, we focus on a single task and describe in detail our coding scheme and analyses.Additionally, we show how a combination of qualitative and computational analysis helped usdevelop metrics to compare novices’ and experts’ performances. We conclude by discussingthe potential of this type of assessment for supporting students’ learning in makerspaces.IntroductionInformal communities of tinkerers, inventors, hackers, and builders have existed across the world for decades.The most famous of these are practically household names—the Homebrew Computer Club in Silicon Valley, theChaos Computer Club in Berlin—but the vast majority are small communities meeting in garages, classrooms,basements, and libraries. In the past decade, a common identity has emerged for the members of these disparategroups: the maker. Since the emergence of Make magazine in 2005, the word “make” has become the dominantsignifier of this new community. The maker movement is made up of a community of makers, who gather inmakerspaces to take part in a common activity: making. As the maker movement spreads widely across the USand Europe, we are beginning to see a growing interest in bringing making into K-12 schools (Halverson &Sheridan, 2014). And rightfully so; many aspects of the maker movement have the potential to disrupt traditionalschooling and positively impact K-12 students. The communities of practice that take root in makerspaces arepowerful, authentic learning environments (Blikstein, 2013; Halverson & Sheridan, 2014; Peppler & Bender,2013). The focus on creating complex, personally meaningful artifacts has known learning benefits (Blikstein &Krannich, 2013; Papert, 1980; Piaget, 1973). However, one notable aspect of the maker movement with importanteducational potential has escaped investigation so far: the maker mindset.From an educational perspective, the maker mindset is one of the most compelling aspects of the makermovement. Its existence implies that becoming a maker involves more than learning how to create products; itinvolves a change in one’s view of the world. If we wanted to identify the maker mindset, what would we lookfor? Because the maker mindset has many definitions and descriptions, it is difficult to know where to start. DaleDoherty, the founder and CEO of Maker Media, Inc., defines the maker mindset as a “can-do attitude… aninvitation to take ideas and turn them into various kinds of reality… It is a chance to participate in communitiesof makers of all ages by sharing your work and experience. Making can be a compelling social experience, builtaround relationships” (Dougherty, 2013). Martin writes that it is “playful, asset- and growth-oriented, failurepositive, and collaborative” (Martin, 2015).Here, we are interested in a more focused definition of the maker mindset. In previous work, it has beenobserved that students seem to be more capable of reasoning about and debugging complicated mechanisms afterparticipating in a making workshop (Blikstein, 2013). Fields et al. developed an assessment called a Debuggemto capture this change (Fields, Searle, Kafai, & Min, 2012), finding that after a four-week electronic textilesworkshop, students were more able to fix faulty designs like short circuits, poor crafting, and incorrect code. Weview this change as part of the shift towards a maker mindset. More specifically, our interpretation of thesefindings is that after taking part in a maker workshop, some students have learned to think more like engineers.Although this type of thinking is difficult to find in common descriptions of the maker mindset, we feel it is animportant outcome with special relevance for K-12 education.The current study was designed to answer three questions about students’ participation in a makerworkshop. First, do students think more like engineers as a result of taking part in a long-term maker workshop?Second, is there a way to reliably and efficiently capture this change? And third, if this change in thinking doesCSCL 2017 Proceedings175© ISLSoccur, can we look more closely at the data and begin to understand the specific ways in which the students’thinking changes?General description of the studyA class of high-school seniors took part in a workshop in our FabLab for several months (Fig. 1). Since theworkshop targeted students’ understanding of complex mechanical systems, we designed assessments that wouldcapture their ability to build, fix or debug them. Before the workshop, we administered 3 tasks to the highschoolers and they completed 3 similar tasks after the workshop. Once the workshop was over, we recruited 18experts (graduate students in mechanical engineering) to complete all six tasks from the pre-test and post-test.Figure 1. Timeline of the study. On the left, three examples of tasks used before and after the workshop toassess its effect on students’ mindset (described in details in the methods section). In the middle, an example ofan Omni-Animal laser cutting project. On the right, the final Rube Goldberg machine.Hypotheses: We expect to see three trends in our data based on participants’ performances at the pre- and posttest: 1) novices should improve from pre to post-test; 2) experts should perform significantly better thannovices; and 3) novices’ behavior should become more similar to experts at the post-test.MethodsSubjects20 high-school seniors (4 males, 16 females) from a low-SES school took part in a workshop organized in ourFabLab. Three female students had to be excluded from our analyses because they dropped out of the workshop.18 graduate students (9 females and 9 males, mean age=24.67, SD=2.13) in mechanical engineering from an R1university were asked to complete the study tasks as a comparison group. They received a $20 gift card for theirparticipation. We will refer to high-schoolers as “novices” and mechanical engineers as “experts” henceforth.InterventionThe students took part in a year-long workshop in our FabLab that stretched across two two-month blocks (Figure1). During each block, the students worked in the lab for 1.5 hours per day, two times a week. In the first twomonths, the students worked in pairs on a project called the Omni-Animal. During this project students learnedhow to design their own three-dimensional creature using computer-aided drawing (CAD) software (Figure 1).Each creature was constructed from 6-14 individual pieces, which the students cut out of a piece of plywood usingthe laser cutter. Due to the difficulty of translating between two and three dimensions, none of the students’ initialdesigns were successful. The students encountered problems like making slots too wide, failing to take thethickness of the material into account, and making pieces too big or too small. All of the students iterated on theirinitial designs multiple times and ultimately completed a successful Omni-Animal. In the second two-monthperiod, the students worked in groups to create an electro-mechanical Rube Goldberg machine (Figure 1). Thestudents worked in groups of two to three with each group working on one part of the machine. Each group createda laser-cut mechanism paired with a microcontroller platform called the GoGo board (Sipitakiat et al., 2004) toadd motion, react to the environment, or create an effect.CSCL 2017 Proceedings176© ISLSMaterialsWe used three tasks as a pre-test and three tasks as a post-test. The first set (A) is shown in the first column ofTable 1, and the second set (B) is shown in the right column. Task 1 is about rebuilding a gear system: 1a is adifferential, and 1b a motor. Task 2 is about fixing a broken system: 2a is a flashlight where the bulb is brokenand the batteries are inverted; 2b is a remote control where the batteries and spring are inverted, the fan is switchedoff. Task 3 is about reconfiguring a micro-controller: the sensor and actuator are plugged into the wrong ports.Those tasks went through several iterations before we were confident that they could capture students’learning. More specifically, since the laser cutting project was about creating three-dimensional creatures fromflat pieces of wood, we expected that students would increase their ability to assemble complex mechanical objectsfrom their individual pieces. They also learned to use the GoGo board, which is an easy to use micro-controllerwith plug-and-play sensors and actuators and block-based programming environment. Since many everydaydevices include a similar input-output structure (e.g., a button / light sensor / motion sensor detects an event, afew lines of code analyze the data, and the system triggers a response with a motor or a speaker), we hypothesizedthat the students would be better equipped to reason about, troubleshoot, and repair everyday devices. Finally, theRube Goldberg project was designed to convey the idea that complex systems are more than the sum of theirparts: during the workshop, each group built one step of the machine using the laser cutter and a GoGo board. Allsix tasks described in Table 1 tried to capture those three aspects to various extent.Table 1: the six tasks given to our participants (a smaller image on the bottom left shows the final product fortask 1 or a step toward fixing the system for task 2). For the scope of this paper, we focused on task 1a.Task 1a – Assembly Puzzle (Gearbox)Task 1b – Assembly Puzzle (Motor)Task 2a (left)– Fixing an everyday object (Flashlight)Task 2b (left) – Fixing an everyday object (remote control)Task 3a (right)– Reconfiguring a Micro-ControllerTask 3b (right) - Reconfiguring a Micro-ControllerDesignHalf of the novices completed set A as a pre-test and set B as a post-test. We counter-balanced those tasks for theother half of the participants. In other words, half of the novices completed tasks 1a, 2a and 3a for the pre-test andtasks 1b, 2b and 3b for the post-test. The other half of the novices completed tasks 1b, 2b and 3b for the pre-testand tasks 1a, 2a and 3a for the post-test. The experts completed all six tasks in a single session, and we alsocounter-balanced set A and B for them.ProcedureNovices were asked to leave the workshop for a short period of time to complete the study. Because of timeconstraints, they were run in pairs sitting side by side at a 3’ by 6’ table. Each participant was given their ownpuzzle to work on. They were instructed not to work together on the puzzles or to look at what the other personCSCL 2017 Proceedings177© ISLSwas working on. First, an Assembly Puzzle was placed in front of each participant on a wooden board (task 1).The participants were told that the object in front of them had been disassembled, that it was their job to try andput it back together in five minutes, and that they should try their hardest and not be frustrated if they were unableto solve the puzzle. They were not given any further information about the object (i.e., no instructions on how toassemble the object). Participants were instructed not to touch the puzzle until the timer was started. Once thetimer was started they had five minutes to try and solve the assembly puzzle. When the time expired, the puzzlewas removed from the table. Next, the repair puzzle was placed in front of the participant (task 2). The participantswere given four minutes to work on this puzzle. When the timer expired, the repair puzzle was removed from thetable, and the reconfiguration puzzle was placed on the table (task 3). They had three minutes to work on thereconfiguration puzzle. When the time expired, the participants were thanked for their time and left the room.Since there was no time constraint associated with participating to a workshop, experts were run one ata time and worked through all six tasks in a single session. Half of the experts worked on the tasks from set Afirst, followed by the tasks from set B (1a-2a-3a-1b-2b-3b), while the other half did the tasks from set B followedby set A (1b-2b-3b-1a-2a-3a). The time-per-task was the same for experts and novices. Video, audio, and bodyposition (using a Kinect sensor) were recorded for the duration of the study.Video codingWe had two major goals which resulted in the development of two distinct video coding schemes. First, we neededa way to determine how close participants came to the correct solution. The 11-point assessment scale wasdeveloped for this purpose. Second, we needed a way to model the participants’ full sequence of meaningfulactions as they worked through each puzzle. We designed the time-based coding scheme for this purpose.The 11-point assessment scaleIn order to meaningfully compare pre-workshop students, post-workshop students, and experts, it was importantto develop a metric that accurately measured how close each participant came to the correct solution. However,most of the participants—including experts—made significant progress on the problem but failed to fullycomplete the gearbox problem in such a short amount of time (5 minutes), so we needed a more nuanced way ofmeasuring progress than percentage of participants who completed the puzzle. Through an iterative coding processwe created an 11-point time-agnostic scale for this purpose.Coding videos with this metric occurred as follows. While watching the video, if the participantperformed an action that matched one of the codes, we assigned a 1 to that code. If the participant’s action matcheda code partially, we assigned a 0.5 to that code. No time information was recorded (hence time-agnostic); that is,at the end of the video, two participants who carried out the same actions in different orders would receive thesame score. At the end of the video, we summed up the scores across codes and assigned this to the participant.A score of 0 meant the participant made no progress on the problem, while a score of 11 meant the participantcompletely solved the problem. The higher the score, the closer to finishing successfully.The time-based coding schemeSince the 11-point assessment scale did not capture any temporal information, we designed a second time-basedvideo coding scheme to categorize the different types of actions participants carried out while attempting to solvethe gearbox problem. This coding scheme allowed us to analyze how sequences of actions differed betweengroups. The final coding scheme contained 14 codes that belonged to 4 categories: planning, evaluation, context,and action. The coding scheme complements the 11-point assessment scale and was designed so that it could betranslated into other coding schemes used in similar analyses (Tschan, 2002; Worsley & Blikstein, 2013).This time-based coding scheme went through a number of iterations. Initially, we coded the exact stateof the gearbox as participants worked on the problem. Each code consisted of a set of pieces with an optional subcode indicating if any pieces were added or taken away in that moment. A major flaw of this coding scheme wasthe inability to distinguish between a correctly-assembled set of parts and an incorrectly-assembled set of parts.Ultimately, this coding scheme proved to be too noisy for any useful analysis. The next iteration of our codingscheme treated the participants’ actions in a more general way. Instead of hundreds of possible codes, we narroweddown the meaningful actions to 9 codes: exploring, looking, rotating, plastic connection, magnetic connection(correct), magnetic connection (incorrect), meshing gears, disassembling, and placing an axle in a hole or bracket.Finally, in order to make our coding scheme cross-compatible with other coding schemes of interest(Tschan, 2002; Worsley & Blikstein, 2013), we added new codes and further refined the existing ones. The finalcoding scheme contained 14 codes in 4 categories. We designed custom software to streamline the process ofcoding the videos. Each time the participant carried out an action, the appropriate code was entered and linked tothe video using a timestamp (Figure 2). After coding a participant’s video, we were left with a full sequence ofCSCL 2017 Proceedings178© ISLSthe participant’s actions during the problem. In other words, we transformed video of the participants’ actions intoa time-stamped sequence of codes.Figure 2: an example of a sequence of actions at different time points coded with the Time-Based CodingScheme. From left to right: axle, rot(ate), plas(tic connection), mesh(ing gears), axle, mag(netic connection).ResultsHypotheses 1 (novices’ improvement from pre to post) and 2 (experts vs novices)For hypotheses 1 and 2, we used the 11-Point Assessment Scale described in the coding section to assign eachparticipant a score between 0 (no progress) and 11 (finished solution) on the Gearbox task (1a). We first visuallyexplored our dataset using boxplots (Fig. 3, left side). Half of the novices completed the gearbox task before theworkshop, and the other half completed it afterwards. All experts completed the task. The descriptive statisticsare as follows: for the novices on the pre-test, N = 9, mean = 2.5, SD = 2, for the novices on the post-test, N = 8,mean = 3.69, SD = 1.85, and for the experts N = 18, mean = 6.97, SD = 2.45. The boxplots also revealed thatthere was an outlier among the novices in the pre-test. One student scored a 7 while the second best student scoreda 3. This particular participant had worked in the lab full-time as an assistant for 3 months preceding the workshopand was excluded from the following analyses. To test our first hypothesis (whether our novices improved frompre to post-test), we used an ANOVA to compare the novices’ performance before and after the workshop. Wefound that a significant improvement from pre to post: F(1,14) = 5.17, p < 0.05, Cohen’s d = 1.14. To test oursecond hypothesis (whether experts performed better than novices), we grouped pre- and post-tests for novicesand used an ANOVA to compare them to the experts. We found that experts did significantly better at the gearboxtask than novices: F(1,34) = 27.00, p < 0.001, Cohen's d = 1.82.Figure 3. Left: Boxplot of the novices’ scores on the 11-point scale for the pre and post-test compared toexperts’ scores on the gearbox task. Right: Composition of the novice and expert groups found by clustering onproblem-solving sequences.Hypothesis 3: novices’ similarity to experts after the workshopSince one of the goals of the study was to see if novices became more like experts after taking part in a long-termworkshop in the FabLab, we needed a way to capture the similarity between any two problem-solving sequences.We used two different techniques to test this hypothesis: first, we translated sequences into an existing codingscheme (Tschan, 2002) to estimate how many “ideal cycles of cognition” (described below) novices and expertswent through. Second, we used computational techniques to compute a similarity score between all participantsand used an unsupervised clustering algorithm to separate participants into groups based on their problem-solvingsequences. The main idea behind both analyses is to see whether novices at the post-test start to display more“expert-like” behaviors.CSCL 2017 Proceedings179© ISLSIdeal cycles of cognitionTschan (2002) found that individuals and groups of students performed better on a problem-solving task whenthey completed a higher number of “ideal cycles of cognition”. An ideal cycle is composed of three steps:planning, acting, and evaluating. We collapsed the 14 codes of the time-based coding scheme as follows:planning (fdis, look, org), acting (axle, axlex, dis, disx, mag, magx, plas) and evaluating (mesh, meshc, rot, test).This distinction is obviously not ideal, because the planning and evaluating phases overlap. But as a preliminaryresult, we found a trend similar to the left plot of Figure 2 (see boxplots on Fig. 5).An ANOVA revealed a significant difference between novices and experts: F(1,34) = 14.45, p < 0.001,Cohen's I = 1.43 (novices mean=1.07, SD=0.77; experts mean=2.81, SD=1.55). For novices, there was a trendin the same direction (although non-significant): F(1,14) = 3.10, p = 0.10, Cohen's d = 0.99 (pre-test mean=0.75,SD=0.83; post-test mean=1.43, SD=0.49).Figure 5. boxplots showing the number of ideal cycles of cognition for novices and experts.Clustering on raw actionsBy clustering participants according to their entire set of actions on the gearbox problem, we were ableto identify two groups of participants. One group contained 13 participants, composed of all of the pre-workshophigh-school students, 62.5% of the post-workshop high-school students, and 6% of the experts. The other groupcontained 19 participants, composed of zero pre-workshop high-school students, 37.5% of the post-workshopsstudents, and 94% of the experts (Fig. 3, right side).We used the R package TraMineR for clustering participants on their raw actions (Gabadinho, Ritschard,Mueller, & Studer, 2011). By computing the edit distance between all pairs of sequences using TraMineR’soptimal matching algorithm, we were able to construct a symmetric distance matrix that captured the similaritybetween all pairs of participants. That is, for all pairs of participants, we computed a single index that capturedthe similarity of their problem-solving trajectories. Finally, we used agglomerative hierarchical clustering(Maechler, Rousseeuw, Struyf, Hubert, & Hornik, 2016) to separate out the two groups of participants who weremost similar to each other.Based on the compositions of these groups, we labeled the first group affordance-aware and the secondgroup affordance-blind. We chose these labels based on the most prevalent actions within each group. Theparticipants in the affordance-aware group carried out a higher proportion of axle-related actions, gear-meshingactions, correct magnetic connections, and rotations (Figure 4). In comparison, the affordance-blind group carriedout a higher proportion of incorrect magnetic connections and incorrect plastic connections (Figure 4). Moreimportantly, the affordance-blind group carried out almost zero axle, gear, or rotation actions. The affordanceblind group seemed unable to perceive the important affordances of the pieces, while the affordance-aware grouptook these into account when solving the gearbox problem.The only sub-group split between the two clusters were the post-workshop high-school students. Ourinterpretation for this split is simple: the workshop had a positive effect. After participating in the workshop,nearly half of the students were more similar in their problem-solving behavior to experts than to pre-workshophigh-school students. To validate the integrity of the two clusters, we compared each cluster’s scores on the timeagnostic 11-point assessment scale using a two-tailed t-test. Not only was there a strongly-significant differencebetween the affordance-blind group (mean=2.35, sd=1.20) and the expert-like group (mean=6.89, sd=2.00)(t(28.23)=7.89, p < 0.001), but the difference in means was even-larger than the previous tested difference (effectsize of previous test is d=1.97, while effect size of current test is d=2.65) (Figure 4).CSCL 2017 Proceedings180© ISLSFigure 4. Top: Frequency of actions by cluster. Bottom: Proportion of actions at each time step for each cluster.Note the higher proportion of axle-related actions (green), meshing gears (dark purple), and rotation (fushia) inthe Affordance-Aware cluster, and the higher proportion of incorrect magnetic connections (sky blue) andincorrect plastic connections (beige) in the Affordance-blind group.DiscussionThe motivation for this work was to develop a new methodology for capturing students’ learning in makerspaces.We have designed a new kind of task-based assessment where students had to rebuild a gear system, fix everydaydevices, and debug a microcontroller. These tasks were designed to assess skills that participants learned duringthe workshop, such as constructing a 3D object from 2D parts, building an input-output system, and understandingthat complex systems are made of small interconnected parts.For our preliminary analyses, we have focused on the first task of this assessment (the gearbox). Wedeveloped an extensive coding scheme to estimate students’ performance and found that participants significantlyimproved from pre- to post-test. This suggests that the workshop had a positive effect on their ability to reasonabout and assemble complex mechanisms. Additionally, we asked experts (graduate students in mechanicalengineering) to complete the same tasks and found that novices became more like experts in their perception ofthe salient features of the problem and in their problem-solving approach. This finding suggests that novices’improvement actually reflects one (or several) skills that experts have gained through many hours of studying andinteracting with mechanical systems. It makes us more confident that we are capturing an important aspect ofwhat constitutes an engineer.These results are promising, but they are also preliminary: we have only analyzed one task out of six.The main reason for this narrow focus is that the coding schemes took a significant amount of time to developand apply. Next, we plan to analyze the 5 other tasks and attempt to replicate the findings of this paper. Anotherlimitation of this work is the fact that by limiting our analysis to a single task we were forced to use betweensubject analyses, which considerably reduced our statistical power. Finally, we acknowledge that our tasks onlycapture a small portion of what constitutes an expert. The job of mechanical engineers is vastly more complexthan rebuilding gear systems and fixing everyday devices.CSCL 2017 Proceedings181© ISLSEven with those limitations, we consider our contribution to be a significant advance in capturinglearning in makerspaces. Previous work was mostly limited to qualitative accounts of students’ experiences inthose spaces or traditional pen and paper questionnaires. We designed semi-authentic engineering tasks, and foundpreliminary evidence that participating in a maker workshop had a positive impact on participants. Not only didthey improve from pre to post, but they also became more similar to mechanical engineers in their actions.While this approach can be viewed as a new type of assessment, we also view it as a way to gain a morenuanced understanding of the types of cognitive change that are fostered in maker spaces (and potentially otherunstructured learning environments). In its current form, our approach provides a starting point for other types oftask-based assessments and analyses. We are currently working on a more general framework that providesguidelines for the design of additional tasks and coding schemes. Finally, it is worth mentioning that we alsocollected all of students’ gestures and body postures using a Kinect sensor. Future work includes the analysis ofthis dataset to extract indicators of expertise that would act as a proxy for the coding schemes we developed inthis paper. In the long run, our hope is to automatically collect and process task-independent measures of students’performance using sensors and machine learning algorithms.ConclusionMakerspaces are inherently messy learning environments. In them, students learn a variety of skills: they comeup with original project ideas; they address problems in their communities; they learn to overcome failure; theylearn to communicate and collaborate with their peers; and finally, they learn to think like engineers. These skillsare central to many current (and upcoming) career paths, but it can be difficult to teach them in formal learningenvironments. Furthermore, it has proven difficult to capture these changes using traditional methods. This paperintroduces a new method for capturing these changes though a combination of task-based assessments andqualitative/computational analysis techniques. Using this method, we found that students who took part in a makerworkshop became more like engineers in their ability to reason about and solve complex problems. Morespecifically, the students learned to recognize the functional affordances of complex mechanisms—that wheelsare for rotating and gears are for meshing. This shift in perspective is an important, empowering educationaloutcome, and provides new motivation for studying the educational impact of fostering a maker mindset in youth.ReferencesBlikstein, P. (2013). Digital fabrication and “making”in education: The democratization of invention. FabLabs:Of Machines, Makers and Inventors, 1–21.Blikstein, P., & Krannich, D. (2013). The makers’ movement and FabLabs in education: experiences,technologies, and research. In Proceedings of the 12th international conference on interaction designand children (pp. 613–616). ACM. Retrieved from http://dl.acm.org/citation.cfm?id=2485884Dougherty, D. (2013). The maker mindset. Design, Make, Play: Growing the next Generation of STEMInnovators, 7–11.Fields, D. A., Searle, K. A., Kafai, Y. B., & Min, H. S. (2012). Debuggems to Assess Student Learning in eTextiles (Abstract Only). In Proceedings of the 43rd ACM Technical Symposium on Computer ScienceEducation (pp. 699–699). New York, NY, USA: ACM. https://doi.org/10.1145/2157136.2157367Gabadinho, A., Ritschard, G., Mueller, N. S., & Studer, M. (2011). Analyzing and Visualizing State Sequencesin R with TraMineR. Journal of Statistical Software, 40(4), 1–37.Halverson, E. R., & Sheridan, K. M. (2014). The Maker Movement in Education. Harvard Educational Review,84(4), 495–504.Maechler, M., Rousseeuw, P., Struyf, A., Hubert, M., & Hornik, K. (2016). Cluster: Cluster Analysis Basics andExtensions.Martin, L. (2015). The Promise of the Maker Movement for Education. Journal of Pre-College EngineeringEducation Research (J-PEER), 5(1). https://doi.org/10.7771/2157-9288.1099Papert, S. (1980). Mindstorms: children, computers, and powerful ideas. New York: Basic Books.Peppler, K., & Bender, S. (2013). Maker movement spreads innovation one project at a time. Phi Delta Kappan,95(3), 22–27.Piaget, J. (1973). To understand is to invent: The future of education. Retrieved http://philpapers.org/rec/PIATUITschan, F. (2002). Ideal Cycles of Communication (or Cognitions) in Triads, Dyads, and Individuals. Small GroupResearch, 33(6), 615–643. https://doi.org/10.1177/1046496402238618Worsley, M., & Blikstein, P. (2013). Towards the development of multimodal action based assessment. InProceedings of the third international conference on learning analytics and knowledge (pp. 94–101).ACM. Retrieved from http://dl.acm.org/citation.cfm?id=2460315CSCL 2017 Proceedings182© ISLS