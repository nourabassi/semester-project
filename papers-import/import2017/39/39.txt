How Technology and Collaboration Promote Formative Feedback:A Role for CSCL Research in Active Learning InterventionsSally P. W. Wu, University of Wisconsin – Madison, pwwu@wisc.eduMartina A. Rau, University of Wisconsin – Madison, marau@wisc.eduAbstract: Recent evidence for the effectiveness of active learning interventions has led educators to advocate for widespread adoption of active learning in undergraduate science, technology, engineering, and mathematics courses. Active learning interventions implement technology and collaboration to engage students actively with the content. Yet, it is unclear how thesefeatures contribute to their effectiveness. Research suggests that these features may enhancelearning by providing formative feedback. To understand how technology and collaborationsupport learning by providing formative feedback, we conducted an observational study in atraditional and an active learning version of an undergraduate chemistry course. Results suggestthat technology-provided feedback in the active learning intervention enhanced collaboration.We identify specific challenges and opportunities in technology design and active learning interventions for computer-supported collaborative learning research to address.IntroductionRecent research in undergraduate science, technology, engineering, and mathematics (STEM) education showsthat active learning is more effective than traditional lectures (Freeman et al., 2014). This research describesactive learning as a broad range of interventions in which students learn through activities and/or discussion inclass, whereas in traditional instruction, students learn through passively listening to a lecture (Freeman et al.,2014). In accordance with the disciplinary-based STEM education research, recent interventions often involve theuse of technology and collaboration to foster student learning from peers (Eddy, Converse, & Wenderoth, 2015;Lom, 2012). However, this active learning research has produced little theory about why and how technologiesand collaboration can support student learning when compared to traditional undergraduate STEM courses.One potential reason why active learning interventions are effective is that technologies and collaborationprovide formative feedback. In traditional STEM courses, instructors typically provide summative correctnessfeedback based on the content, but they provide little formative feedback on how to learn the content. Accordingto Sadler (1989), formative feedback helps students (1) understand what expert performance looks like and (2)assess their current performance, so that they can take measures to (3) bridge the gap between current and expertperformance. Indeed, formative feedback has been shown to enhance student learning by providing correctnessand corrective guidance about progress towards expert performance (Hattie & Timperley, 2007). Active learninginterventions produce more formative feedback because of their use of technology and collaboration (Eddy et al.,2015), yet it is an open question whether students benefit from the additional feedback. Investigating how studentsuse feedback provided in active learning interventions may provide insight into why active learning is effectiveand inform the design of technology-enhanced collaborative learning in such interventions.Gaps in prior research gives rise to the question we examine in this paper: how does technology use andcollaboration in active learning interventions support students’ learning by providing formative feedback, as compared to common practice in traditional instruction? To address this question, we conducted an observationalstudy of a traditional and an active learning version of discussion sections in an undergraduate chemistry course.Active learning in STEM instructionRecent evidence for the effectiveness of active learning interventions has instigated widespread interest in implementing active learning in STEM courses, instead of passive lectures (Eddy et al., 2015). The editor of Journal ofChemical Education stated, “to put it bluntly, everyone should be taken off the control (i.e., traditional lecture)and switched to the treatment (i.e., carefully considered active learning methodologies)” (Pienta, 2015, p. 1).Indeed, chemistry education research has shown that active learning interventions lead to significantly higherlearning outcomes than traditional lectures (Mahalingam, Schaefer, & Morlino, 2008; Paulson, 1999).Active learning interventions emphasize learning by doing (Chi, 2009). Typical active learning interventions involve technologies such as clickers, online practice problems, or simulations, and collaboration such asdiscussions, problem-based workshops, and roundtables (Eddy et al., 2015). Prior research on active learning hasfocused on the effects of implementing these features in lectures held in auditorium-style classrooms (Lom, 2012).To supplement lectures, STEM courses often include discussion sections (also known as recitations orreview sessions). Discussion sections provide opportunity for an instructor to engage with a smaller group ofCSCL 2017 Proceedings279© ISLSstudents and answer students’ questions about the course content. Discussion sections are assumed to naturallyfoster discussion, but research shows that they often foster passive learning found in traditional lectures becausestudents can copy solutions as the instructor solves them (Mahalingam et al., 2008; Paulson, 1999). Therefore, itis important that we understand how to design discussion sections so that they actively engage students withcourse content. Most research on active learning interventions has focused on lecture settings, not on discussionsettings (Eddy et al., 2015; Lom, 2012). Therefore, we know little about how active learning interventions changelearning processes in discussion sections. To address this gap, our study compares traditional and active learninginterventions implemented in discussion sections, particularly on the formative feedback provided.Components of formative feedbackFormative feedback can guide student interactions by indicating correctness on students’ performance and providing corrective redirection for students’ progress. In general, “[f]eedback is commonly defined in terms of information given to the student about the quality of performance” (Sadler, 1989, p. 142). This definition is in linewith other definitions of feedback that convey passing of information to the learner by instructors or instructionaltechnologies (Shute, 2008; Van der Kleij, Feskens, & Eggen, 2015). Effective forms of formative feedback takeinto account students’ needs for redirection (Hattie & Timperley, 2007). Sadler (1989) proposed that formativefeedback to improve student performance should support students in: (1) understanding expert performance, (2)assessing their current performance, and (3) bridging the gap between current and expert performance.Unless these three components are supported, students cannot act upon the feedback to improve learning.Specifically, without (1) an understanding of expert performance, students may proceed in unproductive directions. Without (2) an assessment of their current performance, students may not identify the gaps in their understanding. Without (3) the ability to bridge the gap, students cannot improve their current performance towardsexpert performance. The latter component may explain why “even when teachers provide students with valid andreliable judgments about the quality of their work, improvement does not necessarily follow” (Sadler, 1989, p.119). Instructors must provide correctness and corrective feedback that help students improve their performance(Mahalingam et al., 2008; Michael, 2006), and students must act upon the feedback (Hattie & Timperley, 2007).Technologies offer an effective way to provide formative feedback. For instance, technologies can provide practice problems with immediate feedback and detailed explanations that help students assess and bridgetheir performance in relation to expert performance. Immediate feedback from technologies has shown to be effective at enhancing student learning (Van der Kleij et al., 2015). Further, collaboration offers opportunities forformative feedback. When students work with peers, they have numerous opportunities to give and receive correctness and corrective feedback (Hattie & Timperley, 2007). Our present study seeks to understand the role oftechnologies and collaboration for providing formative feedback in traditional and active learning interventions.Active learning interventions and computer-supported collaborative learningResearch on active learning has been mostly situated in discipline-based education literature. These literaturesfocus on practice-oriented recommendations as to how best to implement active learning interventions. They recommend incorporating technology into STEM courses because it can increase accountability to specific tasks,reduce apprehension by anonymizing responses, and address multiple student responses all at once (Eddy et al.,2015; Lom, 2012). Further, they recommend incorporating collaboration into STEM courses because working ingroups can foster deeper understanding of the content (Eddy et al., 2015; Mahalingam et al., 2008). However,discipline-based education research has failed to provide a theoretical explanation of why and how technologyand collaboration interact in enhancing students’ ability to actively engage with learning materials.Therefore, our research examines how technologies and collaboration interact by providing formativefeedback that can help students learn. This research is of relevance to the field of computer-supported collaborative learning (CSCL) for several reasons. First, our research seeks to uncover how the use of technologies in activelearning interventions helps students collaborate. This may reveal insight into technology development for CSCLthat meets the demands of active learning interventions. Second, our research seeks to uncover how collaborationhelps students benefit from technology feedback. This may reveal additional opportunities for structuring collaborative activities in a way that helps students bridge the gap between their own and expert performance. To addressthese questions, we conducted an observational study to examine how technologies and collaboration supportstudents’ learning through formative feedback in traditional and active learning discussion sections.MethodWe situated our observational study in an introductory accelerated chemistry course taught at a large Midwesternuniversity in Fall 2015. The course involved three weekly lectures, a weekly laboratory, and a weekly discussionsection. We observed two discussion sections taught by teaching assistants (TAs): a traditional discussion sectionCSCL 2017 Proceedings280© ISLSthat involved problem-solving activities without technology support and an active learning discussion section thatinvolved problem-solving activities and incorporated technology support and collaboration.SettingTraditional discussion sections were held in classrooms in the Chemistry building with a table for the TA at thefront, individual desks in rows oriented towards the front, and chalkboards and periodic tables on the walls.Twelve of the 16 discussion sections of the chemistry course were held in this traditional setting. The traditionalsections emphasized problem solving through activities that asked students to engage with course content (e.g.,worksheets). The TA circulated the room to monitor students’ progress, provide feedback, and answer questions.Students could collaborate on the problems, but received no particular support for collaboration. At the end of thediscussion section, the TA reviewed answers to the worksheets or emailed the answer key to the students.The professor of the course designated four sections as active learning discussion sections. These sections were held in a nearby building that provided active learning spaces. These spaces provided large circulartables with outlets, rolling seats, and whiteboards to facilitate collaboration and use of technology (see Figure 1).The four active learning sections emphasized collaborative problem solving. In these sections, students completedpaper worksheets that guided student activity. Worksheets included six to nine questions related to concepts discussed in a previous lecture. Each question directed students to complete a set of online problems followed by aworksheet question. Online problems provided correctness feedback and detailed explanations. The follow-upworksheet question asked about more complex concepts and encouraged students to collaboratively discuss concepts with their partners, table groups, or the TA.Figure 1. Example classroom space designed for active learning. Image from https://teachingacademy.wisc.edu/teach-in-sterling-clc-this-spring/.Data collection and analysisThe first author observed a traditional section taught by Ted and an active learning section taught by Addie (allnames are pseudonyms) from week two to week eight of the semester. In the traditional section, she sat at a deskin a back corner of the class and observed groups of students who worked together near her desk. In the activelearning section, she sat at a table in the back corner of the classroom and observed a group of six students whosat at the table each week. While observing, she did not participate in student discussions but conducted fly-onthe-wall observations using a pen and notebook. She typed up observations following the discussion sections.In week nine of the semester, the first author conducted interviews with the professor, the TAs (Addieand Ted), and five students (three from Ted’s and two from Addie’s section). Two student interviewees weremembers of observed groups, and the other three sat in other areas of the discussion sections that the first authordid not observe, to provide a comparison of student experiences. Interviews were audio recorded and transcribed.We conducted a bottom-up and top-down analysis of the field notes and interviews (Miles & Huberman,1994). Our bottom-up analysis yielded 33 codes describing student interactions with technology and peers in threeco-occurring categories: who was involved in the interactions (people), how they interacted (social interactions),and what they used (materials). Then, we applied the framework proposed by Sadler (1989) for formative feedbackto co-occurring codes (e.g., TA + give-explanation + chalkboard/whiteboard).ResultsWe organize our results from interviews and observations in relation to components of formative feedback (Sadler, 1989): (1) understanding expert performance, (2) assessing current performance, and (3) bridging the gap.CSCL 2017 Proceedings281© ISLSFeedback component 1: Understanding expert performanceAccording to Sadler (1989), students first need to understand expert performance. Our observations revealed thatinformation about expert performance was often provided in the form of correctness explanations by the TA orby instructional materials (e.g., textbook, answer keys, and online problems). In the active learning section, weobserved students reading explanations in online problems. In both the traditional and active learning sections,we observed TAs giving explanations to the whole class using the board or worksheets. For example, the TA ofthe traditional section, Ted, spent 15 minutes during one session at the chalkboard explaining crystalline solids, atopic that he said was not discussed in lecture. He said that “exposure will come from [the discussion section] andlab” and advised students to “read more about it.” In each discussion section, Ted provided answers and explanations to the whole class, individual students, and groups. Ted said he used his discussion section to “connect[worksheets] with what we [instructors] [are] trying to talk about—probably for the exam, for what [students] aregoing to use in the future.” He believed the goal of “the discussion session is more like trying to deliver what theclass is talking about.” He wanted to provide feedback that helps students understand expert performance becausestudents will be tested on it on exams and potentially in future courses.Similarly, we observed Addie, the TA of the active learning discussion section, circulating the room tomonitor collaboration, troubleshoot issues, check understanding of concepts, and explain concepts that studentsdid not understand. Each time she gave feedback on students’ explanations, she also provided her own explanation. For example, in an interview, a student explained, “sometimes the worksheet does have you like tell [Addie]what your answer was and why, which is good because she is usually very critical about that and if you're sort ofvaguely explaining something, she wants you to do a better job and she’ll go through [the explanation] too.” Thus,both Ted and Addie viewed providing expert feedback as a key aspect of their role as TA.However, Addie believed that her role differed slightly from TAs in traditional sections. As she explainedin her interview: “The setup of the discussion section does impact my role somewhat, so I guess if it wasn'tstructured in the way that it currently was, I would probably spend more time like answering questions […] to seelike [students] had any general questions on material in lecture, or on... like their pre-discussion worksheets andsuch.” In Addie’s section, feedback from online problems helped to answer student questions. The feedback provided expert performance that Addie otherwise would have provided if she was in a traditional section.Students’ understanding of expert performance seemed to be a key aspect of the course. The professorstated in his interview that he and the other instructors try to “make [the course] better preparation for studentswho are going to engineering or bio-medical areas.” He then listed the concepts and skills important to the course.His focus on course content highlights how expert performance is emphasized in the design of the course.Our results suggest that understanding expert performance is prevalent in course design and instructorinteractions. The key difference between traditional and active learning sections was that online problems provided extra support for expert feedback by indicating correctness of responses. Thus, students in the active learning sections received more feedback for understanding expert performance than students in the traditional section.Feedback component 2: Assessing current performanceNext, students need to assess their current performance (Sadler, 1989). Our observations revealed that studentsand TAs assessed students’ performance by checking answers for correctness on worksheets or online problems.Both TAs circulated the room to check students’ answers. As Addie explained in her interview: “If Inotice that […] they got the question wrong, then I might ask them if they understand what's going on, and […]if they don’t have any questions, then I would move onto the next […] table to see if they have any questions.”Addie explained that she stopped to check students’ answers, but that she had to move on to other students tomake sure she addresses all student questions. This suggests that she had limited time for each student and maytherefore not be able to help all students assess their current performance.Our observations suggested that peers can augment TA feedback by helping each other assess their current performance. For example, in Ted’s section, we observed a student, Tammy, helping another student, Tom:Tammy: “What about phosphorus?” (looking over at Tom’s worksheet)Tom: “It’s ….” (trying to explain his answer) “Oh no, no it’s not… ah phosphorus, why you dothat to me?” (he erases work on his paper)This excerpt illustrates that Tom did not realize his error until Tammy asked about his answer and he attemptedto explain it. Our observations showed that feedback from peers often triggered students’ assessment of theircurrent performance. This example also suggests that collaboration can help students self-assess their currentperformance through explaining. In an interview, a student in the active learning discussion section, Colleen,CSCL 2017 Proceedings282© ISLSstated that explaining is “one way [she] like[s] to self-assess, it’s being able to explain it.” The following exampleillustrates a common observation from the active learning discussion section where students explained conceptsto each other and used feedback from online problems to assess their performance on a specific problem:Carl:“2-2NO”Colleen: “Wait”Carl:“I don’t think we need this other box”Colleen: “But we aren’t allowed to put intermediates in the rate law”Carl: “No...”Colleen and Carl stare at the computer.Colleen: “I’ll try one...” (clicks on the computer)Carl:“So then the reaction is dependent on that, right? I think…” (starts to explain his reasoning while drawing on his paper, and then suggests an answer)Colleen: “Oh, I’ll try it” (clicks on the computer)Colleen: “Yes” (announcing that they were correct)In this excerpt, Carl and Colleen were unsure how to approach a problem. Carl found a solution but needed feedback from the online problem to assess whether his understanding and explanation was correct.Further, correctness feedback from online problems can help students address gaps in their understanding. For example, a student explained that online problems helped her because “if you got [a problem] wrong,that’s where you learn a lot because I can go back and […] if I just know the numerical answer, like if it’s 8.3,and my answer is way off, then I could just go back and just try different ways to do it and then the one way thatworks, then I’ll know, ok, this is the technique that I need to use to do this problem. Or I can see where I wentwrong, I guess, and that really helps me ‘cause the next time I do it, I make sure I don’t make that same mistake.”This quote illustrates that correctness feedback from online problems may help students identify what they do notunderstand. Further, the feedback may also help students identify how to resolve such gaps.In sum, our observations showed that feedback assessing students’ current performance can be providedby TAs, peers, and online problems. Because TAs must manage many students and can hence not always bereadily available, students had to rely on additional feedback sources to assess their current performance. In thetraditional discussion section, peers served as feedback sources when they checked each other’s answers for correctness. In the active learning discussion section, peers and online problems served as feedback sources. Particularly, online problems provided correctness feedback that augments students’ assessment of current performance.Feedback Component 3: Bridging the Gap Between Current and Expert PerformanceThird, students need to bridge the gap between current and expert performance (Sadler, 1989). Our observationsrevealed that TAs and students bridged the gap by providing corrective explanations. For instance, students mayexplain concepts not covered in the course to support what students do not understand and how to address it:Colleen: “Ok, I’m completely unfamiliar” (referring to the problem regarding the heat formulaand how to calculate it) “It was not in the textbook or the lecture”Carl: “You probably remember q = MCaT” (the heat formula)Colleen: “I didn’t take AP Chem”Carl:“I can explain it if you want”Aaron: “Explain it please” (looks over from his computer)Carl: “You too?” (turns to Aaron)Aaron: “I like how he explains it” (looks at both Colleen and Carl)Carl explains the use of the heat formula by drawing a diagram on his paper.Addie comes over and hears the end of the explanation. She suggests paying attention to theunits because they are different (joules vs. kilojoules).Addie then looks at the paper and realizes students do not understand the heat formula. Shesuggests that Aaron and Colleen do question 3 on the worksheet to get the background.CSCL 2017 Proceedings283© ISLSIn this example, Colleen and Aaron asked Carl to explain a concept that was not covered in existing instruction.Colleen mentioned in her interview that Carl often explained concepts as they worked together (see also the excerpt of Carl and Colleen in the above section). She finds that Carl is “really helpful. He took AP Chemistry so Ithink he just knows some of the basics better.” Hence, Aaron and Colleen seem to value his explanations. Further,Carl’s explanation helped the TA, Addie, realize what students did not understand and provide corrective feedbackneeded to bridge the gap. Hence, collaboration not only helped students, but also the TA, identify and bridge gaps.This observation also suggests that TAs may not be able to resolve the gap easily because their thinkingdiffers from students. Interviews provided further evidence for this observation. For instance, Ted said that he“can see the answers most of the time, but [students] don’t, so […] if we [TAs] work it out too fast, [students] are[…] confused... I’m not sure if they do understand or don’t understand it.” A student in Addie’s active learningsection also identified this difference: “there’s peers who […] try to explain it in a way that you understand, ratherthan teachers explaining it in the way that they think people understand maybe, which is sometimes right.” Sheappreciated peer explanations because TA explanations were not always useful to her own understanding.Addie’s student also added that, “it’s also helpful to listen to other people’s questions because a lot oftimes like you haven’t quite gotten to that yet, or like they have a different insight […] and then um it’s alsohelpful like to try to explain to other people too, so like if you think you have a good aspect on that, it would helpthem out.” This student found collaboration useful because it allowed her to ask questions, explain to peers, andto listen to exchanges among other students.Our observations suggest that collaboration and student explanations occurred less frequently in traditional sections. A student in Ted’s traditional section articulates this observation: “[Ted] says, you can work together but a lot of people don’t work together that much, and I’m probably one of them too, because I don’t know,I wish there was a different way to have us work together than just ok, here’s a worksheet […] no one has seenthe materials before, so then, I don’t know it’s hard to just be able to work together on it because you have toreally understand it personally. A lot of times working together, it’s usually like ok, here’s the answer.” This quoteillustrates that many students in the traditional section did not work together. Those who worked together checkedanswers for correctness to assess their current performance, but did not work with each other to bridge gaps.The student further explained she was hesitant to collaborate because there is risk in working with peers:“I would rather get help from a student, but I probably go to the TA to get help because it’s more convenient […]students could tell me the wrong way to do problems. You have to be careful of that, make sure it’s the right wayand the right answer.” This student worried that corrective feedback from peer explanations included incorrectinformation. Hence, she relied on the TA for feedback. This suggests that, without correctness feedback from anexpert on “the right way and the right answer,” students may be hesitant to collaborate and help each other bridgethe gap between current and expert performance because they perceived peers’ corrective explanations as risky.In sum, our findings suggest that feedback from TAs may not sufficiently bridge the gaps that studentsidentify. Peer feedback seemed to be most effective in helping students bridge the gap between current and expertperformance because students have a better understanding of their peer’s gaps. Such peer collaboration was prevalent in the active learning discussion section, but not in the traditional discussion section because students didnot trust corrective feedback from peers without correctness feedback.DiscussionThis paper presents an observational study of active learning and traditional discussion sections in an undergraduate chemistry course. Interview and observational data showed that students received formative feedback from:(1) TAs and online problems to help students understand expert performance, (2) TAs, online problems, and peersto help students assess their current performance, and (3) TAs and peers to bridge the gap between expert andcurrent performance. In regards to bridging the gap, results suggest that students wanted collaboration and morefeedback from peers, because peers “explain it in a way that [they] understand.” Such peer explanations weremore prevalent in the active learning sections than in the traditional sections.One key difference between the active learning and traditional discussion sections was the availabilityof online problems. In the active learning discussion section, students received correctness feedback from onlineproblems. The correctness feedback may have helped students address confusions and identify gaps between theircurrent and expert performance. Then, students could ask their TAs and peers to help them bridge specific gapswith corrective feedback. In the traditional discussion section, students were not provided feedback from onlineproblems to assess whether their answers were correct. These students checked answers with peers to assess theircurrent performance, but they did not trust the correctness feedback or corrective explanations from peers becausethey might provide incorrect information. Hence, they did not engage in peer explanations to help them bridgethe gap. Therefore, one possible explanation of how technology supported collaboration in active learning interventions is that correctness feedback from the online problems helps students trust corrective feedback from peers,CSCL 2017 Proceedings284© ISLSparticularly in bridging gaps between expert and current performance. Figure 2 shows a theoretical model of thisprocess for active learning and traditional discussion sections.Figure 2. Theoretical model of the interactions between technology, collaboration, and feedback in active learning discussion section (blue, solid lines) and traditional discussion section (orange, dashed lines).From a CSCL perspective, it is striking that the online problems did not directly support collaboration.Rather, they indirectly supported collaborative learning by providing feedback that students elaborated on in peercollaboration. Specifically, the online problems in our study focused on content, not on collaboration. This findingextends CSCL research, which has typically investigated technologies designed to directly affect student collaboration and learning (Strijbos, Kirschner, & Martens, 2004). On the one hand, this indicates that CSCL researchshould investigate how content-focused technologies enhance collaboration without explicitly being intended todo so. Because active learning interventions describe a broad range of interventions that use various types ofeducational technologies and collaborative interventions, these interventions provide a rich context to investigatehow technologies enhance learning through collaboration. In this context, CSCL provides a useful perspectivethat can help explain which mechanisms account for the effectiveness of active learning interventions (Strijbos etal., 2004). On the other hand, CSCL research may further improve active learning interventions. If indirect supports for collaboration are effective, can direct computer-based supports with elaborated feedback further enhancecollaborative learning and yield even more effective active learning interventions?The fact that content-focused correctness feedback supported collaboration by fostering trust in peers’corrective feedback highlights the need to consider trust when designing technology to support collaborativelearning in STEM courses. While students preferred peer feedback, students did not trust it unless it was informedby correctness feedback. Students’ apprehension may result from the emphasis of exams in STEM courses on onecorrect answer as an indicator of expert performance. Thus, students require content-focused correctness feedbackthat assure progress towards expert performance. This has important implications for technology design and implementation. For instance, technology could enhance collaboration by providing access to peer explanations, butstudents may not trust that feedback without confirmation from an expert that the explanations are correct. Oncestudents received correctness feedback that they trust, they may engage with peer explanations that enable themto bridge the gap. Future investigations should confirm our findings that correctness feedback makes active learning interventions effective for collaboration in order to inform the implementation of active learning in STEMcourses. For instance, a follow-up study could investigate whether providing online problems that only providecorrectness feedback to students in the traditional discussion section can also promote collaboration.Our study also raises questions regarding the role of instructors and peers with collaboration and technology. Feedback supports students in understanding expert performance, assessing current performance, andbridging the gap between them. Instructors typically provide all three types of feedback. However, our findingssuggest that they may not be able to effectively do so for all the students. Hence, instructors and students mayboth benefit from technology support that adapts feedback to instructor and student needs. Investigating howinstructors and students use technology feedback can provide insights into how educational technologies mightprovide more specific, tailored guidance that supports student learning (Van der Kleij et al., 2015).LimitationsOur study should be interpreted in light of the following limitations. In general, qualitative studies serve to revealcausal mechanisms in the specific study context. They do not attempt to prove generalizable causal relationships.As with all qualitative studies, our study provides an account of a specific sample, context, and setting. A varietyof factors may contribute to the feedback and collaboration between peers and TAs, such as motivation and abilityof students to provide quality feedback to each other (Nicol & Macfarlane-Dick, 2006). Future research willCSCL 2017 Proceedings285© ISLSinvestigate how these factors influenced interactions among students, instructors, and computer-supported instructional materials and whether they contribute to the effectiveness of active learning interventions. Further,this study focused on formative feedback. Many other aspects affect student learning such as the group dynamicsin which interactions are situated. Future analyses of this data will investigate the sociocultural factors in thediscussion sections that may explain the effectiveness of active learning interventions and provide further insightinto the mechanisms underlying how technology and collaboration support student learning.ConclusionIn sum, an observational study showed an indirect role for technology on collaboration between students in anactive learning discussion section. The technology provided formative feedback that helped students understandexpert performance and assess their current performance. By supporting these two key components of feedback,the technology indirectly enhanced students’ ability to collaboratively bridge gaps between current and expertperformance. The technology also enhanced the instructors’ ability to provide appropriate feedback. Our resultsprovide a first attempt at building a theoretical model describing the mechanisms that account for the effectivenessof active learning interventions in STEM courses. Further, our findings provide directions for further CSCL research that should test whether enhanced technology-supported feedback or direct support for collaboration mayfurther enhance the effectiveness of active learning interventions.ReferencesChi, M. T. H. (2009). Active-constructive-interactive: A conceptual framework for differentiating learningactivities. Topics in Cognitive Science, 1(1), 73–105. http://doi.org/10.1111/j.1756-8765.2008.01005.xEddy, S. L., Converse, M., & Wenderoth, M. P. (2015). PORTAAL: A classroom observation tool assessingevidence-based teaching practices for active learning in large science, technology, engineering, andmathematics classes. CBE-Life Sciences Education, 14(2), 1–16. http://doi.org/10.1187/cbe-14-06-0095Freeman, S., Eddy, S. L., McDonough, M., Smith, M. K., Okoroafor, N., Jordt, H., & Wenderoth, M. P. (2014).Active learning increases student performance in science, engineering, and mathematics. Proceedings ofthe National Academy of Sciences of the United States of America, 111(23), 8410–5.Hattie, J., & Timperley, H. (2007). The power of feedback. Review of Educational Research, 77(1), 81–112.Lom, B. (2012). Classroom activities: Simple strategies to incorporate student-centered activities withinundergraduate science lectures. Journal of Undergraduate Neuroscience Education, 11(1), A64–A71.Mahalingam, M., Schaefer, F., & Morlino, E. (2008). Promoting student learning through group problem solvingin general chemistry recitations. Journal of Chemical Education, 85(11), 1577.Michael, J. (2006). Where’s the evidence that active learning works? Advances in Physiology Education, 30(4),159–167. http://doi.org/10.1152/advan.00053.2006Miles, M. B., & Huberman, A. M. (1994). Qualitative Data Analysis: An Expanded Sourcebook. SAGEPublications.Nicol, D. J., & Macfarlane-Dick, D. (2006). Formative assessment and self-regulated learning: A model and sevenprinciples of good feedback practice. Studies in Higher Education, 31(2), 199–218.http://doi.org/10.1080/03075070600572090Paulson, D. R. (1999). Active learning and cooperative learning in the organic chemistry lecture class. Journal ofChemical Education, 76(8), 1136. http://doi.org/10.1021/ed076p1136Pienta, N. J. (2015). Understanding our students in general chemistry. Journal of Chemical Education, 92(6),963–964. http://doi.org/10.1021/acs.jchemed.5b00330Shute, V. J. (2008). Focus on formative feedback. Review of Educational Research, 78(1), 153–189.http://doi.org/10.3102/0034654307313795Strijbos, J.-W., Kirschner, P. A., & Martens, R. L. (2004). What We Know About CSCL-And Implementing It InHigher Education. Computer-Supported Collaborative Learning.Van der Kleij, F. M., Feskens, R. C. W., & Eggen, T. J. H. M. (2015). Effects of feedback in a computer-basedlearning environment on students’ learning outcomes: A meta-analysis. Review of Educational Research,85(4), 475–511. http://doi.org/10.3102/0034654314564881AcknowledgmentsThis research was funded by the Institute of Education Sciences, U.S. Department of Education, through Award#R305B150003 to the University of Wisconsin-Madison. The opinions expressed are those of the authors and donot represent views of the U.S. Department of Education. We thank John Moore, Rachel Bain, Hannah Bowman,Mark Bollom, Kristopher Kennedy, Luke Oxtoby, Abe Wu, and Katie Ziebarth for their help.CSCL 2017 Proceedings286© ISLS