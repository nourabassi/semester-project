Integrating Physical and Virtual Models in Biology: A Study ofStudents’ Reasoning While Solving a Design ChallengeNicole D. Martin, Dana Gnesdilow, and Sadhana Puntambekarndmartin@wisc.edu, gnesdilow@wisc.edu, puntambekar@education.wisc.eduUniversity of Wisconsin – MadisonAbstract: Using models to explain phenomena is important in science. Virtual and physicalmodels have different affordances that can be integrated to foster students’ learning.Integrating evidence from multiple models to justify explanations is challenging, and weknow little about how students coordinate such information, especially in biology. This studyinvestigated how students’ integrated information from virtual and physical models in adesign-based, biology curriculum. Some students used information from virtual simulations inwritten explanations of changes they would make to their physical models. However, onethird of students did not use the virtual model to justify their revisions, despite prompts frominstructional materials, the teacher, and other group members. Even some students whointegrated these different models did not initially do this without support from externalprompts. This study provided deeper understanding of how students integrated physical andvirtual models, which can help identify the kinds of support students may need.IntroductionConstructing and using models is an important practice in science (Lehrer & Schauble, 2006; National ResearchCouncil, 2012; Windschitl, Thompson, & Braaten, 2008). Scientific models are representations of naturalphenomena that are “testable, revisable, explanatory, conjectural, and generative” (Windschitl et al., 2008, p. 3).They can be generated by students (e.g., drawings, physical representations) or can be generated by others andmanipulated or utilized by students (e.g., simulations). Inquiry centered around models strives to develop anddefend explanations, which importantly includes using evidence to justify and inform these models (Lehrer &Schauble, 2006; Windschitl et al., 2008). Inquiry with virtual models such as simulations offers uniqueopportunities for students’ learning, allowing students to investigate phenomena that might not otherwise bepossible in a classroom. Research has investigated the affordances and learning benefits of virtual versusphysical models and experiments, but this has predominately been done in physics and engineering contexts(e.g., de Jong, Linn, Zacharia, 2013; Zacharia, de Jong, 2014). Findings from research in these contexts suggestthat virtual models can allow students to conduct experiments about unobservable phenomena and alter the timescale of experiments that would take a long time in the real world, whereas physical models can directly exposestudents to real phenomena, authentic problems, and measurement error they would not face in a virtualexperiment (de Jong et al., 2013; Olympiou & Zacharia, 2012). Further, studies suggest that investigations withvirtual models may better support conceptual understanding than with physical models, because investigationswith virtual models allow students to more easily isolate variables, produce cleaner data to analyze, and offermore time for students to engage in experimentation (de Jong et al., 2013).Previous research in this area has primarily explored comparing students' learning on content-basedtests from participating in experiments using either physical or virtual models or participating in differentsequences of physical and virtual experiments (e.g., Zacharia & Olympiou, 2011; Zacharia, Olympiou, &Papaevripidou, 2008). This research has produced mixed findings on the benefits of one modality over the otherand ideal sequencing. Given the unique affordances of physical and virtual models, integrating or blending theusage of these models based on the particular learning objectives of an experiment has the potential to optimizethese affordances for student learning (Olympiou & Zacharia, 2012). While some researchers have begunexploring such integration, there is still little research examining how students integrate and reason aboutinformation from one model to inform their experimentation with another. Further, little research has focused onhow students collaborate while engaging in such tasks.Additionally, while these findings from physics and engineering contexts are informative, it is unclearhow they extend to biology, as less research has been done on physical and virtual models in biology. Thecomplexity of biological processes offers an important context to explore the integration of these models. Manyprocesses in biology are microscopic and take time to observe (e.g., decomposition), so virtual simulations canbe valuable for quickly conducting experiments of such processes in the context of a classroom. The affordancesof both physical and virtual models offer different learning opportunities that could be utilized to deepenstudents’ understanding of biological processes. Similar to physics and engineering, virtual models in biologycan let students conduct experiments that would take a long time in the real world, and physical models can letCSCL 2017 Proceedings327© ISLSstudents experience real phenomena and encounter problems they would not face in a virtual experiment. Basedon these affordances, both types of models can to be integrated to help foster students’ learning. For example,students could use simulated experiments to gain understanding about how to improve a physical model, or theconditions of a physical model could influence how they use a virtual simulation. To successfully do this,students would need to integrate and use evidence from multiple models to appropriately justify an explanation.Previous research has shown that learning from multiple representations (e.g., Ainsworth, 2006) and usingevidence to support scientific explanations (e.g., Sandoval & Millwood, 2005) are both challenging for students.Thus, utilizing information from virtual and physical models to solve a design challenge is a complex task forwhich students likely need support. Recent research in the context of engineering design has shown that usingvirtual models to plan future design decisions and to reflect on previous decisions can both help studentsintegrate information for their designs and understand science concepts (McBride, Vitale, Applebaum, & Linn,2016). However, we still do not know much about how students might integrate information from virtual andphysical models in this way and, thus, know little about how to best support students to do this, especially in thefield of biology. Our research begins to explore i. whether students integrated information from virtual andphysical models; and ii. how students collaborated to try to successfully integrate this information to worktowards solving a design challenge in the context of a design-based, biology curriculum.MethodsThis was an exploratory study investigating students’ ability to integrate information from virtual and physicalmodels in the context of a design-based, biology curriculum. The curriculum challenged students to work inteams to design a compost that would break down quickly and contain a lot of nutrients to reduce the amount ofwaste going into landfills. Students learned key concepts related to energy transformation and matter cycling inecosystems to solve their challenge. To study decomposition and collect data to justify for their designs,students built, monitored, and refined a physical bio-reactor throughout the unit. Since it takes several weeks forcompost to break down, students also used virtual compost simulations to better understand how abiotic factorsinfluence decomposers’ ability to break down matter. They conducted four virtual compost experiments relatedto the carbon to nitrogen ratio of materials, the amount of moisture, the size of the particles, and thecombination of all these factors. Based on what they learned in the virtual experiments, students were requiredto revisit their physical bio-reactors and use the science ideas, data, and evidence from the virtual experiments todecide on and justify one change that would increase decomposition in their physical bio-reactors. The data forthis study focused on students' written explanations in their journals and audio-recorded conversations fromthree groups that occurred during this activity, further described below.ParticipantsThe participants were twenty-six students (N=26), working in seven small groups, from an 8 th grade scienceclass at an urban middle school in a Midwestern city in the United States. This was a “talented and gifted” classtaught by an experienced teacher. The composition of the small groups was determined by the teacher. Wechose this class as a potential exemplar of how high-achieving students might integrate models. We furtherchose this class to see how an experienced teacher might facilitate this process in a student-centered way thatcould provide information about useful teacher supports that helped students integrate different models in adesign-based curriculum.Data sourcesWe identified a Change Page in the students' journals where they needed to provide written responses aboutwhat to change in their physical bio-reactors after conducting four virtual experiments using a compostsimulation. Students were asked to work with their group to provide evidence for a current problem in theirphysical bio-reactors, write a potential change they could make, and then supply evidence for why the proposedchange would caused improved decomposition. To do this, students needed to observe their bio-reactors toidentify problems and use evidence from their virtual experiments to explain the change they wanted to make.The only data students had to provide evidence was from these virtual experiments, and thus they should haveprovided data from the simulation about the ideal conditions for decomposition in compost to support theirchanges. We inductively developed a coding rubric to explore how students provided evidence for the physicalchange from their virtual experiments (see Table 1). Twenty percent of the students' written explanations ontheir Change Pages were coded by the first and second author separately and a 90% agreement was achieved.All disagreements were resolved with discussion. The remaining written responses were coded by the secondauthor.CSCL 2017 Proceedings328© ISLSTo begin understanding how students worked together to construct the explanations recorded on theirChange Pages and to identify if and how students talked about data from their simulations in making theirdecisions, we qualitatively analyzed conversations from the three groups (A, B, C) that we had audio-recordedduring the unit. These groups were chosen for recording throughout the unit by the teacher as beingrepresentative groups of academically average-performing students within the class.Table 1: Coding scheme to evaluate quality of students' written explanations to support their proposed changeLevelNone / IncoherentPartial explanation withoutevidence from the simulationExampleBlank, no explanationWe are adding browns like oats, leaves, newspaper, and sawdust.This will make decomposition fast.We believe that mixing up our compost will distribute the moistureevenly. Currently the moisture is only in the middle of the compost.If we mix it up then the moisture should be distributed evenly.I think the C:N ratio is the most important change. We must increasenitrogen for large ratio and more moisture. If we don’t have enoughbrowns nothing will be able to break down. Our greens are living sonothing will decompose. We decided to add greens to give it moremoisture.Our carbon to nitrogen ratio is too small. [We could] add browns toincrease decomposition speed. The closer the C:N ratio is to 30:1 thebetter. Adding more browns 1 & 2 (63.7 grams) or more carbon. Itwill improve our carbon to nitrogen ratio. In turn, this will improveour smell and moisture level.General explanation withoutevidence from the simulationExplanation refers to simulationExplanation refers to and usesevidence from the simulationPoint Value01234ResultsStudents’ written explanationsPercent of StudentsWe found that some students successfully integrated information from the virtual simulation to inform or justifythe changes to their physical bio-reactors. However, the quality of students’ written explanations for theproposed changes to their physical bio-reactors varied. We found that eight percent of students did not include acoherent explanation at all, while 23 percent of students gave a partial (score = 1, 11.5%) or general (score = 2,11.5%) explanation without providing evidence (see Figure 1 below). Forty-two percent of the students referredto the simulation in their explanation, but did not include specific evidence (score = 3), and 27 percent ofstudents offered an explanation that referred to the simulation and included evidence (score = 4). This meansthat 31 percent of students did not refer to the simulation in their explanation despite scaffolding in the students’journal, teacher prompting, and working with their group for support.Some groups were more successful than others at collaborating to integrate information from thevirtual simulation to provide evidence for making a change in their physical bio-reactor. We calculated anaverage quality of explanation score for each group and found that Groups B and C had the lowest average score(M = 2) and Groups D, E, and F had the highest average scores (M = 3.3, 3.2, 3 respectively). Groups A (M =2.75) and G (M = 2.5) had scores in the middle. See Figure 2.504030201001234Quality of Explanation ScoreFigure 1. Percent of individual students’ (N=26) responses within each quality of explanation score.CSCL 2017 Proceedings0329© ISLSAverage ExplanationScore432102.75A(n=4)3.3322B(n=4)C(n=4)3.232.5DEFG(n=3)(n=5)(n=2)(n=4)GroupFigure 2. Groups’ average quality of explanation scores, number of students per group shown below each group.Groups’ conversationsWe examined groups’ conversations to better understand how students collaborated to integrate informationfrom their virtual model to make a change to their physical model. These three groups (A, B, C) were chosen bythe teacher at the beginning of the unit as representative groups of academically average-performing students tobe audio-recorded throughout. The analysis of Group A, B, and C’s conversations revealed several differencesin how these groups integrated information from their physical and virtual compost models. Group A mademore use of prompts within the instructional materials and from the teacher to have richer conversations aboutusing data from the simulation to inform the changes to their bio-reactor than Groups B and C. Groups B and Cignored, missed, or misinterpreted several prompts that Group A utilized. Vignettes of each group’s interactionsare presented below to illustrate interactions that were more or less successful.Group AOf the three audio-recorded groups, Group A’s students made more references to the simulation in theirexplanations of changes for their bio-reactor than Groups B and C, with a group average score of 2.75. In theirnegotiation of changes to make in their bio-reactor, they first identified that the compost in their physical bioreactor was too wet. Initially, they did not use any evidence from their simulation experiments to justify theirdecisions. However, when the group was explicitly prompted in the instructional materials to give “evidence forwhy this change will cause improved decomposition” on their Change Page, they utilized data they gatheredfrom the virtual simulation to justify their change to their physical bio-reactor:Student 1: Ok, so what’s the evidence?Student 2: We don’t really have evidence.Student 1: How do we know it's too moist?Student 3: Because.Student 1: Maybe that’s the right range, how do you know that’s the right range?Student 3: Because that’s not the right range.Student 4: Because we did the simulation and the right range was 40-60% moisture.Here, students 1 and 2 were unsure about how to justify their proposed change, or even why they thought theircompost was too moist. Student 3 alluded to their moisture level not being in the “right range,” but student 4took this a step further and reminded the group of the results from their virtual experiment investigating theideal moisture content of compost. At this point, the group decided to add oats to their bio-reactor because itwas too wet, and they knew that the ideal amount of moisture should be 40-60% (by weight) based on data fromthe virtual simulation. The teacher then suggested to the whole class that they might want to use the virtualsimulation to try their proposed change before actually making a change to their bio-reactor.Teacher: Make sure you think out the changes that you are going to make. If that requires runningthrough one of the simulators again, do it. Maybe run yours through the simulator.It was not until this direction from the teacher to utilize the virtual model before making physical changes thatthis group began to meaningfully integrate information from both models. As a result of this prompt, Group Adecided to input the conditions of their own physical bio-reactor into the virtual compost simulation andexperimented with adding different amounts of oats to achieve their desired moisture content:Student 4: What was the C to N ratio [in our bio-reactor]?Student 3: 16.9 to 1.CSCL 2017 Proceedings330© ISLS…Student 4: So try some browns [in the simulation].…Student 4: If it doesn’t work, then we shouldn't add anything.…Student 4: Ok, now try it.Student 3: 20 to 1 ratio, with 50% moisture. So add 10 grams of oats.Student 4: Yeah just a little bit. Just to balance out the moisture.…Student 3: So the more, like 10 is about the same, but since ours is moister, I think we need toadd a little bit more, like 15 to dry some of it up.Student 3: So we need like 15 grams of oats or something.Student 4: Yeah that’s it.This excerpt showed how students 3 and 4 integrated information about the conditions in their physical bioreactors with data from the virtual simulation to refine their proposed design change. Instead of just simply“adding oats,” they determined that they needed to specifically add 15 grams of oats to solve their problem.Further, student 4 insisted that they must base their final decision for the physical model on the evidence fromthe virtual model when he said, “If it doesn’t work, then we shouldn’t add anything.” Through this discussion,the group was eventually able to use observations of their bio-reactor and data from both the virtual simulationto inform and justify their design change.Group BGroup B was less successful than Group A in making reference to the simulation in both their writtenexplanations in their journals (average group score = 2 for their quality of explanations) and during theirconversations about making changes to their bio-reactor. Overall, this group of students got caught up in the“doing” of making the change, rather than thoughtfully planning and justifying their change based on data fromthe simulation. Unlike Group A, they did not discuss the instructional prompt in their journals that asked themto provide evidence for their change. They quickly decided on the change for their bio-reactor and left theclassroom to make it outside. When they were outside, they missed the teacher’s suggestion (written above) thatstudents could use the simulation to try out their change, which prompted Group A to have a deeper discussionabout how data from the simulation could inform revising their bio-reactor. Instead, Group B solely focused ondiscussing the state of their physical model and how the materials inside it needed to be mixed:Student 2: Just mix it up and shake it.Student 3: It’s not evenly distributed (inaudible) and it’s moist.Student 1: Yeah but that could also cause problems if we don’t do it correctly.Student 3: Yeah cus we don’t want to have the stuff growing on it.Student 1: Because if, let’s say we don’t distribute everything evenly, it could cause problems. How arewe gonna mix it without taking it out?Student 2: I think if we shake it, I shook it a little bit this morning and everything moved so I thinkwe’ll be able toStudent 1: Yeah we can only move it up and down.Student 2: No it wasStudent 3: I think it’s a little compact right here, like we pressed it down firmly, and now that area isopened, so that area is decomposed and now that area is opened, so I think it’s decomposedenough so like a little bit more space. If we need to grab something to mix it with like apencil that we’re not gonna use again or something.From this excerpt we can see that the students only focused on their physical bio-reactor when deciding to maketheir change. This focus on the physical model continued. They then took measurements of the temperature, pH,and moisture and made other qualitative observations of their bio-reactor. But they never connected thesemeasurements of the current state of their bio-reactor to information they learned in the simulation to provideevidence for why their change would improve decomposition. They simply went outside and worked on mixingthe materials up in their bio-reactor, and their conversation focused around how to accomplish this. This lack ofCSCL 2017 Proceedings331© ISLSusing evidence from the simulation may not have been entirely the students’ fault though. The Change Page inthe students’ journal had been intentionally designed to help the teacher monitor students’ explanations. Sincethe teacher needed to approve students’ proposed changes, the teacher had the opportunity to check students’supporting evidence prior to making their change. In this instance, the teacher allowed Group B to proceed withtheir change without providing evidence from the virtual simulation, perhaps due to the many groups needinghis attention at that time.Group CLike the students in Group B, the students in Group C’s average written explanation score was a 2, slightlylower than Group A’s average score. We found that, unlike Group B, the students in Group C did discuss theprompt in the students’ journals that asked them to provide evidence for why their change would causeincreased decomposition. However, their discussion around this prompt was less focused on using data from thesimulation to provide evidence than Group A’s discussion, described above. Group C discussed two differentpotential changes: reducing the moisture in the bio-reactor because it was too wet and adding more carbon richmaterials because there were not enough. Group C appeared to misinterpret the instructional prompt in theirjournal that asked them to provide evidence to support their change: they gave a prediction for how their changewould help their bio-reactor, rather than providing evidence from their simulation experiments to explain whythey should make the change. For example, they could have discussed what they learned about the ideal C:Nratio range to promote decomposition from doing their virtual experiments and how the ratio in their bio-reactorwas not in this ideal range.Student 1: Evidence for why this change will cause improved decomposition.Student 2: A better smell. A more normal odor, and faster compost.Student 3: Well, no I know that, but why would this change be better? It’ll create a better smell…Student 2: Faster, faster decomposition, it just makes it slow and foul odor. Um, what else?Student 3: I think that’s it, right? Because we don’t really need to say anything else right?Student 2: That’s good.After this exchange, Group C decided to add more carbon to their bio-reactor. When the teacher mentioned tothe entire class that students could use the virtual simulation to test their proposed change before making it, thestudents in Group C then more specifically discussed what materials they could add to increase the carbon totheir bio-reactor. However, they seemed to ignore the teacher’s suggestion and never tested their ideas in thesimulation or mentioned data from the previous simulation experiments they ran about the ideal C:N ratio range,like Group A did with the ideal moisture range. It was not until the teacher visited with Group C individuallyand prompted them to further explain what they thought would happen by adding carbon rich materials that thegroup specifically discussed improving the C:N ratio of their bio-reactor:Teacher: …it says explain why, explain specifically what you expect to happen… Don’t just saysomething like ‘we think it’ll help’, or ‘the process will work more efficiently’, explainspecifically what you expect to happen.Student 2: If we say evening out the carbon to nitrogen ratio, would that be good? Ok.…Student 1: I bet, I bet if we add carbon we will be adding more materials and carbon to even out thenitrogen to carbon ratio.Despite receiving multiple instructional prompts from the journal and the teacher, Group C never discussed datafrom their simulation to provide evidence for their change. These students had previously experimented with thevirtual simulation to learn about the ideal C:N ratio range for decomposition, and they had previously calculatedthe C:N ratio in their own physical bio-reactors; however, they did not make connections between these twomodels to propose and support their revision to their physical model.Conclusions and implicationsWe were interested in exploring whether and how students integrated information from virtual and physicalmodels to work towards solving a design challenge in the context of a design-based, biology curriculum. Thisstudy suggested that some students in a talented and gifted class were able to integrate information from thesedifferent models to work towards solving a challenge when provided with several supports, such as promptsfrom instructional materials, the teacher, and other students in their group. However, even with these supports,about one third of the students’ written explanations and two of the three groups’ conversations showed a lackof integrating information from the virtual and physical models. The findings from our qualitative analysis ofCSCL 2017 Proceedings332© ISLSthree groups additionally suggested that even the one group of students who provided information about thesimulation in their explanations did not initially integrate this information from different models without thesupport from external prompts. These findings importantly contribute to our understanding of students’ abilityto reason about information from different models to inform and justify decisions. Previous research hastheorized about integrating virtual and physical models and has shown enhanced conceptual learning fromexperimentation that blends the affordances of these models over using virtual or physical models (Olympiou &Zacharia, 2012). However, little is known about students’ ability to reason about and coordinate these differentmodels and the information they provide. Our findings shed light on the challenging nature of this task andsuggest the need for additional support. Our findings also align with previous research that identified thatstudents struggle to learn from multiple representations (e.g., Ainsworth, 2006) and justify explanations withevidence (e.g., Sandoval & Millwood, 2005).Our work further extends the findings of prior research by exploring how students used evidence fromdifferent models to write explanations in biology. Students’ use of multiple models in biology may be especiallydifficult, because the time scale between virtual and physical models in biology may add another layer ofcomplexity. For example, many biological processes (such as decomposition) take weeks to observe whereassimulations can be run in seconds. To complicate things more, data collection in biology can be less straightforward than from a virtual simulation because many biological processes are complex systems that are hard toaccurately measure. For example, the contents of a bio-reactor are varied, and temperature and moisture levelsmay be different depending on where students take their measurements. Therefore, the complexity of biologicalsystems may present extra challenges for students to navigate between such diverse models (Hoskinson,Caballero, & Knight, 2013).Additionally, we found that students seemed to pay more attention to their physical models in theirwritten explanations and conversations about their potential revisions. We conjecture that students may focusmore on physical models because they are more concrete and more familiar objects in a science classroom.Perhaps this is also due to the idea that more concrete representations have been shown to better facilitatestudents’ understanding of scientific principles than abstract representations (Goldstone & Son, 2005). This maybe because concrete representations offer information that is more salient and connected to the real world, thusbetter supporting students’ reasoning (Goldstone & Son, 2005). Given that the students in our study were askedto make revisions to their physical bio-reactors, students may have seen these concrete physical models asproviding more relevant and salient information for making their decisions. This may mean that teachers andinstructional materials need to more explicitly discuss the relationships between virtual and physical models inbiology to help students make useful connections between the information represented in both models.The prompts within instructional materials and from the teacher appeared to be crucial in encouragingstudents to integrate data from virtual simulations to make a decision about revising their physical model.However, students did not utilize these prompts in the same way. First, explicitly pressing students for evidencein the student journal resulted in one group making a connection to the virtual simulation and utilizing that datato justify their change. This same prompt was not sufficient for supporting students in other groups to integrateinformation from their virtual and physical models, as either they ignored or misinterpreted the prompt. Second,the teacher’s suggestion that students could use the simulation to try their proposed changes in the virtual modelwas instrumental for one group but ignored by another. For the group of students who decided to follow theteacher’s suggestion, this facilitation was a key turning point to integrate information from their physical bioreactors and the virtual simulation; this helped them refine their solution by utilizing the virtual model to testhow their change would theoretically affect their compost. Even though this prompt was useful for one group,many students continued to focus only on their physical bio-reactors. The idea that various scaffolds areimportant for students when engaging in complex tasks is well known (e.g., Puntambekar & Kolodner, 2005);however, this study deepens our understanding of how particular prompts influenced students’ reasoning andwhere students may need additional or different forms of support to successfully engage in this complex task.While prompts from the teacher appeared to be influential, it seems that it may be difficult for teachersto support students in integrating information from multiple models. Even in a talented and gifted class, itappeared that more numerous and more explicit prompts were needed to support students to improve on such achallenging practice, given that many students did not use information from the virtual simulation in theirexplanations. Since this is such a difficult task for students and facilitating multiple groups of students iscomplex for the teacher, designing instructional materials to help students integrate virtual and physical modelsneeds to be more intentional. Specifically, perhaps one way to better support students in making connectionsbetween different models would be to design activities that explicitly asked students to use information gatheredfrom a virtual model to inform or revise a physical model, and vice versa. For example, students could berequired to use a virtual model to test how changes might affect a physical model. Additionally, teachers likelyCSCL 2017 Proceedings333© ISLSneed more professional development to better support students’ work in this area (Gilbert, 2004; Gnesdilow,Smith, & Puntambekar, 2010), as even an experienced teacher who understood the nature of models could havesupported students more successfully.This study offers an important contribution because it provided a deeper look at how groups of studentsreasoned and negotiated to make connections between physical and virtual models. This information can lead toa more thorough understanding of how many students struggle to accomplish this challenging integration ofinformation. More work needs to be done to understand the kinds of support that students may need to besuccessful. Given that this study took place in a class of identified talented and gifted students with anexperienced science teacher, we wonder how students in a more typical context would perform on the sametask. More research is necessary to understand what types of prompts and scaffolds are necessary to supportstudents’ collaboration to integrate virtual and physical models. Our future research will examine how moreteachers facilitate students’ coordination of information from multiple models to solve a design challenge andhow different facilitation strategies may impact students’ learning.ReferencesAinsworth, S. (2006). DeFT: A conceptual framework for considering learning with multiple representations.Learning and Instruction, 16(3), 183-198.de Jong, T., Linn, M. C., & Zacharia, Z. C. (2013). Physical and virtual laboratories in science and engineeringeducation. Science, 340(6130), 305–8.Gilbert, J. K. (2004). Models and modelling: Routes to more authentic science education. International Journalof Science and Mathematics Education, 2(2), 115-130.Gnesdilow, D., Smith, G.W., & Puntambekar, S., (2010). An analysis of science teachers’ classroom discourserelating to the use of models and simulations in physics. In Zacharia, Z. C., Constantinou, M. P.,Papaevripidou, M. (Eds.) Application of New Technologies in Science Education: Proceedings of theInternational Conference of Computer Based Learning in Science, 141-152. OEIiZK, Warsaw, Poland.Goldstone, R. L., & Son, J. Y. (2005). The transfer of scientific principles using concrete and idealizedsimulations. The Journal of the Learning Sciences, 14(1), 69–110.Hoskinson, A. M., Caballero, M. D., & Knight, J. K. (2013). How can we improve problem solving inundergraduate biology? Applying lessons from 30 years of physics education research. CBE-LifeSciences Education, 12(2), 153-161.Lehrer, R., & Schauble, L. (2006). Cultivating model-based reasoning in science education. In K.Sawyer (Ed.), The Cambridge handbook of the learning sciences (pp. 371–388). New York: CambridgeUniversity Press.McBride, E., Vitale, J., Applebaum, L., & Linn, M. C. (2016). Use of interactive computer models to promoteintegration of science concepts through the engineering design process. In C. K. Looi, J. L. Polman, U.Cress, & P. Reimann (Eds.), Transforming Learning, Empowering Learners: The InternationalConference of the Learning Sciences (ICLS) 2016 (pp. 799–802). Singapore: ISLS.National Research Council. (2012). A Framework for K-12 science education: practices, crosscutting concepts,and core ideas. Washington, D.C.: National Academies Press.Puntambekar, S., & Kolodner, J. (2005). Toward implementing distributed scaffolding: Helping students learnscience from design. Journal of Research in Science Teaching, 42(2), 185-217.Sandoval, W. A., & Millwood, K. A. (2005). The quality of students’ use of evidence in written scientificexplanations. Cognition and Instruction, 23(1), 23–55.Windschitl, M., Thompson, J., & Braaten, M. (2008). Beyond the scientific method: Model-based inquiry as anew paradigm of preference for school science investigations. Science Education, 92(5), 941-967.Zacharia, Z. C., & de Jong, T. (2014). The effects on students’ conceptual understanding of electric circuits ofintroducing virtual manipulatives within a physical manipulatives-oriented curriculum. Cognition andInstruction, 32(2), 101–158.Zacharia, Z. C., & Olympiou, G. (2011). Physical versus virtual manipulative experimentation in physicslearning. Learning and Instruction, 21(3), 317-331.Zacharia, Z. C., Olympiou, G., & Papaevripidou, M. (2008). Effects of experimenting with physical and virtualmanipulatives on students' conceptual understanding in heat and temperature. Journal of Research inScience Teaching, 45(9), 1021-1035.AcknowledgmentsWe thank the students and the teacher who participated in this study. The research reported in this study issupported by a grant from the National Science Foundation to the last author.CSCL 2017 Proceedings334© ISLS