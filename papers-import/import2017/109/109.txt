Assessing Student Generated Infographics for ScaffoldingLearning With Multiple RepresentationsEngida Gebre, Simon Fraser University, egebre@sfu.caAbstract: Student-generated multiple representations are increasingly used in scienceeducation, thereby creating a challenge for educators to assess these inscriptions. This paperused student-generated infographics created collaboratively in classrooms where secondaryschool students were engaged in authentic science news reporting and inductively generatedtools for assessing quality infographics in authentic learning contexts. Results showed studentgenerated infographics can be assessed for dimensionality of the represented topic as well asfor representation of understanding. Findings have implication for facilitating learning withinfographics and analysis of visual research data.Keywords: multiple representations, infographics, assessing student-generated artifactsLemke (1998) identifies two aspects of science literacy: understanding of concepts and the ability to usemultiple representations scientists use to represent and explain a given phenomenon. Accordingly, the ability touse visual representations has become an emerging field of research and practice in science education (Gilbert,2008). This notion of developing representational competence—the ability to understand, create and critiquemultiple representations (diSessa & Sherin, 2000) has moved towards using student-generated representations(Van Meter & Garner, 2005) that involves the use of learned representations such as quantitative charts as wellas inventive and qualitative drawings.One of the challenges in using student-generated representations such as infographics—visualrepresentations of data and ideas—is assessing students’ work and providing feedback because students oftencome up with inventive representations that may not have a benchmark. Lack of meaningful support couldpossibly lead to students’ superficial understanding of the concept and failure to understand the relationshipbetween represented phenomenon and the representation. In this paper, we used preliminary analysis of studentgenerated infographics collected as part of a bigger science literacy project to determine aspects of qualityinfographics. More specifically, we’ll address the question “What are the aspects of quality infographics thatteachers can use to assess student generated infographics”? Addressing such issue will help teachers to facilitateinfographic-based learning and instruction and foster learners’ representational competence.Infographics as multiple representationsThe use of multiple representations in learning and instruction has been a major area of educational research andpractice for decades (Gilbert, 2008). Recent attention has moved towards student-generated representations aslearning tools and outcomes. Researchers consider student-generated representations to be effective learningstrategies similar to summarization and prior knowledge activation (Van Meter & Garner, 2005). An infographic(short for information graphic) is a form of representation of data and ideas often used to communicate with thegeneral public rather than with scientific audience (Gebre & Polman, 2016). It combines the use of quantitativeand qualitative data as well as qualitative cues to facilitate readers’ understanding of the representedinformation.Recent studies show that infographics are increasingly used as learning/instructional tools in secondaryschool context (Gebre & Polman, 2016; Polman & Gebre, 2015). Although infographics are sometimes referredto as data visualizations implying a visual representation of quantitative data, in the context of our projectstudents often combine up to 4 or 5 types of representations in one infographic and work at multiple layersincluding the data level, visualization level and the holistic infographic level. By “layers” we mean levels orbuilding blocks students work on in the construction process. The first is the data layer where studentsunderstand the nature of the data such as quantitative versus qualitative, categorical versus interval data or actualversus proportion data. While the nature of the data they collect depends on the nature and scope of the topicthey are working on, students use multiple sources to triangulate their data and determine its credibility. Theyalso organize the data in a way that is appropriate to what they intend to communicate. Polman & Gebre (2015)described how an experienced designer sorted the data when she was asked to create infographics forpublication from already collected data.Data visualization is the second layer. At this layer, students make choices related to determining whatkind of visualization helps to meaningfully represent the kind of data they have. For example, pictures andCSCL 2017 Proceedings684© ISLSdrawings provide physical association with the represented object. Quantitative graphs, on the other hand, donot have physical association with the represented data; rather, they provide insight about the inherent structureof the data that a table cannot provide. Process related data is best represented using a flowchart or schematicdiagrams. Semantic maps represent relationships between concepts. Decisions about the type of visualizationdepends on the nature of the data, the features of the visualization tool and students’ understanding of therelationship between the two—data and representation. It is also possible that students can come up withinvented or cultural representations which are not taught at school. Thus, this layer deals with appropriatevisualization that in turn helps to develop visual thinking and problem solving (Azevedo, 2000; Reed, 2010).The third layer is the infographic layer or the holistic layer. In this case, the infographic becomes acollection of visualizations representing various data and ideas as well as the organization, layout, andqualitative cues of the communication. When working at this layer, students deal with assembling a holisticargument, constructing explanations, organizing sources, and communicating their understanding based onscientific practices of data-driven inquiry (Kuhn, 2010). In addition to the data and organization, students alsodetermine completeness of representation and contextualize it for possible readers.Assessing student-generated infographicsFormative assessment of students’ work and providing meaningful feedback for improvement is challenging inthe context of infographic-based learning and instruction. This is so because students often choose differenttopics to work on thereby leading to the absence of a defined answer or way of doing the infographic. In conceptmaps, for example, it is possible to use expert-generated map as referent to assess student generated artifacts(Rye & Rubba, 2002). This is because a) often students in a class work on the same project topic in creating theconcept map and b) it is relatively easier to create concept maps by experts in the field that can serve as abenchmark. Infographics are used as both learning and communication tools in the context of science literacywhere students choose different topics that make sense to them or their community. One of the teachers in ourproject memorably said “it is easier for me to let students choose their own project topic and then support themin finding the data they need rather than choosing a common topic for all students and then working on theirmotivation”. The challenge with this approach and infographic-based instruction is the absence of guideline forassessing student-generated artifacts. This is also a challenge in general areas of student-centered instructionespecially when student-generated artifacts are in visual forms. In this paper we use 30 student-generatedinfographics and analyze them inductively to come up with aspects for quality infographics.Project and methodsSTEM Literacy through Infographics (SLI) is a design-based Development and Implementation project (DIP)funded by the US National Science Foundation (NSF). The project focuses on developing young adults’ (grade10 to 12) science literacy, mathematical reasoning and representational competence through actively engagingthem in a collaborative process of creating data-driven infographics for authentic online publication onhttp://science-infographics.org. This DIP is a continuation of a two-year exploratory project and currentlyinvolves five traditional secondary schools, one alternative experimental secondary school, one after schoolprogram and one summer program in three states in the US—with a total of 1084 participating students. Dataused in this paper was collected from two sites in a large metropolitan area in the Midwestern United States: asuburban public secondary school with socioeconomically and ethnically diverse population and an after schoolinternship run at a mid-size private university. What students do in the project is described in detail elsewhere(Gebre & Polman, 2016). However, we briefly described it here to provide context. In face-to-face classroomcontext, students work in pairs or individually on a) identifying a science related topic to work on, b) searchingfor relevant data from online sources and databases (e.g., the Centers for Disease Control, National Institute ofHealth, Environmental Protection Agency, etc.), c) organizing the data and creating infographics, d) providingpeer feedback using online collaborative tools (e.g., VoiceThread), e) getting feedback from an external editorwho is the curator of the online publication website and member of the project (with PhD in Chemistry) and f)revising the infographics and submitting for publication. The data used for this paper are draft and final versionsof 30 infographics produced by participating students in one of the six schools.Data analysis involved openly comparing student-generated infographics and identifying the similarityand/or differences between them. The author used six infographics (that were not part of the 30 analyzed here)and compared them with the purpose of delineating features where infographics differ or align. He thendeveloped a rubric with eight dimensions to determine quality of infographics. Two research assistants reviewedthe dimensions at different times and provided comments for improvement based on their analysis of theinfographics. Both students reported challenges of using the eighth dimension (“parsimony”) as it became toosubjective to assess. We then dropped “parsimony” and scored five more infographics using the rubric. This wasCSCL 2017 Proceedings685© ISLSfollowed by discussion of the scores. One of the research assistants and the author scored another sixinfographics and discussed their results. We then coded the draft and final versions of the remainder of theinfographics.Findings and discussionResults showed student-generated infographics can be assessed in terms of seven features listed belowTypes of representations. Infographics vary in terms of the various forms of representations they have(icons, bar charts, drawing, non-label text). Some infographics have just one visual representation while othershave as many as five types.Distribution of information. A related feature is whether information is distributed over different typesof representations or are students repeating the data they presented in one form to another form. That is, therepetitive versus complementary nature of the data presented in various formats.Dimensionality. Some infographics are rich in terms of content and others not so. Dimensionalityrelates to how many aspects of the represented topic are addressed in the infographic. It has to do with the depth,richness or completeness of the infographic to communicate the intended purpose or to tell a story.Nature of data. In the context of our project, infographics are tools for students to make data drivenarguments about the topic they are interested in. The question then becomes what kinds of data are studentsincluding in their representation (e.g., raw or proportional data, quantitative or qualitative data)Contextualization. Most of the students pick project topics based on whether or not the topic is relevantto them or to someone they know—what we call “personal context” to the project. For example a student choseto work on “cauliflower ear” because he is a wrestler and has experienced the problem personally. The questionis whether such context is communicated to readers through the infographics. Is there enough contextualizationfor readers to understand what the infographic tries to communicate or why it matters? Are readers able toanswer “so what?”Correspondence or alignment between various parts. This criterion includes both conceptual andtechnical alignment between various elements of the infographics. For example, does the title represent what isrepresented in the body of the infographics? Is the use of colors and shapes consistent through out theinfographics?Sources. Are there multiple, credible sources for the data and claims represented?Each aspect discussed above was scored out of 3 points using the rubric. Our purpose was not to focuson the numeric value of the scoring, rather on the qualitative improvement of each infographic (and therepresentational skills of the learners) from the draft to the final version. We focused on the continuum nature ofthe scoring that range from 1 to 3 to represent low, medium and high quality work, respectively.Figure 1 presents the average score for draft and final versions of the 30 infographics. The highestscores are for “distribution of information” and “sources” both in the draft and final versions, implying a)students do not use repetitive representations or they use different kinds of tools to represent different aspects oftheir topic/data, and b) they provide multiple credible sources for the information they include in theirinfographic. Figure 1 also shows that complexity of representation (dimensionality), which has to do with“completeness” of the representation, has the least score both in the draft and final versions. Based on ourclassroom observation, this happens because students sometimes perceive infographics as creating onequantitative chart as opposed to making a complete argument or story.The use of multiple representations is the hallmark of scientific reasoning and explanation (Lemke,1998; Gilbert, 2008; Yore and Hand, 2010). Our findings build on diSessa’s (2002) work on representationaladequacy of student-generated representations and the Polman & Gebre’s (2015) work on framing infographicsas scientific inscriptions. It also extends prior work by inductively generating features of quality infographics.We’ll build on this preliminary analysis to help educators and students who use infographic-based learninginstruction in their classes. From a research perspective, our existing and future analysis contributes tounderstanding cognitive aspects of infographic-based arguments and visual methods in research data analysis.CSCL 2017 Proceedings686© ISLS32.521.5Draft1Final0.50Figure 1. Average scores of draft and final version infographics (N=30).The use of multiple representations and semiotic tools is necessitated by the demands of the sciencecurriculum (Lemke, 1998; Yore & Hand, 2010). Infographic serves as a learning tool and uses multiple forms ofrepresentational tools. Accordingly, assessment of student-generated infographics needs to address both thecontent and the representation aspects of learning. This study helps teachers who design learning withinfographics to consider these two main aspects while assessing students’ work.ReferencesAzevedo, F. (2000). Designing representations of terrain: A study in meta-representational competence. Journal ofMathematical Behavior, 19, 443-480diSessa, A. A. (2002). Students' criteria for representational adequacy. In K. Gravemeijer, R. Lehrer, B. van Oers, & L.Verschaffel (Eds.), Symbolizing, modeling and tool use in mathematics education (pp. 105–129). Dortrecht:Kluwer.diSessa, A. A., & Sherin, B. L. (2000). Meta-representation: An introduction. The Journal of Mathematical Behavior, 19(4),385-398. doi:http://dx.doi.org/10.1016/S0732-3123(01)00051-7Duschl, R. A. (2008). Science education in three-part harmony: Balancing conceptual, epistemic, and social learning goals.Review of Research in Education, 32, 268–291Kuhn, D. (2010). Teaching and learning science as argument. Science Education, 94(5), 810–824.Gebre, E. H., & Polman, J. L. (2016). Developing young adults' representational competence through infographic-basedscience news reporting. International Journal of Science Education, 1-21.Gilbert, J. (2008). Visualization: An Emergent Field of Practice and Enquiry in Science Education. In J. Gilbert, M. Reiner,& M. Nakhleh (Eds.), Visualization: Theory and Practice in Science Education (Vol. 3, pp. 3-24): SpringerNetherlands.Lemke, J. (1998). Multiplying meaning: Visual and verbal semeotics in scientific text. In J. Martin & R. Veel (Eds.),Reading science (pp. 87-113). London: Routledge.Lemke, J. (2004). The literacies of science. In E. W. Saul (Ed.), Crossing borders in literacy and science instruction:Perspectives on theory and practice (pp. 33-47). Newark, DE: International Reading Association/NSTA.Polman, J. L., & Gebre, E. H. (2015). Towards critical appraisal of infographics as scientific inscriptions. Journal ofResearch in Science Teaching, 52(6), 868-893.Reed, S. K. (2010). Thinking visually. New York: Psychology PressRye, J. A., & Rubba, P. A. (2002). Scoring concept maps: An expert map‐based scheme weighted for relationships. SchoolScience and Mathematics, 102(1), 33-44.Van Meter, P., & Garner, J. (2005). The promise and practice of learner-generated drawing: Literature review and synthesis.Educational psychology review, 17(4), 285-325.Yore, L. D., & Hand, B. (2010). Epilogue: P lotting a research agenda for multiple representations, multiple modality, andmultimodal representational competency. Research in Science Education, 40(1), 93-101.AcknowledgmentsThis work was supported by the National Science Foundations [grant numbers IIS-1217052 and IIS-1441561]CSCL 2017 Proceedings687© ISLS