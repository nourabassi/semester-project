Students’ Responses to Curricular Activities as Indicator ofCoherence in Project-Based ScienceWilliam R. Penuel, Katie Van Horne, Samuel Severance, David Quigley, and Tamara Sumnerwilliam.penuel@colorado.edu, katie.vanhorne@colorado.edu, samuel.severance@colorado.edu,david.quigley@colorado.edu, sumner@colorado.eduUniversity of Colorado BoulderAbstract: Project-based learning seeks to engage students through sustained investigation ofreal-world problems or design challenges. Weekly mini-surveys were administered to studentsduring an 8-week project-based learning unit to understand students’ perceptions of alignmentof lessons to the overall challenge, their affective responses to lessons, and how these variedacross lesson types and teachers. Results from a multilevel model revealed significant teacherlevel variance; no differences across lesson types were found.Keywords: project-based learning, science, practical measurement, coherence, student affectIntroductionProject-based learning aims to support the development of deep understanding of subject matter by engagingstudents in sustained investigation of problems (Blumenfeld, Soloway, Marx, Guzdial, & Palincsar, 1991;Edelson, 2001). Organizing projects around specific questions or design challenges is intended to make projectscohere from students’ point of view (Krajcik & Mamlok-Naaman, 2006). Challenges also provide motivation forstudents to develop knowledge components related to disciplinary core ideas (Schwartz & Bransford, 1998).Successful projects require students’ sustained effort, since answering driving questions and solvingdesign challenges unfolds typically over many lessons and weeks (Blumenfeld et al., 1991). Projects withsufficient novelty and authenticity are hypothesized to capture and sustain student interest in subject matterlearning, by helping students connect disciplinary ideas to real-world phenomena (Blumenfeld et al., 1991;Polman, 2012).However, developers cannot presume that a particular driving question will sustain students’ attention.Student motivation depends on their perceptions of the value of the tasks, and the alignment of the specific taskswith the overall phenomenon (Pitts, 2006). Moreover, engagement may vary across lessons of a unit, in ways thatare attributable to both lesson characteristics and individual differences of students (Pitt, 2006). When youth viewtasks as contributing toward answering a driving question or design challenge, they may perceive tasks as morevaluable. Different lesson structures may also influence these perceptions.We know that variation in teaching practices explains differences in student outcomes in project-basedscience instruction (e.g., Fogleman, McNeill, & Krajcik, 2011; Harris, Phillips, & Penuel, 2012). We expect, then,that student experiences of project-based learning vary not only by lesson but also by teacher. In classrooms whereteachers’ enactments of lessons emphasize connections between tasks and driving questions or challenges,students are likely to experience tasks as valuable because they see those connections themselves. To date,research has not examined teacher-level differences in student experiences of project-based learning.MethodsIn this study within a larger design-based implementation research (Penuel, Fishman, Cheng, & Sabelli, 2011)project, we gathered data from students using an electronic “mini-survey” about their experiences of individuallessons in a project-based unit on ecosystems. Our primary purposes were (1) to develop implementation evidencerelated to the experienced coherence of the unit and (2) to contribute to knowledge related to student engagementin project-based learning.The focal unit is an 8-week project-based unit organized around a design challenge: “Select a species oftree to plant in your schoolyard that will maximize biodiversity and ecosystem services.” The unit is intended todevelop understanding of disciplinary core ideas in the life sciences identified in A Framework for K-12 ScienceEducation (National Research Council, 2012). We intended our mini-survey to serve as a “practical measure”(Yeager, Bryk, Muhich, Hausman, & Morales, 2013) that could be implemented by teachers on a regular basis toinform the iterative refinement of individual lessons in the unit.ICLS 2016 Proceedings855© ISLSSampleData from the study come from 592 students of 11 teachers from eight schools in a large urban school district inthe United States West region. The majority of students in the district are Hispanic and 69% participate in thefree/reduced lunch program. Our data sample consists of 1,223 surveys submitted by participating students fromAugust 25 through October 28, 2015.Student mini-surveyTeachers administered the student mini-survey on a weekly basis. The analysis presented here focuses onresponses to two items presented in the survey: (1) “We learned about something today that connects to thechallenge.” (Yes, No, Not sure), and (2) “Today in science class, I felt...” (Excited, Bored, Like a Scientist). Wedid not seek to construct scales from these measures, in accordance with practical measurement’s focus on a fewindicators judged to be of central concern for implementation (Bryk, Gomez, Grunow, & LeMahieu, 2015).The first item addresses students’ perceptions of the alignment of the day’s tasks with the overall designquestion. The second addresses their understanding of the purpose of the day’s lesson. The third addresses theiraffective response. This item was adapted from an earlier study of students’ responses to an elementary-levelproject-based unit in science (Morozov et al., 2014), and we used it to allow for comparisons across projects. Weconjectured that there would be positive associations among perceptions of alignment, understanding of thepurpose for the day’s lesson, and feelings of excitement and identification with science.Approach to analysisWe fit multilevel models to the data using the software HLM 7.0, which were appropriate to the nested nature ofour data. Models each had three levels: observation or occasion, student, and teacher. Each teacher had multiplestudents, and each student had between 1 and 8 different observations where they completed surveys. In onemodel, ratings of connectedness to the challenge over time were outcomes we sought to model in order tounderstand variation associated with both teacher and lesson type. Lessons were grouped into two types: thoseemphasizing discursive practices (e.g., argumentation) and those emphasizing investigation practices (e.g.,planning and conducting an investigation). This nesting structured necessitated a three level model. The threelevel model of our outcomes include time-dependent student predictors in level one (e.g., affective measures andwhether or not students rated the lesson as hands-on or discursive). In a second set of models, students’emotional responses to lessons (excited versus bored) were outcomes, and we modeled variation associated withteacher and with individual students’ ratings of connectedness to the challenge over time.FindingsResults from each of the three-level hierarchical linear model we fit revealed significant teacher level variance;no differences across lesson types were found. Specifically, the unconditional model of outcome “connected tochallenge” produced significant variance at the teacher level. Of the total variance in the model, 30.5% was at theteacher level. Table 1 shows the results of a model that explores the degree to which lesson type accounts forvariation in student ratings that a given lesson was connected to the overall challenge. Neither lesson type wassignificantly associated with type of lesson, though the probability of an investigation-focused lesson being ratedas connected to the challenge was higher than for discursive-focused lessons (43% versus 32%). A significantpercent of variance remained at the teacher level, when these predictors were included in the model.Table 1: Model of lesson connected to the challenge with type of challenge as predictors.Outcome - ModelConnected to ChallengeUnconditional ModelConnected to ChallengeType of LessonPredictorInvestigationfocusedDiscursivefocusedCoefficient inLog Odds(se)Coefficient inProbability-0.28(0.33)-0.37(0.20)0.43% Variance atTeacher Level30.5%34.7%0.32A second model focused on predicting whether students reported feeling excited during the lesson. Anunconditional model fit to the data revealed a significant percent of variance at the teacher level (13.7%). OnceICLS 2016 Proceedings856© ISLSstudent ratings of whether the lesson was connected to the challenge were included as a predictor in the model,the percentage of teacher variance changed to 40.1%. Ratings of connectedness to the challenge were significantlyassociated with students’ reports of being excited during the class. Seventy percent of lessons that studentsreported being connected to the challenge were ones where they also reported being excited.Table 2: Model of excited emotion with lesson connected to the challenge.Outcome - ModelPredictorExcitedExcitedConnected to challengeCoefficient inlog odds (se)Coefficient inprobabilityConnected tochallenge0.84*(0.37)0.70% of Variance atthe Teacher Level13.7%40.1%A third model focused on predicting whether students reported being bored during the lesson. An unconditionalmodel fit to the data revealed a significant percent of variance at the teacher level (29.9%). Once student ratingsof whether the lesson was connected to the challenge were included as a predictor in the model, the percentage ofteacher variance jumped to 40.6%. Ratings of connectedness to the challenge were significantly associated withstudents’ reports of being excited during the class. Thirty-one percent of lessons that students reported beingconnected to the challenge were ones where they also reported being bored.Table 3: Model of bored emotion with lesson connected to the challenge.Outcome - ModelPredictorBoredBoredConnected to challengeCoefficient inlog odds (se)Coefficient inprobabilityConnected tochallenge-0.79(0.48)0.31% of Variance atthe Teacher Level29.9%40.6%Conclusions and implicationsThese findings indicate that teacher-level differences influence student experiences of project-based learning.Teacher-level differences influenced the degree to which students perceived their learning tasks to be connectedto the unit’s design challenge and the tasks’ overall usefulness outside of class. In turn, teachers’ abilities to helpstudents connect tasks and lessons to the unit challenge may influence students’ affective responses, such asfeeling excited during project-based learning tasks. We found no differences across lesson types, which contrastswith the findings reported by Pitt (2006).These findings have multiple implications for practice and research. First, these findings underscore thatteachers need professional development that specifically targets project-based learning instructional practices.Helping students make connections between daily tasks and lessons, and larger unit goals, may be critical formaintaining student interest and engagement during the sustained, in depth investigations typical of scienceprojects. Furthermore, curriculum materials could be enhanced with instructional routines, for instance duringlesson opening and closing, that support students in making relevant connections.Second, our experiences suggest that there is great utility in these relatively simple mini-surveys forgathering rapid feedback from learners as part of a design-based research process. For instance, we have comparedstudent self-report of their affective responses across successive design and implementation cycles to gauge thedegree to which changes in unit tasks and materials were “improving” the unit with respect to student engagement.Asking students to assess the degree to which a daily task or lesson is related to a larger unit challenge also offersa new “student-centered” way of looking at and measuring the coherence of instruction. This approach is apotential complement to other assessments of curricular coherence, which focus on analyzing the connectednessof ideas as represented in instructional materials (Kesidou & Roseman, 2002). By asking students directly, we aretapping into the coherence of the “experienced curriculum” rather than the “formal” curriculum (Gehrke, Knapp,& Sirotnik, 1992), thus shifting the measure to better reflect the actual student experience. In future analysis, wewill compare these student self-report data with classroom observations conducted as part of this larger researchprogram to develop additional evidence related to validity of this approach to studying coherence as experiencedby students.ICLS 2016 Proceedings857© ISLSReferencesBlumenfeld, P., Soloway, E., Marx, R. W., Guzdial, M., & Palincsar, A. S. (1991). Motivating project-basedlearning: Sustaining the doing, supporting the learning. Educational Psychologist, 26(3&4), 369-398.Bryk, A. S., Gomez, L. M., Grunow, A., & LeMahieu, P. (2015). Learning to improve: How America's schoolscan get better at getting better. Cambridge, MA: Harvard University Press.Edelson, D. C. (2001). Learning-for-use: A framework for the design of technology-supported inquiry activities.Journal of Research in Science Teaching, 38(3), 355-385.Fogleman, J., McNeill, K. L., & Krajcik, J. (2011). Examining the effect of teachers' adaptations of a middleschool science inquiry-oriented curriculum unit on student learning. Journal of Research in ScienceTeaching, 48(2), 149-169.Gehrke, N. J., Knapp, M. S., & Sirotnik, K. A. (1992). In search of the school curriculum. Review of Research inEducation, 18, 51-110.Harris, C. J., Phillips, R. S., & Penuel, W. R. (2012). Examining teachers’ instructional moves aimed at developingstudents' ideas and questions in learner-centered science classrooms. Journal of Science TeacherEducation, 23(7), 768-788. Retrieved from http://dx.doi.org/10.1007/s10972-011-9237-0Kesidou, S., & Roseman, J. E. (2002). How well do middle school science programs measure up? Findings fromProject 2061's curriculum review. Journal of Research in Science Teaching, 39(6), 522-549.Krajcik, J. S., & Mamlok-Naaman, R. (2006). Using driving questions to motivate and sustain student interest inlearning science. In K. Tobin (Ed.), Teaching and learning science: An encyclopedia (pp. 317-327).Westport, CT: Greenwood Publishing.Morozov, A., Herrenkohl, L., Shutt, K., Thummaphan, P., Vye, N., Abbott, R. D., & Scalone, G. (2014).Emotional engagement in agentive science environments. In J. L. Polman, E. Kyza, K. O'Neill, & I.Tabak (Eds.), Proceedings of the 11th International Conference of the Learning Sciences (pp. 11521156). Boulder, CO: International Society of the Learning Sciences.National Research Council. (2012). A framework for K-12 science education: Practices, crosscutting concepts,and core ideas. Washington, DC: National Research Council.Penuel, W. R., Fishman, B. J., Cheng, B., & Sabelli, N. (2011). Organizing research and development at theintersection of learning, implementation, and design. Educational Researcher, 40(7), 331-337.Pitts, V. M. (2006). Do students buy in? A study of goal and role adoption by students in project-based curricula.(doctoral dissertation), Northwestern University.Polman, J. L. (2012). Trajectories of participation and identification in learning communities involvingdisciplinary practices. In D. Y. Dai (Ed.), Design research on learning and thinking in educationalsettings: Enhancing intellectual growth and functioning (pp. 225-242). New York, NY: Routledge.Schwartz, D. L., & Bransford, J. D. (1998). A time for telling. Cognition and Instruction, 16(4), 475-522.Yeager, D., Bryk, A. S., Muhich, J., Hausman, H., & Morales, L. (2013). Practical measurement. Palo Alto, CA:Carnegie Foundation for the Advancement of Teaching.AcknowledgmentsWe thank participating teachers and students, as well as our school district partner. This material is based uponwork supported by the National Science Foundation under Grant No. 1147590. Any opinions, findings, andconclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflectthe views of the National Science Foundation.ICLS 2016 Proceedings858© ISLS