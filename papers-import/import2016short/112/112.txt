Supporting Planning and Conducting ExperimentsSiswa A. N. van Riesen, Hannie Gijlers, Anjo Anjewierden, and Ton de Jong,s.a.n.vanriesen@utwente.nl, a.h.gijlers@utwente.nl, a.a.anjewierden@utwente.nl, a.j.m.dejong@utwente.nlUniversity of TwenteAbstract: In inquiry learning learners design and conduct experiments. Learners experiencedifficulties with the involved processes and need guidance to design useful experiments. Toguide students in this we created a configurable experiment design tool that is usable in multipledomains. The tool was tested with two configurations; one with a CVS structure in whichlearners had to design at least three experimental trials before conducting their experiment, andone in which this was not required. In the current study secondary students designed andconducted experiments in an online lab about buoyancy and Archimedes’ principle. Threeconditions were compared in terms of students’ conceptual knowledge gain. Students workedwith one configuration of the tool, or with no tool. Results showed significant differencesbetween conditions for lower prior knowledge students’ learning gain about buoyancy.IntroductionInquiry learning stimulates learners to actively construct their own knowledge by means of doing investigations,allowing them to gain higher-order understandings, instead of passively absorbing information presented to them.Learners follow (part of an) inquiry cycle that comprises orienting on the topic of interest, formulating hypothesesand/or research questions, setting up and conducting experiments, drawing conclusions, and reflecting upon theirinquiry (Pedaste et al., 2015). Moreover, inquiry learning promotes learners’ autonomous working attitudes andinquiry skills, both of which are important educational objectives in current curricula worldwide; it also promotesa positive attitude towards learning, and it motivates them to acquire, integrate, and apply new knowledge(Edelson, Gordin, & Pea, 1999).An important phase of inquiry learning is the investigation phase during which learners design andconduct experiments to test a hypothesis or answer a research question. Based on results from their experimentsthey analyse their data and draw conclusions accordingly. The experimentation phase thus builds a bridge betweenthe hypothesis or research question and the analysis of the data.However, learners find it difficult to design valuable experiments (de Jong & van Joolingen, 1998). Itinvolves several processes and requires understanding of inquiry. They need to understand that they have to designexperiments with which they can test their hypothesis or answer their research question. Often learners designexperiments that do not comply with their hypothesis or research question, for instance by including variables thathave nothing to do with it (de Jong & van Joolingen, 1998).After selecting relevant variables, learners need to determine what to measure (dependent variable), vary(independent variable) and control for (controlled variable). Then they have to assign values to the independentand controlled variables. Learners often vary too many variables, which makes it difficult to draw correctconclusions because any effect that occurs may be due to a variety of influences. An effective strategy oftenapplied by professional researchers is the Control of Variable Strategy (CVS) in which only one variable of interestis varied and all other variables are kept constant (Klahr & Nigam, 2004). CVS allows learners to draw conclusionsfrom unconfounded experiments.In order to successfully learn from experimentation learners must plan and apply systematic ways ofdesigning experiments (de Jong & Njoo, 1992). However, research indicates that learners tend not to analyse atask or problem they have to solve, but to act immediately, without planning (Manlove, Lazonder, & de Jong,2006). If learners do engage in planning, they often use unsystematic ways, which may cause them to strugglewith the task (de Jong & van Joolingen, 1998).Guiding learners in planning and conducting experiments helps them to design useful and systematicexperiments from which they can derive knowledge (Zacharia et al., 2015). In computer supported inquirylearning environments some of the most often used forms of guidance are heuristics and tools. Heuristics are hintsor suggestions about how to carry out assignments, actions, or learning processes. Examples of heuristics to directlearners to apply the CVS strategy are ‘vary one thing at a time (VOTAT)’, and ‘control all other variables byusing the same value across experimental trials’ (Veermans, van Joolingen, & de Jong, 2006). Tools can transformor take over part of a task and thereby help learners accomplish tasks they would be unable to do on their own (deJong, 2006). An example is a monitoring tool in which experiments are stored (Veermans, de Jong, & vanJoolingen, 2000). Learners can replay conducted trials, and rearrange them in ascending or descending order toICLS 2016 Proceedings823© ISLSbe better able to compare results. It eliminates the difficulty of remembering conducted experimental trials,interpreting results, and simultaneously thinking of appropriate follow-up trials.Based on heuristics and scaffolding elements that have shown to be effective for learning, an ExperimentDesign Tool (EDT) was developed that can be applied in different domains and configured so that it fits teachers’intentions with the inquiry learning activity. In the current study two configurations of the EDT were comparedin terms of effectiveness regarding students’ learning gain about buoyancy and Archimedes’ principle. Oneconfiguration incorporated the CVS-strategy and required planning; learners were obliged to apply CVS and toplan multiple trials before conducting their experiment. The other configuration had a more exploratory character;learners were free to conduct their designed trials when they wanted to and were not obliged to apply CVS.MethodIn the current study students planned and conducted experiments in an online learning environment aboutbuoyancy and Archimedes’ Principle. Three learning environments were compared with different levels of supportfor planning and conducting experiments, but that were the same in all other aspects. In two learning environmentsstudents received additional support for planning and conducting experiments by means of one of the twoconfigurations of the EDT. In the third learning environment students were not guided by an additional tool.ParticipantsA total of 159 third grade pre-university students (aged 15) from three secondary schools in the Netherlands wererandomly assigned to one of the conditions. After eliminating outliers and students that missed a session -e.g. oneclass missed a session because of an overlooked field trip- 104 students remained for analyses.Learning environmentsThe three learning environments in which students worked were all structured in similar ways. They all consistedof instructions, research questions, a virtual lab, a mechanism to prepare experiments, a help button to retrievedomain information, and a conclusion text box. Upon entering the environment, instructions appeared explainingthat the student had to design experiments and conduct those in a virtual lab in order to answer research questions.The environment contained a total of fourteen questions presented one by one. For each research question studentshad to design and conduct an experiment, and draw a conclusion accordingly. Once students had submitted aconclusion, a new research question appeared for which they again had to design and conduct an experiment.(a)(b)Figure 1. The learning environments for the (a) control group, and (b) EDT conditions.The learning environments only differed in the support offered to students (Figure 1). The environmentof the control condition did not contain a tool to help plan experiments. Experiments were prepared and conducteddirectly in the lab by means of sliders to adjust the values of the variables in the experiment.The environments of the EDT conditions each contained one configuration of the EDT to guide thepreparation and conduction of experiments. It provided students with structure in the form of a table thatincorporated and emphasised three types of variables (independent, controlled and dependent). Students couldselect variables from a box and decide per variable if they wanted to vary it across experimental trials, keep itICLS 2016 Proceedings824© ISLSconstant, or measure it. After they specified the variables, they assigned values -within a restricted range- to theindependent and controlled variables. For each independent variable they specified a different value per trial. Foreach controlled variable they assigned one value; the EDT automatically assigned that same value to all trialswithin the experiment. Results for the dependent variable could only be entered after the trials were conducted. Inaddition to preparing the experiments, the EDT offered a second table in which all the previously conducted trialswere presented. In this table, variables could be sorted in ascending or descending order, making it easier to reachconclusions or decide if more trials or even experiments were required to answer the research question. At alltimes, students could see instructions about how to operate the EDT. The instructions were just-in-time and werebased on students’ actions. For example, when students started planning an experiment, they were instructed todrag and drop all properties to the boxes vary or keep constant, and to drag at least one variable to measure. Thetwo configurations of the EDT differed in three aspects. In one configuration students were obliged to 1) applyCVS, 2) plan at least three trials before conducting experiments, and they 3) received different instructions thatwere congruent with these two aspects. In the other configuration students were able to, but not obliged to, do thesame and they received instructions congruent with the configuration.AssessmentStudents’ conceptual knowledge of buoyancy and Archimedes’ principle was assessed both before and after theintervention with a parallel pre- and post-test. The test consisted of 58 open questions that measured students’understanding of the key concepts and principles of the topics in the virtual lab.ProcedureThe study was performed during four sessions of 50 to 60 minutes each, over a period of two and a half weeks. Inthe first session the students received instructions about what they were going to do. Thereafter they had half anhour to complete the pre-test, which was sufficient for all students to finish. Finally, they were randomly assignedto a condition, received instructions about the upcoming tasks, the learning environment was shown, and theycould ask any question. During the second session students first received a booklet matching the condition theywere assigned to and then individually worked with the learning environment behind a computer to learn aboutbuoyancy. The booklet contained instructions about the tasks, and all the research questions they had to answerduring the session. The third session was similar to the second session; students also worked with the learningenvironment but the topic of investigation was Archimedes’ principle instead of buoyancy. During the fourthsession students took the post-test and were informed about the purpose of the study.ResultsA significant conceptual knowledge learning gain was found in all conditions for buoyancy (control condition:n = 34, Z = 3.226, p = .001; exploratory EDT condition: n = 33, Z = 3.302, p = .001; more structured EDTcondition: n = 37, Z = 3.015, p = .003) and for Archimedes’ principle (control condition: n = 34, Z = 3.554, p <.001; exploratory EDT condition: n = 33, Z = 2.943, p = .003; more structured EDT condition: n = 37, Z = 2.757,p = .006) using Wilcoxon signed-rank tests. Independent-Samples Kruskal-Wallis Tests showed no significantdifferences between the conditions for both parts of the test (buoyancy: H (2) = .253, p = .881; Archimedes’principle: H (2) = .651, p = .722).However, research shows that tools can be especially effective for low prior knowledge students. Wedivided students in two groups based on pre-test scores; one group included students with the 50% lowest scoresand the other group included students with the 50% highest scores. Independent-Samples Kruskal-Wallis Testsdemonstrated a significant difference in learning gain between conditions for lower prior knowledge students onbuoyancy, H (2) = 6.17, p = .046, in favour of the exploratory EDT condition (control: M = 7.17, SD = 5.68;exploratory EDT: M = 10.93, SD = 5.28; more structured EDT: M = 5.75, SD = 6.58), but not for Archimedes’principle. Also, no significant difference was found for higher prior knowledge students.Conclusions and implicationsThe current study showed a different effect of guidance for lower prior knowledge students, who performed betterwith guidance in the form of an exploratory EDT on the first domain, than for higher prior knowledge students,who did equally well with and without guidance, which is in line with other research. Higher prior knowledgestudents often demonstrate more well-structured, goal-oriented inquiry behaviour; they use more sophisticatedstrategies to induce knowledge and encounter less problems than lower prior knowledge students (Hmelo,Nagarajan, & Roger, 2000). Additional support for higher prior knowledge students has found to be redundant,because they already have sufficient knowledge to construct mental representations (Kalyuga, 2007).ICLS 2016 Proceedings825© ISLSResearch about the level of support for lower prior knowledge students shows that they find it difficultto interpret support, and perform better when they first have the opportunity to explore the domain of interest bythemselves rather than immediately starting with systematic ways of designing experiments. However, researchalso suggests that these learners benefit from more support because additional guidance helps overcome missingschemas and reduces working memory load (Roll, Briseno, Yee, & Welsh, 2014). Interestingly, the current studyshowed that lower prior knowledge learners performed best when they were guided by a tool, bút this effect wasonly found in the first domain and the tool had to be configured so that students could still explore the domainwithout too many restrictions. The different effect of guidance for buoyancy and Archimedes’ principle might beexplained by the level of difficulty of the two domains. Buoyancy -the first domain of experimentation- isgenerally regarded as easier than Archimedes’ principle by students. We hypothesise that students receivedenough support to let them perform better on the tasks within buoyancy, whereas they may have needed additionalsupport for the more difficult topic of Archimedes’ principle. Future studies should focus on students’ inquiryprocesses and the level and form of support they need in distinct domains with different levels of difficulty.Referencesde Jong, T. (2006). Computer simulations - Technological advances in inquiry learning. Science, 312, 532-533.de Jong, T., & Njoo, M. (1992). Learning and instruction with computer simulations: Learning processes involvedComputer-based learning environments and problem solving (pp. 411-427): Springer.de Jong, T., & van Joolingen, W. R. (1998). Scientific discovery learning with computer simulations of conceptualdomains. Review of Educational Research, 68, 179-201. doi:10.2307/1170753Edelson, D. C., Gordin, D. N., & Pea, R. D. (1999). Addressing the challenges of inquiry-based learning throughtechnology and curriculum design. Journal of the Learning Sciences, 8, 391-450.Hmelo, C. E., Nagarajan, A., & Roger, S. (2000). Effects of high and low prior knowledge on construction of ajoint problem space. Journal of Experimental Education, 69, 36-56. doi:10.1080/00220970009600648Kalyuga, S. (2007). Expertise reversal effect and its implications for learner-tailored instruction. EducationalPsychology Review, 19, 509-539. doi:10.1007/s10648-007-9054-3Klahr, D., & Nigam, M. (2004). The equivalence of learning paths in early science instruction: effect of directinstruction and discovery learning. Psychol Sci, 15, 661-667. doi:10.1111/j.0956-7976.2004.00737.xManlove, S., Lazonder, A. W., & de Jong, T. (2006). Regulative support for collaborative scientific inquirylearning. Journal of Computer Assisted Learning, 22, 87-98. doi:10.1111/j.1365-2729.2006.00162.xPedaste, M., Maeots, M., Siiman, L. A., de Jong, T., van Riesen, S. A. N., Kamp, E. T., . . . Tsourlidaki, E. (2015).Phases of inquiry-based learning: Definitions and the inquiry cycle. Educational research review, 14,47-61. doi:10.1016/j.edurev.2015.02.003Roll, I., Briseno, A., Yee, N., & Welsh, A. (2014). Not a magic bullet: The effect of scaffolding on knowledge andattitudes in online simulations. Paper presented at the Learning and becoming in practice: TheInternational Conference of the Learning Sciences (ICLS) 2014, Boulder, Colorado, USA.Veermans, K., de Jong, T., & van Joolingen, W. R. (2000). Promoting self-directed learning in simulation-baseddiscovery learning environments through intelligent support. Interactive Learning Environments, 8, 229255. doi:10.1076/1049-4820(200012)8:3;1-D;FT229Veermans, K., van Joolingen, W. R., & de Jong, T. (2006). Use of heuristics to facilitate scientific discoverylearning in a simulation learning environment in a physics domain. International Journal of ScienceEducation, 28, 341-361. doi:10.1080/09500690500277615Zacharia, Z. C., Manoli, C., Xenofontos, N., de Jong, T., Pedaste, M., van Riesen, S. A. N., . . . Tsourlidaki, E.(2015). Identifying potential types of guidance for supporting student inquiry when using virtual andremote labs in science: A literature review. Educational technology research and development, 63(2),257-302. doi:10.1007/s11423-015-9370-0AcknowledgmentThe reported work was partially funded by the European Union in the context of the Go-Lab project (GrantAgreement no. 317601) under the Information and Communication Technologies (ICT) theme of the 7thFramework Programme for R&D (FP7). This document does not represent the opinion of the European Union,and the European Union is not responsible for any use that might be made of its content.ICLS 2016 Proceedings826© ISLS