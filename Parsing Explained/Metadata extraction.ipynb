{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline  \n",
    "import seaborn as sns\n",
    "import regex as reg\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import regex #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the metadata:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a more annotated indepth explination on the code implememted in the parsing script:\n",
    "\n",
    "The following function extracts the metadata provided in the xml files. We iteratively parse the file as it does not follow standard xml format perfectly, hence using a regular parser would lead to errors being thrown.  The reason the xml can not be parsed with a regular paser is that the following tag is not present: `xmlns:dc =\"http://purl.org/dc/elements/1.1/\"` . This is not an issue in the web version. \n",
    "\n",
    "The information we want to extract is in the sections of the qualifier, element and subject attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(file_path): \n",
    "    i = 0 \n",
    "    tmp = ''\n",
    "    key = ''\n",
    "    xml2 = etree.iterparse(file_path, recover=True)\n",
    "    data = []\n",
    "    for action, elem in xml2:\n",
    "        data.append((elem.attrib, elem.tag, elem.text))\n",
    "    data_dict = {}\n",
    "    \n",
    "    \n",
    "    for attrib, tag, text in data: \n",
    "        try : \n",
    "            tmp = key\n",
    "            \n",
    "            key = attrib.get('qualifier')\n",
    "            element = attrib.get('element')\n",
    "            \n",
    "            #way to distinguish eliminate some nan!\n",
    "            if key == 'none':\n",
    "                key = element\n",
    "            \n",
    "            if key in data_dict.keys() : \n",
    "                i = i + 1 \n",
    "                data_dict[key + str(i)] = text\n",
    "            else : \n",
    "                i = 0 \n",
    "                data_dict[key] = text\n",
    "                \n",
    "        except TypeError: \n",
    "            if 'subject' in tag:\n",
    "                if 'subject' in data_dict.keys():\n",
    "                    data_dict['subject'].append(text)\n",
    "                else:\n",
    "                    data_dict['subject'] = [text]\n",
    "                    \n",
    "        #parse irregular files\n",
    "        if 'subject' not in data_dict.keys():\n",
    "            f = open(file_path)\n",
    "            text = f.read()\n",
    "            data_dict['subject'] = reg.findall('&gt;([\\w\\-\\ \\;]*)&lt;', text)\n",
    "            if len(data_dict['subject']) == 1:\n",
    "                data_dict['subject'] = reg.split(';', data_dict['subject'][0])\n",
    "            \n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all data \n",
    "\n",
    "We go over all the folders and parse all the xml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "rootdir = '../papers-import/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {}\n",
    "i= 0\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        path = os.path.join(subdir, file)\n",
    "        if 'dublin_core' in (path) :\n",
    "            i += 1\n",
    "            num_doc = subdir[len(rootdir):]\n",
    "            all_data[num_doc] = parse(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got data from 874 files\n"
     ]
    }
   ],
   "source": [
    "print('got data from {} files'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add all the xml data into one dataframe\n",
    "df_data = pd.DataFrame(list(all_data.values()), index=all_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to datetime format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(arg): \n",
    "    \"\"\"Convert string to date time format\"\"\"\n",
    "    try : \n",
    "        arg = dateutil.parser.parse(arg)\n",
    "    except TypeError: \n",
    "        arg = arg \n",
    "    return arg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.parser\n",
    "df_data['available'] = df_data['available'].apply(lambda x : convert(x))\n",
    "df_data['accessioned'] = df_data['accessioned'].apply(lambda x : convert(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of columns in the dataframe. We have one paper per row, with authors in the author columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([   'abstract', 'accessioned',      'author',     'author1',\n",
       "           'author2',     'author3',   'available',    'citation',\n",
       "               'iso',      'issued',   'publisher',     'subject',\n",
       "             'title',        'type',         'uri',          None,\n",
       "           'author4',     'author5',     'author6',     'author7',\n",
       "           'author8',     'author9',    'author10',    'author11',\n",
       "          'author12',    'author13',    'author14',    'author15',\n",
       "          'author16',    'author17',    'author18',    'author19',\n",
       "          'author20',    'author21',    'author22',    'author23',\n",
       "          'author24',    'author25',    'author26'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting all authors in to one dataframe:\n",
    "\n",
    "Now we will change the above dataframe structure so that we only have one author column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go from multiple author columns to a single one\n",
    "cleaning = df_data.reset_index().melt(\n",
    "    id_vars=['index','subject', 'iso', 'uri','type','publisher','title', \n",
    "             'issued', 'accessioned', 'citation', 'available', 'abstract'])\n",
    "cleaning = cleaning[cleaning.value.notna()]\n",
    "cleaning = cleaning[cleaning.variable.notna()]\n",
    "\n",
    "#remove garbage entries\n",
    "cleaning = cleaning[cleaning.value.map(lambda x: len(x) > 2)]\n",
    "#remove na\n",
    "cleaning = cleaning[cleaning.value.notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associate an order to the authors: this will later be used to merge with the exracted emails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "cleaning['author_order'] = cleaning.variable.map(\n",
    "    lambda x: 0 if len(re.search('\\d*$', x).group(0)) == 0 else int(re.search('\\d*$', x).group(0)))\n",
    "\n",
    "del cleaning['variable']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe currently looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>subject</th>\n",
       "      <th>iso</th>\n",
       "      <th>uri</th>\n",
       "      <th>type</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>issued</th>\n",
       "      <th>accessioned</th>\n",
       "      <th>citation</th>\n",
       "      <th>available</th>\n",
       "      <th>abstract</th>\n",
       "      <th>value</th>\n",
       "      <th>author_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>import2016full/61</td>\n",
       "      <td>[attentional anchor, coordination, eye-trackin...</td>\n",
       "      <td>en</td>\n",
       "      <td>info:doi/10.22318/icls2016.61</td>\n",
       "      <td>Book chapter</td>\n",
       "      <td>Singapore: International Society of the Learni...</td>\n",
       "      <td>Exposing Piaget’s Scheme: Empirical Evidence f...</td>\n",
       "      <td>2016-07</td>\n",
       "      <td>2017-03-21 12:05:42+00:00</td>\n",
       "      <td>Abrahamson, D., Shayan, S., Bakker, A., &amp; van ...</td>\n",
       "      <td>2017-03-21 12:05:42+00:00</td>\n",
       "      <td>The combination of two methodological resource...</td>\n",
       "      <td>Abrahamson, Dor</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>import2016full/95</td>\n",
       "      <td>[teacher learning, science teaching, professio...</td>\n",
       "      <td>en</td>\n",
       "      <td>info:doi/10.22318/icls2016.95</td>\n",
       "      <td>Book chapter</td>\n",
       "      <td>Singapore: International Society of the Learni...</td>\n",
       "      <td>Secondary Teachers’ Emergent Understanding of ...</td>\n",
       "      <td>2016-07</td>\n",
       "      <td>2017-03-21 12:05:42+00:00</td>\n",
       "      <td>Sandoval, W. A., Kawasaki, J., Cournoyer, N., ...</td>\n",
       "      <td>2017-03-21 12:05:42+00:00</td>\n",
       "      <td>Abstract: The Next Generation Science Standard...</td>\n",
       "      <td>Sandoval, William A.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>import2016full/59</td>\n",
       "      <td>[play,  games,  science inquiry,  embodied cog...</td>\n",
       "      <td>en</td>\n",
       "      <td>info:doi/10.22318/icls2016.59</td>\n",
       "      <td>Book chapter</td>\n",
       "      <td>Singapore: International Society of the Learni...</td>\n",
       "      <td>Blending Play and Inquiry in Augmented Reality...</td>\n",
       "      <td>2016-07</td>\n",
       "      <td>2017-03-21 12:05:42+00:00</td>\n",
       "      <td>DeLiema, D., Saleh, A., Lee, C., Enyedy, N., D...</td>\n",
       "      <td>2017-03-21 12:05:42+00:00</td>\n",
       "      <td>Researchers have increasingly demonstrated how...</td>\n",
       "      <td>DeLiema, David</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>import2016full/92</td>\n",
       "      <td>[collaborative learning, conceptual convergenc...</td>\n",
       "      <td>en</td>\n",
       "      <td>info:doi/10.22318/icls2016.92</td>\n",
       "      <td>Book chapter</td>\n",
       "      <td>Singapore: International Society of the Learni...</td>\n",
       "      <td>Making Sense of Making Waves: Co-constructing ...</td>\n",
       "      <td>2016-07</td>\n",
       "      <td>2017-03-21 12:05:42+00:00</td>\n",
       "      <td>Hardy, L. &amp; White, T. (2016). Making Sense of ...</td>\n",
       "      <td>2017-03-21 12:05:42+00:00</td>\n",
       "      <td>In this paper we argue that collaborative lear...</td>\n",
       "      <td>Hardy, Lisa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>import2016full/66</td>\n",
       "      <td>[ESL, teachers, coaching, student learning]</td>\n",
       "      <td>en</td>\n",
       "      <td>info:doi/10.22318/icls2016.66</td>\n",
       "      <td>Book chapter</td>\n",
       "      <td>Singapore: International Society of the Learni...</td>\n",
       "      <td>The Effects of Coaching on the Teaching and Le...</td>\n",
       "      <td>2016-07</td>\n",
       "      <td>2017-03-21 12:05:42+00:00</td>\n",
       "      <td>Raval, H., Kaul, C., &amp; McKenney, S. (2016). Th...</td>\n",
       "      <td>2017-03-21 12:05:42+00:00</td>\n",
       "      <td>Although English is mandatorily introduced as ...</td>\n",
       "      <td>Raval, Harini</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               index                                            subject iso  \\\n",
       "0  import2016full/61  [attentional anchor, coordination, eye-trackin...  en   \n",
       "1  import2016full/95  [teacher learning, science teaching, professio...  en   \n",
       "2  import2016full/59  [play,  games,  science inquiry,  embodied cog...  en   \n",
       "3  import2016full/92  [collaborative learning, conceptual convergenc...  en   \n",
       "4  import2016full/66        [ESL, teachers, coaching, student learning]  en   \n",
       "\n",
       "                             uri          type  \\\n",
       "0  info:doi/10.22318/icls2016.61  Book chapter   \n",
       "1  info:doi/10.22318/icls2016.95  Book chapter   \n",
       "2  info:doi/10.22318/icls2016.59  Book chapter   \n",
       "3  info:doi/10.22318/icls2016.92  Book chapter   \n",
       "4  info:doi/10.22318/icls2016.66  Book chapter   \n",
       "\n",
       "                                           publisher  \\\n",
       "0  Singapore: International Society of the Learni...   \n",
       "1  Singapore: International Society of the Learni...   \n",
       "2  Singapore: International Society of the Learni...   \n",
       "3  Singapore: International Society of the Learni...   \n",
       "4  Singapore: International Society of the Learni...   \n",
       "\n",
       "                                               title   issued  \\\n",
       "0  Exposing Piaget’s Scheme: Empirical Evidence f...  2016-07   \n",
       "1  Secondary Teachers’ Emergent Understanding of ...  2016-07   \n",
       "2  Blending Play and Inquiry in Augmented Reality...  2016-07   \n",
       "3  Making Sense of Making Waves: Co-constructing ...  2016-07   \n",
       "4  The Effects of Coaching on the Teaching and Le...  2016-07   \n",
       "\n",
       "                accessioned  \\\n",
       "0 2017-03-21 12:05:42+00:00   \n",
       "1 2017-03-21 12:05:42+00:00   \n",
       "2 2017-03-21 12:05:42+00:00   \n",
       "3 2017-03-21 12:05:42+00:00   \n",
       "4 2017-03-21 12:05:42+00:00   \n",
       "\n",
       "                                            citation  \\\n",
       "0  Abrahamson, D., Shayan, S., Bakker, A., & van ...   \n",
       "1  Sandoval, W. A., Kawasaki, J., Cournoyer, N., ...   \n",
       "2  DeLiema, D., Saleh, A., Lee, C., Enyedy, N., D...   \n",
       "3  Hardy, L. & White, T. (2016). Making Sense of ...   \n",
       "4  Raval, H., Kaul, C., & McKenney, S. (2016). Th...   \n",
       "\n",
       "                  available  \\\n",
       "0 2017-03-21 12:05:42+00:00   \n",
       "1 2017-03-21 12:05:42+00:00   \n",
       "2 2017-03-21 12:05:42+00:00   \n",
       "3 2017-03-21 12:05:42+00:00   \n",
       "4 2017-03-21 12:05:42+00:00   \n",
       "\n",
       "                                            abstract                 value  \\\n",
       "0  The combination of two methodological resource...       Abrahamson, Dor   \n",
       "1  Abstract: The Next Generation Science Standard...  Sandoval, William A.   \n",
       "2  Researchers have increasingly demonstrated how...        DeLiema, David   \n",
       "3  In this paper we argue that collaborative lear...           Hardy, Lisa   \n",
       "4  Although English is mandatorily introduced as ...         Raval, Harini   \n",
       "\n",
       "   author_order  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['import2015full/172', 'import2015full/334', 'import2015full/262',\n",
       "       'import2015short/1101', 'import2015short/172',\n",
       "       'import2015short/334', 'import2015short/139',\n",
       "       'import2015short/1116', 'import2015short/262', 'import2018/167',\n",
       "       'import2018/289', 'import2018/244', 'import2018/292',\n",
       "       'import2018/259', 'import2018/267', 'import2018/251',\n",
       "       'import2018/454', 'import2018/465', 'import2018/517',\n",
       "       'import2018/528', 'import2018/521', 'import2018/348',\n",
       "       'import2018/526', 'import2018/370', 'import2018/519',\n",
       "       'import2018/527', 'import2018/518', 'import2018/520',\n",
       "       'import2018/516', 'import2018/529', 'import2018/511',\n",
       "       'import2018/410', 'import2018/249', 'import2018/427',\n",
       "       'import2018/480', 'import2018/236', 'import2018/253',\n",
       "       'import2018/514', 'import2018/513', 'import2018/525',\n",
       "       'import2018/522', 'import2018/523', 'import2018/524',\n",
       "       'import2018/512', 'import2018/515', 'import2017/116',\n",
       "       'import2017/111', 'import2017/118', 'import2017/119',\n",
       "       'import2017/117', 'import2017/112', 'import2017/115',\n",
       "       'import2017/114', 'import2017/113'], dtype=object)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning[cleaning.subject.map(len) == 0]['index'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaning[cleaning.subject.map(len) == 0]['index'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now parse citation to get the shortened name (which can be matched to refrences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `get_names` variable matches the general format of surnames in citations\n",
    "the `section_before_year` identifies parts with contain the names in APA style citation, by looking for the section before the (year) which comes right after the names of the authors in this citation style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_names = r'([\\w\\-\\&]*[\\,] [\\p{L}\\.\\ ]+[\\&\\,]?)'\n",
    "section_before_year = r'[\\S\\s]*\\(\\d{4}\\)'\n",
    "\n",
    "cleaning.reset_index(drop=True, inplace=True)\n",
    "\n",
    "cleaning['shortend_names'] = cleaning.citation.map(lambda x: re.match(section_before_year, x, re.U).group(0)).map(\n",
    "    lambda x: [x.replace(',', '').replace('&', '').rstrip() for x in regex.findall(get_names, x)])\n",
    "\n",
    "cleaning['shortend_names'] = cleaning.apply(lambda x: x['shortend_names'][x['author_order']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some renaming of the columns to make them easier to understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning.rename(columns={'index': 'file', 'value':'long_name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the file name match the convention\n",
    "cleaning['file'] = cleaning.file.map(lambda x: x.replace('/', '_'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the identifier string, which can be used to match references in paper to papers in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_authors_month(sentence, debug = False):\n",
    "    \"\"\"Gets name of authors + reference to time of publication\"\"\"\n",
    "    sentence = unicodedata.normalize('NFC', sentence)\n",
    "    #regex to match the pattern presented, people are misspelling prone, hence the long regex\n",
    "    regex = r'[\\p{L}\\,\\ \\.\\:\\;\\/\\&\\-\\'\\`\\(\\)\\’\\–\\¨\\…\\‐\\*\\´\\＆\\\\]*\\([\\,\\ \\p{L}\\d\\-]*(18|19|20)\\d{2}[\\,\\ \\p{L}\\d\\-]*\\)'\n",
    "    match_bad_year = r'[\\S\\s]*\\((18|19|20)\\d{2}\\/(18|19|20)\\d{2}\\)'\n",
    "\n",
    "    #we don't just have years, we also have sentences such as these instead of dates\n",
    "    match_press = r'[\\S\\s]*\\((i|I)n (P|p)ress|manuscript under review\\)'\n",
    "    match_forth = r'[\\S\\s]*\\((f|F)orthcoming\\)'\n",
    "    match_accepted = r'[\\S\\s]*\\((a|A)ccepted\\)'\n",
    "    match_submitted = r'[\\S\\s]*\\((s|S)ubmitted\\)'\n",
    "    match_underreview = r'[\\S\\s]*\\((u|U)nder (R|r)eview\\)'\n",
    "\n",
    "    #sentence = sentence.lower()\n",
    "    if reg.match(regex, sentence):\n",
    "        s = reg.search(regex, sentence).group(0)\n",
    "        if len(s) > 9:\n",
    "            return s\n",
    "    elif re.match(match_bad_year, sentence):\n",
    "        return re.search(match_bad_year, sentence).group(0)\n",
    "    elif re.match(match_press, sentence):\n",
    "        return re.search(match_press, sentence).group(0)\n",
    "    elif re.match(match_forth, sentence):\n",
    "        return re.search(match_forth, sentence).group(0)\n",
    "    elif re.match(match_accepted, sentence):\n",
    "        return re.search(match_accepted, sentence).group(0)\n",
    "    elif re.match(match_submitted, sentence):\n",
    "        return re.search(match_submitted, sentence).group(0)\n",
    "    elif re.match(match_underreview, sentence):\n",
    "        return re.search(match_underreview, sentence).group(0)\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def author_title(x):\n",
    "    \"\"\"Gets author and tite part of reference string\"\"\"\n",
    "    ref = x\n",
    "    authors = get_authors_month(x)  \n",
    "    if isinstance(authors, float):\n",
    "        return None\n",
    "    \n",
    "    search = len(authors)+1\n",
    "    #In Looi is a comming occurence hence it is inlcuded here\n",
    "    end = re.search('\\.|\\?|In Looi', ref[search:])\n",
    "    if end:\n",
    "        end = end.start()\n",
    "    else:\n",
    "        end = 0\n",
    "    return ref[: (search+end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning['identifier'] = cleaning[cleaning.citation.notna()].citation.map(lambda x: author_title(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unifying the names:\n",
    "\n",
    "\n",
    "We see that often people spell their name differently across years, hence we try to unify the naming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1644            van den Ende, Joan\n",
       "2201    van der Schaaf, Marieke F.\n",
       "1851             von Davier, Alina\n",
       "2933          von Davier, Alina A.\n",
       "8                     Öztok, Murat\n",
       "Name: long_name, dtype: object"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning.long_name.sort_values().drop_duplicates().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the unique names and then use intersection to determine if one names is equal to another (we consider it equal if it contains at least two names that are equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = cleaning.long_name.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We \"normalize\" the names, that is convert them all into the same type of unicode formating to get even more overlap, otherwise, letters that look the same may not be matched by string comparison, as their unicode is different.\n",
    "\n",
    "We visualy check that this way works by printing out matched. We can see that quite a lot of people have alternate versions of name spelling.\n",
    "\n",
    "We will hoever keep some version of these alternate spellings to have a database of person names which will later be used when we extract affiliations from the papers themselves. Check the parsing universities notebook for more information. (Implemented in the scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sandoval, William | Sandoval, William A.\n",
      "Barth-Cohen, Lauren | Barth-Cohen, Lauren A.\n",
      "Gil, Alfredo Jornet | Jornet, Alfredo\n",
      "Flood, Virginia J | Flood, Virginia J.\n",
      "Yoon, Susan | Yoon, Susan A.\n",
      "Gutiérrez, José | Gutiérrez, José Francisco\n",
      "Olsen, Jennifer | Olsen, Jennifer K.\n",
      "Dornfeld, Catherine | Dornfeld, Catherine L.\n",
      "Tissenbaum, Catherine Louise Dornfeld | Dornfeld, Catherine L.\n",
      "Lanouette, Kathryn | Lanouette, Kathryn A.\n",
      "Litts, Breanne K | Litts, Breanne K.\n",
      "Tan, Edna Tan | Tan, Edna\n",
      "Tomar, Gaurav | Tomar, Gaurav Singh\n",
      "Yip, Jason | Yip, Jason C.\n",
      "Hickey, Daniel | Hickey, Daniel T.\n",
      "Margulieux, Lauren E | Margulieux, Lauren E.\n",
      "Siebert-Evenstone, Amanda | Siebert-Evenstone, Amanda L.\n",
      "Martin, Nicole | Martin, Nicole D.\n",
      "Wise, Alyssa | Wise, Alyssa Friend\n",
      "Irgens, Golnaz Arastoopour | Arastoopour, Golnaz\n",
      "Tissenbaum, Catherine Louise Dornfeld | Dornfeld, Catherine\n",
      "Danish, Joshua | Danish, Joshua A.\n",
      "Richard, Gabriela | Richard, Gabriela T.\n",
      "Jacobson, Michael | Jacobson, Michael J.\n",
      "Jordan, Michelle E. Jordan | Jordan, Michelle E.\n",
      "Minshew, Lana | Minshew, Lana M.\n",
      "Slotta, James | Slotta, James D.\n",
      "sommer, stephen | Sommer, Stephen\n",
      "Pöysä-Tarhonen, Johanna | Pöysä-Tarhonen, Johanna\n",
      "Penuel, William R. | Penuel, William\n",
      "Clegg, Tamara L. | Clegg, Tamara\n",
      "Ramey, Kay E | Ramey, Kay E.\n",
      "Walsh, Elizabeth | Walsh, Elizabeth M.\n",
      "Barber-Lester, Kelly Johnson | Barber-Lester, Kelly J.\n",
      "Barber-Lester, Kelly | Barber-Lester, Kelly J.\n",
      "Splichal, Jin Michael | Splichal, Jin MIchael\n",
      "Rau, Martina | Rau, Martina A.\n",
      "Smith, Blaine Elizabeth | Smith, Blaine\n",
      "Chan, Carol K.K. | Chan, Carol\n",
      "Chan, Carol K. K. | Chan, Carol\n",
      "Searle, Kristin A. | Searle, Kristin A\n",
      "Polman, Joseph | Polman, Joseph L\n",
      "Linn, Marcia C. | Linn, Marcia\n",
      "Ruis, Andrew R. | Ruis, Andrew\n",
      "Wallon, Robert C. | Wallon, Robert C\n",
      "Peppler, Kylie | Peppler, Kylie A\n",
      "betser, sagit | Betser, Sagit\n",
      "Keifert, Danielle | Keifert, Danielle Teodora\n",
      "Keifert, Danielle T. | Keifert, Danielle Teodora\n",
      "Lui, Debora A. | Lui, Debora\n",
      "Walker, Justice T. | Walker, Justice Toshiba\n",
      "Wu, Sally P. W. | Wu, Sally P.W.\n",
      "Wu, Sally | Wu, Sally P.W.\n",
      "Chase, Catherine C. | Chase, Catherine\n",
      "Halverson, Erica R. | Halverson, Erica\n",
      "Gomoll, Andrea S. | Gomoll, Andrea Sarah\n",
      "Kafai, Yasmin B. | kafai, yasmin\n",
      "Kafai, Yasmin | kafai, yasmin\n",
      "Quintana, Rebecca | Quintana, Rebecca M\n",
      "Quintana, Rebecca M. | Quintana, Rebecca M\n",
      "Dyer, Elizabeth B | Dyer, Elizabeth B.\n",
      "Keifert, Danielle T. | Keifert, Danielle\n",
      "Cavera, Veronica L. | Cavera, Veronica L\n",
      "Applebaum, Lauren | Applebaum, Lauren R.\n",
      "Quintana, Rebecca M. | Quintana, Rebecca\n",
      "Lund, Kristine S. | Lund, Kristine\n",
      "Noushad, Noora F Noushad | Noushad, Noora F.\n",
      "Taylor, Katie Headrick Taylor | Taylor, Katie Headrick\n",
      "Wu, Sally | Wu, Sally P. W.\n",
      "Schwendimann, Beat | Schwendimann, Beat A.\n",
      "Vitale, Jonathan M. | Vitale, Jonathan\n",
      "Eagan, Brendan | Eagan, Brendan R.\n",
      "Eagan, Brendan R | Eagan, Brendan R.\n",
      "Brami, Uzi | Brami, Uzi Zevik\n",
      "Schmitt, Lara | Schmitt, Lara Johanna\n",
      "Levy, Sharona T. | Levy, Sharona T\n",
      "Levy, Sharona | Levy, Sharona T\n",
      "Harrer, Benedikt Walter | Harrer, Benedikt W.\n",
      "Easterday, Matthew | Easterday, Matthew W.\n",
      "Hmelo-Silver, Cindy | Hmelo-Silver, Cindy E.\n",
      "Kafai, Yasmin | Kafai, Yasmin B.\n",
      "Pea, Roy D. | Pea, Roy\n",
      "Chan, Carol K. K. | Chan, Carol K.K.\n",
      "Shaffer, David | Shaffer, David Williamson\n",
      "Gu, Xiaoqing Gu | Gu, Xiaoqing\n",
      "Krämer, Nicole | Krämer, Nicole C.\n",
      "Krämer, Nicole | Krämer, Nicole C.\n",
      "Champion, Dionne | Champion, Dionne N.\n",
      "Rosé, Carolyn Penstein | Rosé, Carolyn\n",
      "Rosé, Carolyn P. | Rosé, Carolyn\n",
      "Sayre, Eleanor C. | Sayre, Eleanor C\n",
      "Williams, Joseph Jay | Williams, Joseph\n",
      "Barber-Lester, Kelly | Barber-Lester, Kelly Johnson\n",
      "Kang, Seokbin | kang, Seokbin\n",
      "Brown, David E. | Brown, David\n",
      "Levy, Sharona | Levy, Sharona T.\n",
      "McDonald, Scott P. | McDonald, Scott\n",
      "McElhaney, Kevin W. | McElhaney, Kevin W\n",
      "Eagan, Brendan R | Eagan, Brendan\n",
      "Gerber, Elizabeth | Gerber, Elizabeth M.\n",
      "Quick, Joshua | Quick, Joshua D.\n",
      "Rosé, Carolyn P. | Rosé, Carolyn Penstein\n",
      "Trăușan-Matu, Ștefan | Trăușan-Matu, Ștefan\n",
      "Fields, Deborah | Fields, Deborah A.\n",
      "von Davier, Alina A. | von Davier, Alina\n",
      "Franklin, Scott V. | Franklin, Scott\n",
      "Nathan, Mitchell J. | Nathan, Mitchell\n",
      "Silva, Brenda López | Silva, Brenda Lopez\n",
      "Phillips, Abigail | Phillips, Abigail Leigh\n",
      "Kirschner, Paul A. | Kirschner, Paul\n",
      "Krämer, Nicole | Krämer, Nicole\n",
      "Häkkinen, Päivi | Häkkinen, Päivi\n",
      "Froehlich, Jon | Froehlich, Jon E.\n",
      "Mäkitalo, Åsa | Mäkitalo, Åsa\n",
      "Weiss, Patrice L. | Weiss, Patrice L. Tamar\n"
     ]
    }
   ],
   "source": [
    "d= {}\n",
    "for i, m in enumerate(names):\n",
    "    for j, n in enumerate(names):\n",
    "        if i < j and not 'de' in m:\n",
    "            y = set([i.lower() for i in reg.split(' |\\,', unicodedata.normalize('NFC', m)) if len(reg.sub('\\.', '', i)) > 1])\n",
    "            name = set([i.lower() for i in reg.split(' |\\,', unicodedata.normalize('NFC', n)) if len(reg.sub('\\.', '', i)) > 1])\n",
    "            if len(name.intersection(y)) > 1 and n!= m and not ('Lee' in n or 'Lee' in m):\n",
    "                d[n]= m\n",
    "                print(n, '|', m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the number of current names so we can see how the unification of spelling reduces the number of unique names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1971"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaning.long_name.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a mapping using the above matched, and we can now us it to unify the naming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning.loc[cleaning['long_name'].isin(d.keys()), 'long_name'] = cleaning.long_name.map(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1878"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaning.long_name.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can se that we reduce the number of distinct names quite a bit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataset is clean we can explore it, check the analysis section for more information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
